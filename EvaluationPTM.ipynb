{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b662402",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 ' + \\\n",
    "            '--dataset all --n-word 30000 --epochs-1 100 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 50 ' + \\\n",
    "            '--n-cluster 20 ' + \\\n",
    "            '--stage-1-ckpt trained_model/news_model_paraphrase-MiniLM-L6-v2_stage1_20t_2000w_99e.ckpt ' + \\\n",
    "            '--palmetto-dir /home/minseo/jupyter_dir/PTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30320a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/don12/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/don12/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/don12/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset, BertDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import ConcatDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ee0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2,3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80a6d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    # 각 stage에서의 epochs수 \n",
    "    parser.add_argument('--epochs-1', default=50, type=int,\n",
    "                        help='Number of training epochs for Stage 1')   \n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    #각 stage에서의 batch size\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    #data set정의 \n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    # 클러스터 수와 topic의 수는 20 (k==20)\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    # 단어vocabulary는 2000로 setting\n",
    "    parser.add_argument('--n-word', default=30000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1,2,3], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "   \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str,\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "#데이터들을 로드함(텍스트중에 none값을 제거/dataset_name에 따라 textData에 들어가는 내용이 달라짐)\n",
    "def data_load(dataset_name, sample_size=10000):\n",
    "    should_measure_hungarian = False\n",
    "    textData = []\n",
    "    \n",
    "    if dataset_name == 'twitter':\n",
    "        dataset = TwitterDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Twitter\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'reddit':\n",
    "        dataset = RedditDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Reddit\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'youtube':\n",
    "        dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"YouTube\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'all':\n",
    "        twitter_dataset = TwitterDataset(sample_size=sample_size)\n",
    "        reddit_dataset = RedditDataset(sample_size=sample_size)\n",
    "        youtube_dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        \n",
    "        # filtering NaN values\n",
    "        textData += [(text, \"Twitter\") for text in twitter_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"Reddit\") for text in reddit_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"YouTube\") for text in youtube_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name!\")\n",
    "    \n",
    "    return textData, should_measure_hungarian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51b3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5aa99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aa510b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b89543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터셋 초기화(data.py에서 확인 가능)\n",
    "twitter_ds = TwitterDataset()\n",
    "reddit_ds = RedditDataset()\n",
    "youtube_ds = YoutubeDataset()\n",
    "\n",
    "\n",
    "total_text_list = twitter_ds.texts + reddit_ds.texts + youtube_ds.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fd20f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73fb73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, bert, text_list, N_word, vectorizer=None, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.nonempty_text = [text for text in text_list if len(text) > 0]\n",
    "        \n",
    "        # Remove new lines\n",
    "        self.nonempty_text = [re.sub(\"\\n\",\" \", sent) for sent in self.nonempty_text]\n",
    "                \n",
    "        # Remove Emails\n",
    "        self.nonempty_text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove new line characters\n",
    "        self.nonempty_text = [re.sub('\\s+', ' ', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove distracting single quotes\n",
    "        self.nonempty_text = [re.sub(\"\\'\", \"\", sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        self.jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert)\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        self.N_word = N_word\n",
    "        \n",
    "        if vectorizer == None:\n",
    "            self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "            self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text))\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            \n",
    "        self.org_list = []\n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            org_input = self.tokenizer(sent, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "            org_input['input_ids'] = torch.squeeze(org_input['input_ids'])\n",
    "            org_input['attention_mask'] = torch.squeeze(org_input['attention_mask'])\n",
    "            self.org_list.append(org_input)\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray()\n",
    "        vectorized_input = vectorized_input.astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                             for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp        \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nonempty_text)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.org_list[idx], self.bow_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db8c642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [07:24<00:00, 675.24it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=total_text_list, N_word=n_word, vectorizer=None, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0622589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all_texts to create a BoW matrix\n",
    "total_bow_matrix = trainds.vectorizer.transform(total_text_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734fdcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 30000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확인용 코드\n",
    "total_bow_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30d99e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_word = total_bow_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea48a7",
   "metadata": {},
   "source": [
    "# Re_fornulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bce9246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d944153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84af9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cosine_similarity_matrix(total_bow_matrix, batch_size=128):\n",
    "    n_rows = total_bow_matrix.shape[0]\n",
    "    similarity_matrix = np.empty((n_rows, n_rows), dtype=np.float32)\n",
    "    \n",
    "    for i in tqdm(range(0, n_rows, batch_size)):\n",
    "        start_idx = i\n",
    "        end_idx = min(i + batch_size, n_rows)\n",
    "        batch_data = total_bow_matrix[start_idx:end_idx]\n",
    "        \n",
    "        batch_similarity = cosine_similarity(batch_data, total_bow_matrix)\n",
    "        \n",
    "        similarity_matrix[start_idx:end_idx, :] = batch_similarity\n",
    "        similarity_matrix[:, start_idx:end_idx] = batch_similarity.T\n",
    "        \n",
    "    np.fill_diagonal(similarity_matrix, -1)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c85d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2344 [1:15:18<139:19:48, 215.64s/it]"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도 매트릭스를 계산합니다.\n",
    "similarity_matrix = compute_cosine_similarity_matrix(total_bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e77a94c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7397484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, similarity_matrix, N_word, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        self.N_word = N_word\n",
    "        \n",
    "        self.jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])\n",
    "        \n",
    "#         english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "#         self.stopwords_list = set(english_stopwords)\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        self.similarity_matrix = torch.tensor(similarity_matrix)  # NumPy 배열을 PyTorch 텐서로 변환\n",
    "        sim_weight, sim_indices = self.similarity_matrix.topk(k=k, dim=-1)\n",
    "        zip_iterator = zip(np.arange(len(sim_weight)), sim_indices.squeeze().data.numpy())\n",
    "        self.pos_dict = dict(zip_iterator)\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return idx, self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77d36092",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0d364b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f80f642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:02<00:00, 1096.14it/s]\n",
      "100%|██████████| 3000/3000 [01:58<00:00, 25.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, similarity_matrix, n_word, lemmatize=True)    \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "496e2acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(finetuneds.embedding_list)\n",
    "len(finetuneds.bow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e28e1",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2cd3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436ab94",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18a5f68a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 9.53564 - dist: 0.19823 - cons: -0.05022\n",
      "Epoch-1 / recon: 9.13438 - dist: 0.18431 - cons: -0.05065\n",
      "Epoch-2 / recon: 8.88061 - dist: 0.17088 - cons: -0.05226\n",
      "Epoch-3 / recon: 8.70116 - dist: 0.16038 - cons: -0.05364\n",
      "Epoch-4 / recon: 8.56932 - dist: 0.15393 - cons: -0.05470\n",
      "Epoch-5 / recon: 8.46608 - dist: 0.14910 - cons: -0.05656\n",
      "Epoch-6 / recon: 8.37859 - dist: 0.14531 - cons: -0.06054\n",
      "Epoch-7 / recon: 8.30504 - dist: 0.14158 - cons: -0.06400\n",
      "Epoch-8 / recon: 8.24169 - dist: 0.13910 - cons: -0.06691\n",
      "Epoch-9 / recon: 8.18338 - dist: 0.13740 - cons: -0.06964\n",
      "Epoch-10 / recon: 8.13027 - dist: 0.13609 - cons: -0.07242\n",
      "Epoch-11 / recon: 8.08097 - dist: 0.13466 - cons: -0.07504\n",
      "Epoch-12 / recon: 8.03558 - dist: 0.13329 - cons: -0.07764\n",
      "Epoch-13 / recon: 7.99295 - dist: 0.13225 - cons: -0.07989\n",
      "Epoch-14 / recon: 7.95253 - dist: 0.13091 - cons: -0.08180\n",
      "Epoch-15 / recon: 7.91408 - dist: 0.12966 - cons: -0.08384\n",
      "Epoch-16 / recon: 7.87664 - dist: 0.12848 - cons: -0.08560\n",
      "Epoch-17 / recon: 7.84085 - dist: 0.12738 - cons: -0.08713\n",
      "Epoch-18 / recon: 7.80661 - dist: 0.12639 - cons: -0.08891\n",
      "Epoch-19 / recon: 7.77353 - dist: 0.12549 - cons: -0.09052\n",
      "Epoch-20 / recon: 7.74156 - dist: 0.12486 - cons: -0.09197\n",
      "Epoch-21 / recon: 7.71031 - dist: 0.12408 - cons: -0.09341\n",
      "Epoch-22 / recon: 7.68057 - dist: 0.12353 - cons: -0.09475\n",
      "Epoch-23 / recon: 7.65120 - dist: 0.12276 - cons: -0.09622\n",
      "Epoch-24 / recon: 7.62264 - dist: 0.12209 - cons: -0.09754\n",
      "Epoch-25 / recon: 7.59464 - dist: 0.12147 - cons: -0.09878\n",
      "Epoch-26 / recon: 7.56766 - dist: 0.12089 - cons: -0.09990\n",
      "Epoch-27 / recon: 7.54122 - dist: 0.12026 - cons: -0.10108\n",
      "Epoch-28 / recon: 7.51532 - dist: 0.11961 - cons: -0.10209\n",
      "Epoch-29 / recon: 7.49046 - dist: 0.11916 - cons: -0.10300\n",
      "Epoch-30 / recon: 7.46591 - dist: 0.11870 - cons: -0.10389\n",
      "Epoch-31 / recon: 7.44208 - dist: 0.11814 - cons: -0.10489\n",
      "Epoch-32 / recon: 7.41839 - dist: 0.11762 - cons: -0.10576\n",
      "Epoch-33 / recon: 7.39532 - dist: 0.11717 - cons: -0.10666\n",
      "Epoch-34 / recon: 7.37289 - dist: 0.11672 - cons: -0.10743\n",
      "Epoch-35 / recon: 7.35097 - dist: 0.11631 - cons: -0.10810\n",
      "Epoch-36 / recon: 7.32968 - dist: 0.11599 - cons: -0.10871\n",
      "Epoch-37 / recon: 7.30859 - dist: 0.11563 - cons: -0.10942\n",
      "Epoch-38 / recon: 7.28792 - dist: 0.11540 - cons: -0.11007\n",
      "Epoch-39 / recon: 7.26786 - dist: 0.11514 - cons: -0.11069\n",
      "Epoch-40 / recon: 7.24820 - dist: 0.11491 - cons: -0.11125\n",
      "Epoch-41 / recon: 7.22908 - dist: 0.11465 - cons: -0.11174\n",
      "Epoch-42 / recon: 7.21010 - dist: 0.11448 - cons: -0.11219\n",
      "Epoch-43 / recon: 7.19138 - dist: 0.11428 - cons: -0.11272\n",
      "Epoch-44 / recon: 7.17308 - dist: 0.11415 - cons: -0.11315\n",
      "Epoch-45 / recon: 7.15509 - dist: 0.11400 - cons: -0.11358\n",
      "Epoch-46 / recon: 7.13752 - dist: 0.11386 - cons: -0.11392\n",
      "Epoch-47 / recon: 7.12013 - dist: 0.11383 - cons: -0.11425\n",
      "Epoch-48 / recon: 7.10310 - dist: 0.11379 - cons: -0.11458\n",
      "Epoch-49 / recon: 7.08624 - dist: 0.11379 - cons: -0.11480\n",
      "------- Evaluation results -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:00<00:00, 323.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between Topic Labels and Extra Labels: 0.28148959498096826\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConcatDataset' object has no attribute 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 130\u001b[0m\n\u001b[1;32m    124\u001b[0m         mi \u001b[38;5;241m=\u001b[39m mutual_info_score(topic_labels, platform_labels)\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMutual Information between Topic Labels and Extra Labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m         label_accuracy \u001b[38;5;241m=\u001b[39m measure_hungarian_score(\n\u001b[1;32m    129\u001b[0m                              topic_dist,\n\u001b[0;32m--> 130\u001b[0m                              [target \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtextData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m)\n\u001b[1;32m    131\u001b[0m                               \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m non_empty_text_index]\n\u001b[1;32m    132\u001b[0m                          )\n\u001b[1;32m    133\u001b[0m         results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_match\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_accuracy\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m#     print(results)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#     print()\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m#     results_list.append(results)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConcatDataset' object has no attribute 'targets'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "args.stage_2_repeat = 1\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "#     model.load_state_dict(torch.load(model_stage1_name), strict=True)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=True, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        for batch_idx, batch in enumerate(trainloader):  \n",
    "            _,org_input, pos_input, org_bow, pos_bow = batch\n",
    "            org_input = org_input.cuda(gpu_ids[0])\n",
    "            org_bow = org_bow.cuda(gpu_ids[0])\n",
    "            pos_input = pos_input.cuda(gpu_ids[0])\n",
    "            pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "            batch_size = org_input.size(0) #org_input_ids.size(0)\n",
    "\n",
    "            org_dists, org_topic_logit = model.decode(org_input)\n",
    "            pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "            org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "            pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "            # reconstruction loss\n",
    "            # batchmean\n",
    "#             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "#             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "            \n",
    "#             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "#             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "            \n",
    "            recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * (1-org_bow), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * (1-pos_bow), axis=1), axis=0)\n",
    "            recons_loss *= 0.5\n",
    "\n",
    "            # consistency loss\n",
    "            pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "            cons_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution loss\n",
    "            # batchmean\n",
    "#             distmatch_loss = dist_match_loss(torch.cat((org_topic), dim=0), dirichlet_alpha_2)\n",
    "            distmatch_loss = dist_match_loss(torch.cat((org_topic,), dim=0), dirichlet_alpha_2)\n",
    "\n",
    "\n",
    "            loss = args.coeff_2_recon * recons_loss + \\\n",
    "                   args.coeff_2_cons * cons_loss + \\\n",
    "                   args.coeff_2_dist * distmatch_loss \n",
    "            \n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(cons_loss.item(), bsz)\n",
    "            rlosses.update(recons_loss.item(), bsz)\n",
    "            distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    print(\"------- Evaluation results -------\")\n",
    "    all_list = {}\n",
    "#     for e, i in enumerate(model.beta.cpu().topk(10, dim=1).indices):\n",
    "#         word_list = []\n",
    "#         for j in i:\n",
    "#             word_list.append(vocab_dict_reverse[j.item()])\n",
    "#         all_list[e] = word_list\n",
    "#         print(\"topic-{}\".format(e), word_list)\n",
    "\n",
    "#     topic_words_list = list(all_list.values())\n",
    "#     now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "#     results = get_topic_qualities(topic_words_list, palmetto_dir=args.palmetto_dir,\n",
    "#                                   reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "#                                   filename=f'results/{now}.txt')\n",
    "    \n",
    "    if should_measure_hungarian:\n",
    "        topic_dist = torch.empty((0, n_topic))\n",
    "        model.eval()\n",
    "        evalloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "        non_empty_text_index = [i for i, text in enumerate(total_text_list) if len(text) != 0]\n",
    "        assert len(finetuneds) == len(non_empty_text_index)\n",
    "        \n",
    "        # 추가: 각 데이터의 토픽 라벨과 추가 라벨을 저장하는 리스트\n",
    "        topic_labels = []\n",
    "        platform_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(evalloader):\n",
    "                org_idx,org_input, _, org_bow, __ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                topic_dist = torch.cat((topic_dist, org_topic.detach().cpu()), 0)\n",
    "                \n",
    "                batch_size = org_input.size(0)\n",
    "                for i in range(batch_size):\n",
    "                    topic_labels.append(org_topic[i].argmax().item())\n",
    "                    platform_labels.append(org_idx[i] // 1000)\n",
    "                    \n",
    "        # Mutual Information 계산\n",
    "        mi = mutual_info_score(topic_labels, platform_labels)\n",
    "        print(f\"Mutual Information between Topic Labels and Extra Labels: {mi}\")\n",
    "\n",
    "                \n",
    "        label_accuracy = measure_hungarian_score(\n",
    "                             topic_dist,\n",
    "                             [target for i, target in enumerate(textData.targets)\n",
    "                              if i in non_empty_text_index]\n",
    "                         )\n",
    "        results['label_match'] = label_accuracy\n",
    "\n",
    "#     print(results)\n",
    "#     print()\n",
    "#     results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7308a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_measure_hungarian = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86996e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "should_measure_hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d5720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "don",
   "language": "python",
   "name": "don"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
