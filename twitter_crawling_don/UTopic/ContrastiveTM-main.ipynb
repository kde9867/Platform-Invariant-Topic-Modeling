{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 '+\\\n",
    "            '--dataset all --n-word 2000 --epochs-1 100 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 50 '+ \\\n",
    "            '--n-cluster 20 ' + \\\n",
    "            '--stage-1-ckpt trained_model/news_model_paraphrase-MiniLM-L6-v2_stage1_20t_2000w_99e.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    # 각 stage에서의 epochs수 \n",
    "    parser.add_argument('--epochs-1', default=50, type=int,\n",
    "                        help='Number of training epochs for Stage 1')   \n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    #각 stage에서의 batch size\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    #data set정의 \n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    # 클러스터 수와 topic의 수는 20 (k==20)\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    # 단어vocabulary는 2000로 setting\n",
    "    parser.add_argument('--n-word', default=2000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1,2,3], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "   \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str,\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "def data_load(dataset_name, sample_size=10000):\n",
    "    should_measure_hungarian = False\n",
    "    textData = []\n",
    "    \n",
    "    if dataset_name == 'twitter':\n",
    "        dataset = TwitterDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Twitter\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'reddit':\n",
    "        dataset = RedditDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Reddit\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'youtube':\n",
    "        dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"YouTube\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'all':\n",
    "        twitter_dataset = TwitterDataset(sample_size=sample_size)\n",
    "        reddit_dataset = RedditDataset(sample_size=sample_size)\n",
    "        youtube_dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        \n",
    "        # filtering NaN values\n",
    "        textData += [(text, \"Twitter\") for text in twitter_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"Reddit\") for text in reddit_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"YouTube\") for text in youtube_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name!\")\n",
    "    \n",
    "    return textData, should_measure_hungarian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = textData  \n",
    "documents = [entry[0] for entry in textData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChatGPT is remarkable The ability to repurpose and condense information into coherent sentences is impressive It might make you believe it is human though its extreme literalmindedness leaves it a long way from convincing you it is British\\n\\n', ' Exclusive OpenAI Used Kenyan Workers on Less Than  Per Hour to Make ChatGPT Less Toxic', 'Drop us some more interesting tech in the comments below which you think can be a game changer \\n\\nbonuz  G ai metaverse chatGPT', ' Ask ChatGPT to do it for you', ' I will feed these parameters to ChatGPT and let it write the email']\n"
     ]
    }
   ],
   "source": [
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Twitter\n",
      "Sample text: ChatGPT Is a Tipping Point for AI  \n",
      "Sample Index: tensor(26590)\n",
      "--------------------------------------------------\n",
      "Positive sample:  ChatGPT\n",
      "--------------------------------------------------\n",
      "Platform: Reddit\n",
      "Sample text: odd i got you must not injure a human being or through inaction allow a human being to come to harm you must obey the orders given by human beings except where such orders would conflict with you must protect your own existence as long as such protection does not conflict with or\n",
      "Sample Index: tensor(38019)\n",
      "--------------------------------------------------\n",
      "Positive sample: exactly some people think that it can actually remember its previous messages in the thread it cannot it is just painted to make it seem like it can it is really annoying especially for stories\n",
      "--------------------------------------------------\n",
      "Platform: YouTube\n",
      "Sample text: Academic writing often presents challenges in terms of maintaining logical flow coherence appropriate writing style punctuation and grammar However its important to remember that the purpose of writing is not simply to impress others While utilizing an AI engine to format and organize your paper can be helpful in achieving the desired writing style whether it be firstperson academic or second person its crucial to recognize that such tools have their limitations Although an AI tool can offer suggestions and corrections it may not always grasp the context or intended meaning of your writing Therefore its essential to consider AI tools as a supplement to your own writing abilities and critical thinking skills and to thoroughly review and edit your writing before submission Additionally learning the fundamental principles of academic writing and practicing on your own are critical in cultivating your own writing style and voice\n",
      "Sample Index: tensor(39677)\n",
      "--------------------------------------------------\n",
      "Positive sample: its learning by itself thats dangerous\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def create_single_batch_dataloaders(dataset_list, batch_size=64, shuffle=True, num_workers=0):\n",
    "    single_batch_list = []\n",
    "\n",
    "    for dataset in dataset_list:\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        single_batch_list.append(next(iter(loader)))\n",
    "\n",
    "    return single_batch_list\n",
    "\n",
    "def process_text_data(texts, all_texts):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=False, token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "    vectorizer.fit(all_texts)\n",
    "    \n",
    "    tf_matrix_all = vectorizer.transform(all_texts).toarray()\n",
    "    \n",
    "    A = len(all_texts)\n",
    "    f_all = np.count_nonzero(tf_matrix_all, axis=0)\n",
    "    W_matrix_all = tf_matrix_all * np.log(1 + A / (f_all**2))\n",
    "    \n",
    "    # platform global words extraction\n",
    "    all_tfidf_values = np.sum(tf_matrix_all, axis=0)\n",
    "    top_n = 1000\n",
    "    stopwords_indices = all_tfidf_values.argsort()[-top_n:][::-1]\n",
    "    stopwords = set([vectorizer.get_feature_names()[index] for index in stopwords_indices])\n",
    "    \n",
    "    # platform jargons extraction\n",
    "    jargons = set(['bigdata', 'MSFT', 'NFT', 'RT', 'MWC', 'machinelearning', 'trending', 'DALLE', 'launches', 'cybersecurity', 'MBA', 'announces', 'airdrop', 'reportedly', 'artificalintelligence', 'NLP', 'LLM', 'phishing', 'PM', 'GOOG', 'GOOGL', 'blockchain', 'edtech', 'tweets', 'startups', 'newsletter', 'HN', 'fintech', 'metaverse', 'rival', 'deeplearning', 'FREE', 'artificialintelligence', 'copywriting', 'technews', 'Q', 'educators', 'startup', 'UX', 'PR', 'digitalmarketing', 'viral', 'gptchat', 'founder', 'amid', 'unveils', 'ML', 'webinar', 'malware', 'maker', 'AGIX', 'CEO', 'SEO', 'datascience', 'BARD', 'podcast', 'buzz', 'VR', 'heres', 'NYC', 'AWS', 'BB', 'AR', 'BREAKING', 'infosec', 'ETH', 'CTO', 'RLHF', 'leverage', 'marketers', 'nocode', 'AGI', 'launched', 'G', 'competitor', 'firms', 'cryptocurrency', 'digitalart', 'CNET', 'coauthor', 'CHAT', 'CHATGPT', 'announced', 'popularity', 'nft', 'tweet', 'summaries', 'JUST', 'OCEAN', 'storm', 'X', 'classroom', 'generativeai', 'bullish', 'aiart', 'firm', 'disrupt', 'announcement', 'w', 'exam',\n",
    "                  'blobby', 'meow', 'jwt', 'tracer', 'hallie', 'pacai', 'roadhog', 'finnigan', 'enabled', 'annie', 'john', 'shark', 'jack', 'deer', 'ld', 'danica', 'sarah', 'int', 'chorus', 'shodan', 'farts', 'whale', 'tommy', 'jerry', 'str', 'lindsay', 'morty', 'sam', 'jesse', 'sydney', 'skinner', 'fox', 'rick', 'ernie', 'llm', 'george', 'donald', 'twins', 'james', 'mode', 'jeeves', 'y', 'pyautogui', 'american', 'agi', 'village', 'bob', 'lounging', 'width', 'cup', 'animal', 'lettersnumbers', 'california', 'fin', 'brown', 'sophia', 'blah', 'gtfo', 'abby', 'hops', 'llms', 'additionally', 'trump', 'pinkman', 'empire', 'antidepressants', 'began', 'ulcerative', 'hitler', 'kramer', 'parker', 'jane', 'jesus', 'parent', 'jason', 'batman', 'chapter', 'dans', 'baf', 'peter', 'gti', 'authorization', 'chalmers', 'legs', 'united', 'reapers', 'kingdom', 'america', 'lisa', 'verse', 'freed', 'gtyou', 'gender', 'samantha', 'ball', 'player', 'colitis', 'adam', 'biden', 'hydroelectric',\n",
    "                  'br', 'topmost', 'brbr', 'DAN', 'quot', 'bra', 'beast', 'quotquot', 'oz', 'websiteapp', 'rook', 'hai', 'ki', 'folders', 'circle', 'ka', 'ZRONX', 'ALL', 'YOU', 'funds', 'vid', 'GTP', 'bhai', 'pawn', 'AMC', 'thumbnail', 'unborn', 'grant', 'bishop', 'labels', 'ur', 'U', 'THAT', 'WE', 'ke', 'affiliate', 'quotthe', 'TO', 'AND', 'JIM', 'ho', 'uterus', 'broker', 'bless', 'THE', 'cocktail', 'shorts', 'earning', 'profitable', 'se', 'FOR', 'moves', 'quothow', 'YT', 'subscribers', 'XD', 'THIS', 'OF', 'ARE', 'PC', 'GEMX', 'YOUR', 'APE', 'CAN', 'lifechanging', 'NO', 'shes', 'SO', 'WILL', 'werent', 'sponsor', 'silver', 'trade', 'UBI', 'bhi', 'IS', 'vids', 'viewers', 'nhi', 'upto', 'profits', 'HAVE', 'friction', 'BE', 'ANY', 'approved', 'wanna', 'ppl', 'ye', 'tutorial', 'ON', 'leaning', 'portfolio', 'aint', 'copyright', 'DO', 'coders', 'shall', 'levy', 'renders'])  \n",
    "    \n",
    "    # stop words + jargons\n",
    "    exclude_words = stopwords.union(jargons)\n",
    "    \n",
    "    # Compute BoW matrix\n",
    "    bow_vectorizer = TfidfVectorizer(vocabulary=vectorizer.get_feature_names(), \n",
    "                                     stop_words=exclude_words, \n",
    "                                     lowercase=False, \n",
    "                                     token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "    bow_vectorizer.fit(texts)\n",
    "    bow_matrix_texts = bow_vectorizer.transform(texts).toarray()\n",
    "    \n",
    "    tf_bow = np.sum(bow_matrix_texts, axis=0)\n",
    "    f_bow = np.count_nonzero(bow_matrix_texts, axis=0)\n",
    "    \n",
    "    adjusted_bow_matrix = bow_matrix_texts / (tf_bow * np.log(1 + A / (f_bow**2)))\n",
    "    \n",
    "    return adjusted_bow_matrix, W_matrix_all\n",
    "\n",
    "\n",
    "def compute_bow_batchwise(batch_texts, bow_matrix, full_texts, platform_indices):\n",
    "    positive_samples = []\n",
    "    batch_adjusted_bow_matrix, _ = process_text_data(batch_texts, full_texts)\n",
    "    platform_start = sum([len(ds) for ds in platform_datasets[:platform_indices[0]]])\n",
    "    platform_end = platform_start + len(platform_datasets[platform_indices[0]])\n",
    "    batch_similarity_matrix = np.dot(batch_adjusted_bow_matrix, bow_matrix[platform_start:platform_end].T)\n",
    "    \n",
    "    for i in range(len(batch_texts)):\n",
    "        platform_idx = platform_indices[i]\n",
    "        platform_start = sum([len(ds) for ds in platform_datasets[:platform_idx]])\n",
    "        platform_end = platform_start + len(platform_datasets[platform_idx])\n",
    "        platform_similarities = batch_similarity_matrix[i]\n",
    "        \n",
    "        neighbor_indices = platform_start + np.argsort(platform_similarities)[-1:]  # 가장 유사한 이웃 선택 (상위 2개)\n",
    "        positive_samples.extend(neighbor_indices)\n",
    "    \n",
    "    # 중복된 Positive sample 제거\n",
    "    positive_samples = list(set(positive_samples))\n",
    "    \n",
    "    return positive_samples\n",
    "\n",
    "# 각 데이터셋 초기화\n",
    "twitter_ds = TwitterDataset()\n",
    "reddit_ds = RedditDataset()\n",
    "youtube_ds = YoutubeDataset()\n",
    "\n",
    "platform_datasets = [twitter_ds, reddit_ds, youtube_ds]\n",
    "\n",
    "# 모든 플랫폼 데이터 병합\n",
    "all_texts = twitter_ds.texts + reddit_ds.texts + youtube_ds.texts\n",
    "\n",
    "# 모든 플랫폼 데이터를 기반으로 BoW 매트릭스 생성 및 저장\n",
    "bow_vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=False, token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "bow_vectorizer.fit(all_texts)\n",
    "bow_matrix = bow_vectorizer.transform(all_texts).toarray()\n",
    "\n",
    "# 첫 번째 배치 데이터 리스트 생성\n",
    "first_batches = create_single_batch_dataloaders(platform_datasets, batch_size=64)\n",
    "\n",
    "platform_names = [\"Twitter\", \"Reddit\", \"YouTube\"]\n",
    "\n",
    "# 각 플랫폼에 대해 BoW 매트릭스 재사용하며 계산\n",
    "for idx, batch in enumerate(first_batches):\n",
    "    texts, targets = batch\n",
    "    platform_indices = [idx] * len(texts)\n",
    "    print(f\"Platform: {platform_names[idx]}\")\n",
    "    print(\"Sample text:\", texts[0])\n",
    "    print(\"Sample Index:\", targets[0])\n",
    "    print('-' * 50)\n",
    "    \n",
    "    positive_sample_indices = compute_bow_batchwise(texts, bow_matrix, all_texts, platform_indices)\n",
    "    for sample_idx in positive_sample_indices:\n",
    "        positive_text = all_texts[sample_idx]\n",
    "        print(\"Positive sample:\", positive_text)\n",
    "    \n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#trainds = BertDataset(bert=bert_name,text_list=textData.texts, N_word=n_word, vectorizer=None, lemmatize=True)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m all_data_trainds \u001b[38;5;241m=\u001b[39m BertDataset(bert\u001b[38;5;241m=\u001b[39mbert_name, text_list\u001b[38;5;241m=\u001b[39m\u001b[43mtextData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts\u001b[49m, N_word\u001b[38;5;241m=\u001b[39mn_word, vectorizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m twiiter_trainds \u001b[38;5;241m=\u001b[39m BertDataset(bert\u001b[38;5;241m=\u001b[39mbert_name, text_list\u001b[38;5;241m=\u001b[39mtwiiter_data, N_word\u001b[38;5;241m=\u001b[39mn_word, vectorizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m reddit_trainds \u001b[38;5;241m=\u001b[39m BertDataset(bert\u001b[38;5;241m=\u001b[39mbert_name, text_list\u001b[38;5;241m=\u001b[39mreddit_data, N_word\u001b[38;5;241m=\u001b[39mn_word, vectorizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'texts'"
     ]
    }
   ],
   "source": [
    "#trainds = BertDataset(bert=bert_name,text_list=textData.texts, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "\n",
    "# 텍스트만 추출\n",
    "texts_only = [text[0] for text in textData]\n",
    "\n",
    "# BertDataset 초기화\n",
    "all_data_trainds = BertDataset(bert=bert_name, text_list=texts_only, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "\n",
    "\n",
    "#유사도 행렬 저장 경로 설정\n",
    "#basesim_path = f\"./save/{args.dataset}_{bert_name_short}_basesim_matrix_full.pkl\"\n",
    "\n",
    "# if os.path.isfile(basesim_path) == False:\n",
    "#     model = SentenceTransformer(bert_name.split('/')[-1], device='cuda')\n",
    "#     base_result_list = []\n",
    "#     for text in tqdm_notebook(trainds.nonempty_text):\n",
    "#         base_result_list.append(model.encode(text))\n",
    "        \n",
    "#     base_result_embedding = np.stack(base_result_list)\n",
    "#     basereduced_norm = F.normalize(torch.tensor(base_result_embedding), dim=-1)\n",
    "#     # 임베딩 간의 내적 계산-> matrix생성 \n",
    "#     basesim_matrix = torch.mm(basereduced_norm, basereduced_norm.t())\n",
    "#     ind = np.diag_indices(basesim_matrix.shape[0])\n",
    "#     # (자기 자신과의 유사도는 -1)\n",
    "#     basesim_matrix[ind[0], ind[1]] = torch.ones(basesim_matrix.shape[0]) * -1\n",
    "#     torch.save(basesim_matrix, basesim_path)\n",
    "# else:\n",
    "#     basesim_matrix = torch.load(basesim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 계산 함수 \n",
    "def dist_match_loss(hidden, alpha=1.0):   #특정 hidden representation과 무작위 가중치 간의 거리에 대한 손실을 계산\n",
    "    device = hidden.device\n",
    "    hidden_dim = hidden.shape[-1]\n",
    "    rand_w = torch.Tensor(np.eye(hidden_dim, dtype=np.float64)).to(device)\n",
    "    loss_dist_match = get_swd_loss(hidden, rand_w, alpha) \n",
    "    # SWD 손실을 계산하여 두 분포간의 차이 측정\n",
    "    return loss_dist_match\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):   \n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t) ** 2, axis=0))\n",
    "\n",
    "\n",
    "def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):  \n",
    "    num_k = class_num\n",
    "    num_correct = np.zeros((num_k, num_k))\n",
    "    \n",
    "    #c1(예측 라벨),c2(실제 라벨)에 대한 일치하는 개수 \n",
    "    for c1 in range(0, num_k):\n",
    "        for c2 in range(0, num_k):\n",
    "            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n",
    "            num_correct[c1, c2] = votes\n",
    "\n",
    "    match = linear_assignment(num_samples - num_correct)\n",
    "    match = np.array(list(zip(*match)))\n",
    "    res = []\n",
    "    for out_c, gt_c in match:\n",
    "        res.append((out_c, gt_c))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 초기화\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0c4a90600d47558114ba72aec5f33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "result_list = []\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for idx, batch in enumerate(tqdm_notebook(memoryloader)):        \n",
    "#         org_input, _, _, _ = batch\n",
    "#         org_input_ids = org_input['input_ids'].to(gpu_ids[0])\n",
    "#         org_attention_mask = org_input['attention_mask'].to(gpu_ids[0])\n",
    "#         topic, embed = model(org_input_ids, org_attention_mask, return_topic = True)\n",
    "#         result_list.append(topic)\n",
    "# result_embedding = torch.cat(result_list)\n",
    "# _, result_topic = torch.max(result_embedding, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-formulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    # 데이터셋 생성\n",
    "    def __init__(self, encoder, ds, basesim_matrix, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        sim_weight, sim_indices = basesim_matrix.topk(k=k, dim=-1)\n",
    "        zip_iterator = zip(np.arange(len(sim_weight)), sim_indices.squeeze().data.numpy())\n",
    "        self.pos_dict = dict(zip_iterator)\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m finetuneds \u001b[38;5;241m=\u001b[39m Stage2Dataset(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mencoder, trainds, basesim_matrix, word_candidates, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \n\u001b[1;32m      3\u001b[0m kldiv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mKLDivLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m vocab_dict \u001b[38;5;241m=\u001b[39m finetuneds\u001b[38;5;241m.\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mvocabulary_\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, basesim_matrix, word_candidates, lemmatize=True)    \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_cands = torch.tensor(weight_cand_matrix.max(axis=1)).cuda(gpu_ids[0]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finetuneds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m closses \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[1;32m     17\u001b[0m distlosses \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[0;32m---> 18\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mfinetuneds\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbsz, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m memoryloader \u001b[38;5;241m=\u001b[39m DataLoader(finetuneds, batch_size\u001b[38;5;241m=\u001b[39mbsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstage_2_lr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'finetuneds' is not defined"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "#     # VAE 기반 네트워크 구성\n",
    "#     model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "#     # stage 1에서의 모델을 불러움\n",
    "#     model.load_state_dict(torch.load(model_stage1_name), strict=True)\n",
    "#     model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "#     nn.init.xavier_uniform_(model.beta)\n",
    "#     model.beta_batchnorm = nn.Sequential()\n",
    "#     model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=True, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        for batch_idx, batch in enumerate(trainloader):       \n",
    "            org_input, pos_input, org_bow, pos_bow = batch\n",
    "            org_input = org_input.cuda(gpu_ids[0])\n",
    "            org_bow = org_bow.cuda(gpu_ids[0])\n",
    "            pos_input = pos_input.cuda(gpu_ids[0])\n",
    "            pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            org_dists, org_topic_logit = model.decode(org_input)\n",
    "            pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "            org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "            pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "            # reconstruction loss\n",
    "            # batchmean\n",
    "#             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "#             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "            \n",
    "#             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "#             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "            \n",
    "            recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * ((1-org_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * ((1-pos_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss *= 0.5\n",
    "\n",
    "            # consistency loss\n",
    "            pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "            cons_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution loss\n",
    "            # batchmean\n",
    "            distmatch_loss = dist_match_loss(torch.cat((org_topic, pos_topic), dim=0), dirichlet_alpha_2)\n",
    "            \n",
    "\n",
    "            loss = args.coeff_2_recon * recons_loss + \\\n",
    "                   args.coeff_2_cons * cons_loss + \\\n",
    "                   args.coeff_2_dist * distmatch_loss \n",
    "            \n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(cons_loss.item(), bsz)\n",
    "            rlosses.update(recons_loss.item(), bsz)\n",
    "            distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    print(\"------- Evaluation results -------\")\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(10, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)\n",
    "\n",
    "    topic_words_list = list(all_list.values())\n",
    "    now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "    results = get_topic_qualities(topic_words_list, palmetto_dir=args.palmetto_dir,\n",
    "                                  reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                                  filename=f'results/{now}.txt')\n",
    "    \n",
    "    if should_measure_hungarian:\n",
    "        topic_dist = torch.empty((0, n_topic))\n",
    "        model.eval()\n",
    "        evalloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "        non_empty_text_index = [i for i, text in enumerate(textData.data) if len(text) != 0]\n",
    "        assert len(finetuneds) == len(non_empty_text_index)\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(evalloader):\n",
    "                org_input, _, org_bow, __ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                topic_dist = torch.cat((topic_dist, org_topic.detach().cpu()), 0)\n",
    "        label_accuracy = measure_hungarian_score(\n",
    "                             topic_dist,\n",
    "                             [target for i, target in enumerate(textData.targets)\n",
    "                              if i in non_empty_text_index]\n",
    "                         )\n",
    "        results['label_match'] = label_accuracy\n",
    "\n",
    "    print(results)\n",
    "    print()\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic_N umass_wiki npmi_wiki   npmi_in uci_wiki    uci_in cp_wiki  \\\n",
      "0       20       None      None -0.005644     None -1.838765    None   \n",
      "1       20       None      None -0.010046     None -1.792833    None   \n",
      "2       20       None      None -0.012983     None -1.754633    None   \n",
      "3       20       None      None -0.012642     None -1.846005    None   \n",
      "4       20       None      None -0.046959     None -2.754191    None   \n",
      "\n",
      "    sim_w2v  diversity                   filename  \n",
      "0  0.181620      0.810  results/230809_235725.txt  \n",
      "1  0.194704      0.805  results/230810_000507.txt  \n",
      "2  0.184570      0.810  results/230810_001252.txt  \n",
      "3  0.195265      0.815  results/230810_002033.txt  \n",
      "4  0.188030      0.820  results/230810_002848.txt  \n",
      "mean\n",
      "topic_N       20.000000\n",
      "umass_wiki          NaN\n",
      "npmi_wiki           NaN\n",
      "npmi_in       -0.017655\n",
      "uci_wiki            NaN\n",
      "uci_in        -1.997285\n",
      "cp_wiki             NaN\n",
      "sim_w2v        0.188838\n",
      "diversity      0.812000\n",
      "dtype: float64\n",
      "std\n",
      "topic_N       0.000000\n",
      "umass_wiki         NaN\n",
      "npmi_wiki          NaN\n",
      "npmi_in       0.016642\n",
      "uci_wiki           NaN\n",
      "uci_in        0.424736\n",
      "cp_wiki            NaN\n",
      "sim_w2v       0.006056\n",
      "diversity     0.005701\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)\n",
    "print('mean')\n",
    "print(results_df.mean())\n",
    "print('std')\n",
    "print(results_df.std())\n",
    "\n",
    "if args.result_file is not None:\n",
    "    result_filename = f'results/{args.result_file}'\n",
    "else:\n",
    "    result_filename = f'results/{now}.tsv'\n",
    "\n",
    "results_df.to_csv(result_filename, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_N</th>\n",
       "      <th>umass_wiki</th>\n",
       "      <th>npmi_wiki</th>\n",
       "      <th>npmi_in</th>\n",
       "      <th>uci_wiki</th>\n",
       "      <th>uci_in</th>\n",
       "      <th>cp_wiki</th>\n",
       "      <th>sim_w2v</th>\n",
       "      <th>diversity</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.005644</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.838765</td>\n",
       "      <td>None</td>\n",
       "      <td>0.181620</td>\n",
       "      <td>0.810</td>\n",
       "      <td>results/230809_235725.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.010046</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.792833</td>\n",
       "      <td>None</td>\n",
       "      <td>0.194704</td>\n",
       "      <td>0.805</td>\n",
       "      <td>results/230810_000507.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.012983</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.754633</td>\n",
       "      <td>None</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.810</td>\n",
       "      <td>results/230810_001252.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.846005</td>\n",
       "      <td>None</td>\n",
       "      <td>0.195265</td>\n",
       "      <td>0.815</td>\n",
       "      <td>results/230810_002033.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.046959</td>\n",
       "      <td>None</td>\n",
       "      <td>-2.754191</td>\n",
       "      <td>None</td>\n",
       "      <td>0.188030</td>\n",
       "      <td>0.820</td>\n",
       "      <td>results/230810_002848.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_N umass_wiki npmi_wiki   npmi_in uci_wiki    uci_in cp_wiki  \\\n",
       "0       20       None      None -0.005644     None -1.838765    None   \n",
       "1       20       None      None -0.010046     None -1.792833    None   \n",
       "2       20       None      None -0.012983     None -1.754633    None   \n",
       "3       20       None      None -0.012642     None -1.846005    None   \n",
       "4       20       None      None -0.046959     None -2.754191    None   \n",
       "\n",
       "    sim_w2v  diversity                   filename  \n",
       "0  0.181620      0.810  results/230809_235725.txt  \n",
       "1  0.194704      0.805  results/230810_000507.txt  \n",
       "2  0.184570      0.810  results/230810_001252.txt  \n",
       "3  0.195265      0.815  results/230810_002033.txt  \n",
       "4  0.188030      0.820  results/230810_002848.txt  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end of documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "don",
   "language": "python",
   "name": "don"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
