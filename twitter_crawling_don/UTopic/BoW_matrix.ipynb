{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6e11d7",
   "metadata": {},
   "source": [
    "# BoW Matrix 구현\n",
    "1. 모든 플랫폼에서 주로 사용되는 단어(불용어)를 추출: TF-IDF\n",
    "2. 각 플랫폼에서 고유하게 사용되는 단어(jargon)를 추출: c-TF-IDF\n",
    "3. 불용어와 jargon을 제외한 나머지 단어들로 BoW 행렬을 구성\n",
    "4. matrix값 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1498f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65d0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "twitter_jargon_df = pd.read_csv('text_preprocess.csv', lineterminator=\"\\n\")\n",
    "twitter_jargon_df['content'].fillna('', inplace=True)\n",
    "tweets_jargon = twitter_jargon_df['content'].sample(50000)\n",
    "\n",
    "reddit_jargon_df = pd.read_csv('reddit_total_preprocessed_cleaned.csv')\n",
    "reddit_jargon_df['preprocessed_text'].fillna('', inplace=True)\n",
    "reddit_jargon = reddit_jargon_df['preprocessed_text'].sample(50000)\n",
    "\n",
    "youtube_jargon_df = pd.read_csv('testVideoMetaDataResult_Pre.csv')\n",
    "youtube_jargon_df['comment_Text'].fillna('', inplace=True)\n",
    "youtubes_jargon = youtube_jargon_df['comment_Text'].sample(50000)\n",
    "\n",
    "\n",
    "tweets_jargon_list = tweets_jargon.tolist()\n",
    "reddit_jargon_list = reddit_jargon.tolist()\n",
    "youtube_jargon_list = youtubes_jargon.tolist()\n",
    "\n",
    "documents = tweets_jargon_list + reddit_jargon_list + youtube_jargon_list\n",
    "labels = [\"Twitter\"] * len(tweets_jargon_list) + [\"Reddit\"] * len(reddit_jargon_list) + [\"YouTube\"] * len(youtube_jargon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7b5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=False, token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "tf_matrix = vectorizer.fit_transform(documents).toarray()  # we need array form for element-wise operations\n",
    "\n",
    "A = len(documents)\n",
    "f = np.count_nonzero(tf_matrix, axis=0)\n",
    "W_matrix = tf_matrix * np.log(1 + A / (f**2))\n",
    "\n",
    "twitter_data = W_matrix[:len(tweets_jargon_list)]\n",
    "reddit_data = W_matrix[len(tweets_jargon_list):len(tweets_jargon_list) + len(reddit_jargon_list)]\n",
    "youtube_data = W_matrix[len(tweets_jargon_list) + len(reddit_jargon_list):]\n",
    "\n",
    "sum_twitter = np.squeeze(np.asarray(twitter_data.sum(axis=0)))\n",
    "sum_reddit = np.squeeze(np.asarray(reddit_data.sum(axis=0)))\n",
    "sum_youtube = np.squeeze(np.asarray(youtube_data.sum(axis=0)))\n",
    "\n",
    "top_1000_twitter = [vectorizer.get_feature_names()[index] for index in sum_twitter.argsort()[-1000:][::-1]]\n",
    "top_1000_reddit = [vectorizer.get_feature_names()[index] for index in sum_reddit.argsort()[-1000:][::-1]]\n",
    "top_1000_youtube = [vectorizer.get_feature_names()[index] for index in sum_youtube.argsort()[-1000:][::-1]]\n",
    "\n",
    "# print(\"Top 100 Twitter Jargons:\", top_1000_twitter)\n",
    "# print(\"Top 100 Reddit Jargons:\", top_1000_reddit)\n",
    "# print(\"Top 100 YouTube Jargons:\", top_1000_youtube)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "491552e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "vectorizer = TfidfVectorizer(lowercase=False, token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "tfidf_matrix = vectorizer.fit_transform(documents).toarray()\n",
    "all_tfidf_values = np.sum(tfidf_matrix, axis=0)\n",
    "top_n = 10000  # 상위 1000개의 단어만 불용어로 추출\n",
    "stopwords_indices = all_tfidf_values.argsort()[-top_n:][::-1]\n",
    "stopwords = [vectorizer.get_feature_names()[index] for index in stopwords_indices]\n",
    "\n",
    "# platform jargons\n",
    "jargons = top_1000_twitter + top_1000_reddit + top_1000_youtube\n",
    "\n",
    "# BoW 행렬 구성\n",
    "exclude_words = stopwords + jargons\n",
    "bow_vectorizer = TfidfVectorizer(stop_words=exclude_words, lowercase=False, token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "tf_bow = np.sum(bow_matrix, axis=0)\n",
    "f_bow = np.count_nonzero(bow_matrix, axis=0)\n",
    "\n",
    "adjusted_bow_matrix = bow_matrix / (tf_bow * np.log(1 + A / (f_bow**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40541d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted BoW Matrix Shape: (150000, 72106)\n"
     ]
    }
   ],
   "source": [
    "print(\"Adjusted BoW Matrix Shape:\", adjusted_bow_matrix.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
