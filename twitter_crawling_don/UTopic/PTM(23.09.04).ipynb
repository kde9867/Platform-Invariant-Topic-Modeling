{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39179581",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 '+\\\n",
    "            '--dataset all --n-word 2000 --epochs-1 100 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 50 '+ \\\n",
    "            '--n-cluster 20 ' + \\\n",
    "            '--stage-1-ckpt trained_model/news_model_paraphrase-MiniLM-L6-v2_stage1_20t_2000w_99e.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4086fa2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40485da1",
   "metadata": {},
   "source": [
    "torch.cuda.is_available() #gpu 사용확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c46f00",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a189699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    # 각 stage에서의 epochs수 \n",
    "    parser.add_argument('--epochs-1', default=50, type=int,\n",
    "                        help='Number of training epochs for Stage 1')   \n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    #각 stage에서의 batch size\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    #data set정의 \n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    # 클러스터 수와 topic의 수는 20 (k==20)\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    # 단어vocabulary는 2000로 setting\n",
    "    parser.add_argument('--n-word', default=2000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1,2,3], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "   \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str,\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "#데이터들을 로드함(텍스트중에 none값을 제거/dataset_name에 따라 textData에 들어가는 내용이 달라짐)\n",
    "def data_load(dataset_name, sample_size=100000):\n",
    "    should_measure_hungarian = False\n",
    "    textData = []\n",
    "    \n",
    "    if dataset_name == 'twitter':\n",
    "        dataset = TwitterDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Twitter\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'reddit':\n",
    "        dataset = RedditDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Reddit\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'youtube':\n",
    "        dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"YouTube\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'all':\n",
    "        twitter_dataset = TwitterDataset(sample_size=sample_size)\n",
    "        reddit_dataset = RedditDataset(sample_size=sample_size)\n",
    "        youtube_dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        \n",
    "        # filtering NaN values\n",
    "        textData += [(text, \"Twitter\") for text in twitter_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"Reddit\") for text in reddit_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"YouTube\") for text in youtube_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name!\")\n",
    "    \n",
    "    return textData, should_measure_hungarian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6765b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b4564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a57f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2024bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(dataset_list, batch_size=64, shuffle=True, num_workers=0):\n",
    "    dataloaders = []\n",
    "\n",
    "    for dataset in dataset_list:\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        dataloaders.append(loader)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "def process_text_data(texts, all_texts):\n",
    "    # jargons as provided\n",
    "    jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])\n",
    "    \n",
    "    # Standard English stop words + jargons\n",
    "    exclude_words = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(jargons)\n",
    "    \n",
    "#     new_vocab = set(bow_vectorizer.get_feature_names()) - jargons\n",
    "    vectorizer = TfidfVectorizer(stop_words=list(exclude_words), \n",
    "                                 lowercase=False, \n",
    "                                 token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "\n",
    "    vectorizer.fit(all_texts) \n",
    "    bow_matrix_texts = vectorizer.transform(texts).toarray()\n",
    "    \n",
    "    # Normalize the matrix so that the sum is 1\n",
    "    epsilon = 1e-10  # To avoid division by zero\n",
    "    row_sums = bow_matrix_texts.sum(axis=1)\n",
    "    normalized_bow_matrix_texts = bow_matrix_texts / (row_sums[:, np.newaxis] + epsilon)\n",
    "    \n",
    "    print(\"Shape of the normalized BoW matrix:\", normalized_bow_matrix_texts.shape)\n",
    "\n",
    "    return normalized_bow_matrix_texts\n",
    "\n",
    "def compute_bow_batchwise(batch_texts, bow_matrix, full_texts):\n",
    "    positive_samples = []\n",
    "    \n",
    "    batch_adjusted_bow_matrix = process_text_data(batch_texts, full_texts)\n",
    "    \n",
    "    # batch_adjusted_bow_matrix와 bow_matrix 간의 코사인 유사도 계산\n",
    "    cosine_sim_matrix = cosine_similarity(batch_adjusted_bow_matrix, bow_matrix)\n",
    "    \n",
    "    # batch_texts에 있는 각 텍스트에 대해 full_texts에서 인덱스를 찾음\n",
    "    for i, batch_text in enumerate(batch_texts):\n",
    "        # full_texts에서 batch_text의 인덱스 찾기\n",
    "        self_index = full_texts.index(batch_text) if batch_text in full_texts else None\n",
    "        \n",
    "        # 찾은 인덱스의 유사도를 -1로 설정\n",
    "        if self_index is not None:\n",
    "            cosine_sim_matrix[i][self_index] = -1\n",
    "    \n",
    "#     # 자기 자신에 대한 유사도를 -1로 설정\n",
    "#     for i, batch_text_vector in enumerate(batch_adjusted_bow_matrix):\n",
    "#         self_index = np.where((batch_text_vector == bow_matrix).all(axis=1))[0]\n",
    "#         if len(self_index) > 0:\n",
    "#             cosine_sim_matrix[i][self_index] = -1\n",
    "\n",
    "    # 각 batch_text에 대해 가장 유사도가 높은 full_text의 인덱스를 찾음\n",
    "    for i in range(len(batch_adjusted_bow_matrix)):\n",
    "        positive_sample_index = np.argmax(cosine_sim_matrix[i])\n",
    "        positive_samples.append(positive_sample_index)\n",
    "\n",
    "    return positive_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75590ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Twitter\n",
      "Shape of the normalized BoW matrix: (8, 122907)\n",
      "Sample text 1:  Yes but ChatGPT democratises cheating Any student with an internet connection can do it for any topic without payment yet\n",
      "Sample Index 1: 56919\n",
      "Positive sample for text 1: i would say this technology democratises this ability some way\n",
      "Positive sample Index for text 1: 169946\n",
      "--------------------\n",
      "Sample text 2: I used Chat GPT to take a transcribed video using  and then give me  title ideas\n",
      "\n",
      " The titles were brilliant Wow just wow\n",
      "Sample Index 2: 9312\n",
      "Positive sample for text 2: I Love the title of the video\n",
      "Positive sample Index for text 2: 240960\n",
      "--------------------\n",
      "Sample text 3: Does Ketamine hold therapeutic value ChatGPT \n",
      "Sample Index 3: 80506\n",
      "Positive sample for text 3: ChatGPT Response on its value for Industry \n",
      "\n",
      "Read \n",
      "Positive sample Index for text 3: 14370\n",
      "--------------------\n",
      "Sample text 4: AI must be regulated says CTO of ChatGPT maker OpenAI\n",
      "\n",
      "Sample Index 4: 94259\n",
      "Positive sample for text 4: AI must be regulated says CTO of ChatGPT maker OpenAI  Fortune \n",
      "Positive sample Index for text 4: 92442\n",
      "--------------------\n",
      "Sample text 5: I keep writing imagine into my ChatGPT prompts\n",
      "Sample Index 5: 88757\n",
      "Positive sample for text 5:  Training him on writing ChatGPT prompts now\n",
      "Positive sample Index for text 5: 51754\n",
      "--------------------\n",
      "Sample text 6: in my mind all chatGPT answers are delivered by him \n",
      "Sample Index 6: 39282\n",
      "Positive sample for text 6:  This tweet delivered to you by ChatGPT \n",
      "Positive sample Index for text 6: 89763\n",
      "--------------------\n",
      "Sample text 7: How its wont be long before these programs are generating imagesvideo in real time When you interface with a program like ChatGPT it could be generating a visual imageavatar as for example\n",
      "Sample Index 7: 67732\n",
      "Positive sample for text 7:  Is this tweet an example of ChatGPT Who is really generating these tweets\n",
      "Positive sample Index for text 7: 75684\n",
      "--------------------\n",
      "Sample text 8: Yesterday Google unveiled a new AI tool called Bard which is an obvious response to the already popular ChatGPT They will roll out Bard to the public in the new few weeks\n",
      "Sample Index 8: 99127\n",
      "Positive sample for text 8:   Queried ChatGPT  a new AI tool by  about this Heres its response \n",
      "Positive sample Index for text 8: 37736\n",
      "--------------------\n",
      "--------------------------------------------------\n",
      "Platform: Reddit\n",
      "Shape of the normalized BoW matrix: (8, 122907)\n",
      "Sample text 1: how can i use this to search for help with coding problems\n",
      "Sample Index 1: 22309\n",
      "Positive sample for text 1: Use ChatGPT to help coding\n",
      "Positive sample Index for text 1: 36757\n",
      "--------------------\n",
      "Sample text 2: this is absolutely true something changed recently\n",
      "Sample Index 2: 95075\n",
      "Positive sample for text 2: yes absolutely true\n",
      "Positive sample Index for text 2: 153282\n",
      "--------------------\n",
      "Sample text 3: nope mine were also wiped out\n",
      "Sample Index 3: 47193\n",
      "Positive sample for text 3:  to  of Humanity will be wiped out before the Year \n",
      "Positive sample Index for text 3: 244978\n",
      "--------------------\n",
      "Sample text 4: hey mods can we get some sort of rule against blatant agendaposting with dubious prompts\n",
      "Sample Index 4: 23363\n",
      "Positive sample for text 4: Is there a rule that ChatGPT cant be Speaker\n",
      "Positive sample Index for text 4: 4312\n",
      "--------------------\n",
      "Sample text 5: what are some other examples\n",
      "Sample Index 5: 61720\n",
      "Positive sample for text 5: More examples of how ChatGPT is a Democrat \n",
      "\n",
      "\n",
      "Positive sample Index for text 5: 76156\n",
      "--------------------\n",
      "Sample text 6: worst than this\n",
      "Sample Index 6: 34434\n",
      "Positive sample for text 6: Cloudflare is the worst part of ChatGPT\n",
      "Positive sample Index for text 6: 84278\n",
      "--------------------\n",
      "Sample text 7: you insist ai cannot be concious but you cannot prove or disprove even another human being concious\n",
      "Sample Index 7: 27500\n",
      "Positive sample for text 7: yea and why do not you just argue that particles are concious and everything is concious just because there is not sufficent evidence to disprove that something is concious does not mean that it is concious and it is most likely not from the opinions of most forefront ai safety researcher\n",
      "Positive sample Index for text 7: 145005\n",
      "--------------------\n",
      "Sample text 8: well duh\n",
      "Sample Index 8: 18968\n",
      "Positive sample for text 8: duh\n",
      "Positive sample Index for text 8: 180447\n",
      "--------------------\n",
      "--------------------------------------------------\n",
      "Platform: YouTube\n",
      "Shape of the normalized BoW matrix: (8, 122907)\n",
      "Sample text 1:   without the creator of algebra muslim the Christians would not have been able to do alot of the mathematics in the first place\n",
      "Sample Index 1: 46547\n",
      "Positive sample for text 1: chatgpt is a muslim\n",
      "Positive sample Index for text 1: 134535\n",
      "--------------------\n",
      "Sample text 2: Everytime I hear about what the EU is doing in contrast to the US i just shake my head likei gotta move there before thos country kills me\n",
      "Sample Index 2: 94953\n",
      "Positive sample for text 2: ChatGPT Amendment Shows the EU is Regulating by Outrage \n",
      "Positive sample Index for text 2: 9988\n",
      "--------------------\n",
      "Sample text 3: This has to be one of if not the most powerful vid youve done since I started following you Ive had the same experience here with our new friend\n",
      "Sample Index 3: 17900\n",
      "Positive sample for text 3: I have a new friend now ChatGPT \n",
      "Positive sample Index for text 3: 56224\n",
      "--------------------\n",
      "Sample text 4: I personally believe tech like this with its dataset should be public but whatever I kinda get why they dont but still\n",
      "Sample Index 4: 46904\n",
      "Positive sample for text 4:  I only believe Chatgpt\n",
      "Positive sample Index for text 4: 95184\n",
      "--------------------\n",
      "Sample text 5: I think delirious cartoon should go into the alpha beta show as like a video game and planktonizer can have the diaper babies as his minions and he could be trying to kill the alpha beta team and delirious bob and friends\n",
      "Sample Index 5: 93642\n",
      "Positive sample for text 5: very alpha of you\n",
      "Positive sample Index for text 5: 177621\n",
      "--------------------\n",
      "Sample text 6:  Marie Knapp But many human partners dont either or sometimes are actively malicious\n",
      "Sample Index 6: 50461\n",
      "Positive sample for text 6: probably even malicious\n",
      "Positive sample Index for text 6: 188649\n",
      "--------------------\n",
      "Sample text 7: I use ChatGPT in the mornings UTC   US is asleep  Into the afternoon and evenings becomes unusable  they will charge soon and freeloaders will disappear\n",
      "Sample Index 7: 38333\n",
      "Positive sample for text 7: US Top News  Thu   Dec    UTC  What is ChatGPT and how does the AI work \n",
      "Positive sample Index for text 7: 35904\n",
      "--------------------\n",
      "Sample text 8: Who got an ad for chatgpt \n",
      "Sample Index 8: 26797\n",
      "Positive sample for text 8: I got a ChatGPT ad before the video\n",
      "Positive sample Index for text 8: 279290\n",
      "--------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "##상수를 따로 뺌\n",
    "platform_names = [\"Twitter\", \"Reddit\", \"YouTube\"]\n",
    "\n",
    "def main():\n",
    "    # 각 데이터셋 초기화(data.py에서 확인 가능)\n",
    "    twitter_ds = TwitterDataset()\n",
    "    reddit_ds = RedditDataset()\n",
    "    youtube_ds = YoutubeDataset()\n",
    "\n",
    "    platform_datasets = [twitter_ds, reddit_ds, youtube_ds]\n",
    "    all_texts = twitter_ds.texts + reddit_ds.texts + youtube_ds.texts\n",
    "    \n",
    "    # 위에서 한 전처리와 같이 처리를 하여 벡터화 시킴\n",
    "    exclude_words = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(jargons)\n",
    "\n",
    "    bow_vectorizer = TfidfVectorizer(stop_words=list(exclude_words), lowercase=False, token_pattern=r'\\b([A-Z]+|[a-z]+)\\b')\n",
    "    bow_vectorizer.fit(all_texts)\n",
    "\n",
    "    # Transform all_texts to create a BoW matrix\n",
    "    bow_matrix = bow_vectorizer.transform(all_texts).toarray()\n",
    "\n",
    "    # Create dataloaders(데이터로더를 사용하여 배치사이즈만큼 각 플랫폼의 데이터를 가져옴)\n",
    "    dataloaders = create_dataloaders(platform_datasets, batch_size=8)\n",
    "    \n",
    "    #테스트 결과 확인\n",
    "    for platform_idx, dataloader in enumerate(dataloaders):\n",
    "        # 첫 번째 배치만 가져옴\n",
    "        batch = next(iter(dataloader))\n",
    "        texts, targets = batch\n",
    "\n",
    "        print(f\"Platform: {platform_names[platform_idx]}\")\n",
    "\n",
    "        positive_sample_indices = compute_bow_batchwise(texts, bow_matrix, all_texts)\n",
    "\n",
    "        for sample_idx, positive_sample_index in enumerate(positive_sample_indices):\n",
    "            print(f\"Sample text {sample_idx + 1}: {texts[sample_idx]}\")\n",
    "            print(f\"Sample Index {sample_idx + 1}: {targets[sample_idx]}\")\n",
    "            positive_text = all_texts[positive_sample_index]\n",
    "            print(f\"Positive sample for text {sample_idx + 1}: {positive_text}\")\n",
    "            print(f\"Positive sample Index for text {sample_idx + 1}: {positive_sample_index}\")\n",
    "            print('-' * 20)\n",
    "\n",
    "        print('-' * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53754a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for platform_idx, dataloader in enumerate(dataloaders):\n",
    "        # 첫 번째 배치만 가져옴\n",
    "    batch = next(iter(dataloader))\n",
    "    texts, targets = batch\n",
    "\n",
    "    print(f\"Platform: {platform_names[platform_idx]}\")\n",
    "        \n",
    "    positive_sample_indices = compute_bow_batchwise(texts, bow_matrix, all_texts)\n",
    "\n",
    "    for sample_idx, positive_sample_index in enumerate(positive_sample_indices):\n",
    "        print(f\"Sample text {sample_idx + 1}: {texts[sample_idx]}\")\n",
    "        print(f\"Sample Index {sample_idx + 1}: {targets[sample_idx]}\")\n",
    "        positive_text = all_texts[positive_sample_index]\n",
    "        print(f\"Positive sample for text {sample_idx + 1}: {positive_text}\")\n",
    "        print(f\"Positive sample Index for text {sample_idx + 1}: {positive_sample_index}\")\n",
    "        print('-' * 20)\n",
    "        \n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e805ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for platform_idx, dataloader in enumerate(dataloaders):\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            texts, targets = batch\n",
    "            \n",
    "            print(f\"Platform: {platform_names[platform_idx]}\")\n",
    "            print(f\"Batch index: {batch_idx}\")\n",
    "            print(\"Sample text:\", texts[0])\n",
    "            print(\"Sample Index:\", targets[0])\n",
    "            print('-' * 50)\n",
    "            \n",
    "            positive_sample_indices = compute_bow_batchwise(texts, bow_matrix, all_texts)\n",
    "\n",
    "            for sample_idx, positive_sample_index in enumerate(positive_sample_indices):\n",
    "                positive_text = all_texts[positive_sample_index]\n",
    "                print(f\"Positive sample for text {sample_idx}: {positive_text}\")\n",
    "\n",
    "            print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
