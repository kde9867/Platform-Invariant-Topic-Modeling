{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c355e0",
   "metadata": {},
   "source": [
    "# Setting(dataset, parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a009520",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 ' + \\\n",
    "            '--dataset all --n-word 30000 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 ' + \\\n",
    "            '--n-cluster 20 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c1d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2397191/2934782487.py:23: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from utils import AverageMeter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "import scipy.stats\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset, BertDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "\n",
    "from data import BertDataset, Stage2Dataset\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ab2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bce59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "\n",
    "    #각 stage에서의 batch size\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    #data set정의 \n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    # 클러스터 수와 topic의 수는 20 (k==20)\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    # 단어vocabulary는 2000로 setting\n",
    "    parser.add_argument('--n-word', default=30000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1,2,3], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "   \n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8d1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "# textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5ae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"모든 랜덤 요소에 대한 시드를 고정합니다.\"\"\"\n",
    "    random.seed(seed_value)  \n",
    "    np.random.seed(seed_value)  \n",
    "    torch.manual_seed(seed_value)  \n",
    "    if torch.cuda.is_available():  \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56d5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a80e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 각 데이터셋 초기화(data.py에서 확인 가능)\n",
    "twitter_ds = TwitterDataset()\n",
    "reddit_ds = RedditDataset()\n",
    "youtube_ds = YoutubeDataset()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 전체 데이터를 train과 test로 7:3으로 분할\n",
    "train_twitter_texts, test_twitter_texts, train_twitter_labels, test_twitter_labels = train_test_split(\n",
    "    twitter_ds.texts, twitter_ds.labels, train_size=0.7, random_state=41)\n",
    "\n",
    "train_reddit_texts, test_reddit_texts, train_reddit_labels, test_reddit_labels = train_test_split(\n",
    "    reddit_ds.texts, reddit_ds.labels, train_size=0.7, random_state=41)\n",
    "\n",
    "train_youtube_texts, test_youtube_texts, train_youtube_labels, test_youtube_labels = train_test_split(\n",
    "    youtube_ds.texts, youtube_ds.labels, train_size=0.7, random_state=41)\n",
    "\n",
    "# train 데이터를 다시 train과 val로 9:1로 분할\n",
    "train_twitter_texts, val_twitter_texts, train_twitter_labels, val_twitter_labels = train_test_split(\n",
    "    train_twitter_texts, train_twitter_labels, test_size=0.1, random_state=41)\n",
    "\n",
    "train_reddit_texts, val_reddit_texts, train_reddit_labels, val_reddit_labels = train_test_split(\n",
    "    train_reddit_texts, train_reddit_labels, test_size=0.1, random_state=41)\n",
    "\n",
    "train_youtube_texts, val_youtube_texts, train_youtube_labels, val_youtube_labels = train_test_split(\n",
    "    train_youtube_texts, train_youtube_labels, test_size=0.1, random_state=41)\n",
    "\n",
    "# 각 데이터의 플랫폼을 합침\n",
    "train_total_label = train_twitter_labels + train_reddit_labels + train_youtube_labels\n",
    "train_total_text_list = train_twitter_texts + train_reddit_texts + train_youtube_texts\n",
    "\n",
    "val_total_label = val_twitter_labels + val_reddit_labels + val_youtube_labels\n",
    "val_total_text_list = val_twitter_texts + val_reddit_texts + val_youtube_texts\n",
    "\n",
    "test_total_label = test_twitter_labels + test_reddit_labels + test_youtube_labels\n",
    "test_total_text_list = test_twitter_texts + test_reddit_texts + test_youtube_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9c2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 37800/37800 [00:53<00:00, 712.15it/s]\n",
      "100%|██████████| 4200/4200 [00:04<00:00, 855.18it/s]\n",
      "100%|██████████| 18000/18000 [00:23<00:00, 770.20it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=train_total_text_list, platform_label = train_total_label, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "valds = BertDataset(bert=bert_name, text_list=val_total_text_list, platform_label = val_total_label, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds = BertDataset(bert=bert_name, text_list=test_total_text_list, platform_label = test_total_label, N_word=n_word, vectorizer=None, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba29252",
   "metadata": {},
   "source": [
    "# 원본 데이터 Mean pooling 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2229cd",
   "metadata": {},
   "source": [
    "# mean_pooling 함수 정의\n",
    "def mean_pooling(embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb71fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 문장의 평균 풀링 임베딩 계산\n",
    "trainds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in trainds:\n",
    "    trainds_embeddings.append(pooled_embedding)\n",
    "\n",
    "# 평균 풀링된 임베딩 매트릭스 생성\n",
    "train_mean_pooled_embeddings = torch.stack(trainds_embeddings)\n",
    "\n",
    "# 모든 문장의 평균 풀링 임베딩 계산\n",
    "valds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in valds:\n",
    "    valds_embeddings.append(pooled_embedding)\n",
    "\n",
    "# 평균 풀링된 임베딩 매트릭스 생성\n",
    "val_mean_pooled_embeddings = torch.stack(valds_embeddings)\n",
    "\n",
    "# 모든 문장의 평균 풀링 임베딩 계산\n",
    "testds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in testds:\n",
    "    testds_embeddings.append(pooled_embedding)\n",
    "\n",
    "# 평균 풀링된 임베딩 매트릭스 생성\n",
    "test_mean_pooled_embeddings = torch.stack(testds_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd08ab9",
   "metadata": {},
   "source": [
    "# Re_fornulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85826fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97909bc",
   "metadata": {},
   "source": [
    "# Get pos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89a726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_cosine_similarity_indices(mean_pooled_embeddings, batch_size=500):\n",
    "    n_rows = mean_pooled_embeddings.shape[0]\n",
    "    max_similarity_indices = np.zeros(n_rows, dtype=np.int64)\n",
    "\n",
    "    for start_idx in range(0, n_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_rows)\n",
    "        batch_data = mean_pooled_embeddings[start_idx:end_idx]\n",
    "\n",
    "        batch_similarity = cosine_similarity(batch_data, mean_pooled_embeddings)\n",
    "\n",
    "        # 자기 자신과의 유사도를 -1로 설정\n",
    "        for i, original_idx in enumerate(range(start_idx, end_idx)):\n",
    "            batch_similarity[i, original_idx] = -1\n",
    "\n",
    "        # 각 행에서 가장 큰 값을 가진 인덱스 찾기\n",
    "        max_indices = np.argmax(batch_similarity, axis=1)\n",
    "        max_similarity_indices[start_idx:end_idx] = max_indices\n",
    "\n",
    "        #logger.info(f\"{end_idx}/{n_rows} 데이터 처리 완료\")\n",
    "\n",
    "    return max_similarity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3016fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_pooled_embeddings를 2차원 형태로 변환\n",
    "train_mean_pooled_embeddings_2d = train_mean_pooled_embeddings.squeeze()\n",
    "val_mean_pooled_embeddings_2d = val_mean_pooled_embeddings.squeeze()\n",
    "test_mean_pooled_embeddings_2d = test_mean_pooled_embeddings.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc96d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 매트릭스 계산\n",
    "train_similarity_matrix = compute_max_cosine_similarity_indices(train_mean_pooled_embeddings_2d)\n",
    "val_similarity_matrix = compute_max_cosine_similarity_indices(val_mean_pooled_embeddings_2d)\n",
    "test_similarity_matrix = compute_max_cosine_similarity_indices(test_mean_pooled_embeddings_2d)  # 테스트 데이터셋에 대해서도 동일하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "745200ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37800,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12fdaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3abcb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37800/37800 [00:31<00:00, 1196.41it/s]\n",
      "100%|██████████| 37800/37800 [19:05<00:00, 32.98it/s]\n",
      "100%|██████████| 4200/4200 [00:02<00:00, 1548.81it/s]\n",
      "100%|██████████| 4200/4200 [02:04<00:00, 33.85it/s]\n",
      "100%|██████████| 18000/18000 [00:14<00:00, 1275.59it/s]\n",
      "100%|██████████| 18000/18000 [08:57<00:00, 33.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, train_similarity_matrix, n_word, lemmatize=True)\n",
    "\n",
    "valfinetuneds = Stage2Dataset(model.encoder, valds, val_similarity_matrix, n_word, lemmatize=True) \n",
    "\n",
    "testfinetuneds = Stage2Dataset(model.encoder, testds, test_similarity_matrix, n_word, lemmatize=True) \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06d9e537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37800"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(finetuneds.embedding_list)\n",
    "len(finetuneds.bow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daded5c",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a3e6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9818d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_measure_hungarian = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86eb85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d91e4",
   "metadata": {},
   "source": [
    "# Seperate Platform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df7a3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from collections import defaultdict\n",
    "\n",
    "# 커스텀 샘플러 구현\n",
    "class PlatformSampler(Sampler):\n",
    "    def __init__(self, dataset, platform_label):\n",
    "        self.indices = [i for i, label in enumerate(dataset.platform_label_list) if label == platform_label]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e66859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 플랫폼별 데이터로더 생성 함수\n",
    "def create_platform_dataloader(dataset, platform_label, batch_size=32):\n",
    "    sampler = PlatformSampler(dataset, platform_label)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    return dataloader\n",
    "\n",
    "def create_platform_dataloader(dataset, platform_label, batch_size=32, num_workers=0):\n",
    "    # PlatformSampler는 플랫폼 라벨에 따라 데이터셋에서 샘플링하는 커스텀 샘플러입니다.\n",
    "    # 이 샘플러는 여기서 정의하거나, 필요에 따라 다른 샘플링 로직을 구현할 수 있습니다.\n",
    "    sampler = PlatformSampler(dataset, platform_label)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01506b05",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd17d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 9.18218 - dist: 0.15830 - cons: -0.09501\n",
      "Epoch-1 / recon: 8.90246 - dist: 0.14250 - cons: -0.11963\n",
      "Epoch-2 / recon: 8.75436 - dist: 0.13385 - cons: -0.13652\n",
      "Epoch-3 / recon: 8.65146 - dist: 0.12876 - cons: -0.14787\n",
      "Epoch-4 / recon: 8.57034 - dist: 0.12491 - cons: -0.15582\n",
      "Epoch-5 / recon: 8.50191 - dist: 0.12191 - cons: -0.16149\n",
      "Epoch-6 / recon: 8.44209 - dist: 0.11951 - cons: -0.16552\n",
      "Epoch-7 / recon: 8.38847 - dist: 0.11777 - cons: -0.16833\n",
      "Epoch-8 / recon: 8.33908 - dist: 0.11635 - cons: -0.17019\n",
      "Epoch-9 / recon: 8.29337 - dist: 0.11545 - cons: -0.17123\n",
      "Epoch-10 / recon: 8.25057 - dist: 0.11504 - cons: -0.17156\n",
      "Epoch-11 / recon: 8.21015 - dist: 0.11517 - cons: -0.17135\n",
      "Epoch-12 / recon: 8.17176 - dist: 0.11559 - cons: -0.17083\n",
      "Epoch-13 / recon: 8.13523 - dist: 0.11630 - cons: -0.16991\n",
      "Epoch-14 / recon: 8.10010 - dist: 0.11723 - cons: -0.16874\n",
      "Epoch-15 / recon: 8.06629 - dist: 0.11842 - cons: -0.16746\n",
      "Epoch-16 / recon: 8.03383 - dist: 0.11972 - cons: -0.16605\n",
      "Epoch-17 / recon: 8.00253 - dist: 0.12119 - cons: -0.16457\n",
      "Epoch-18 / recon: 7.97225 - dist: 0.12276 - cons: -0.16305\n",
      "Epoch-19 / recon: 7.94286 - dist: 0.12437 - cons: -0.16149\n",
      "Epoch-20 / recon: 7.91439 - dist: 0.12601 - cons: -0.15994\n",
      "Epoch-21 / recon: 7.88684 - dist: 0.12771 - cons: -0.15840\n",
      "Epoch-22 / recon: 7.86003 - dist: 0.12937 - cons: -0.15687\n",
      "Epoch-23 / recon: 7.83410 - dist: 0.13103 - cons: -0.15536\n",
      "Epoch-24 / recon: 7.80892 - dist: 0.13279 - cons: -0.15388\n",
      "Epoch-25 / recon: 7.78441 - dist: 0.13448 - cons: -0.15243\n",
      "Epoch-26 / recon: 7.76064 - dist: 0.13618 - cons: -0.15103\n",
      "Epoch-27 / recon: 7.73755 - dist: 0.13783 - cons: -0.14966\n",
      "Epoch-28 / recon: 7.71507 - dist: 0.13945 - cons: -0.14834\n",
      "Epoch-29 / recon: 7.69317 - dist: 0.14104 - cons: -0.14705\n",
      "Epoch-30 / recon: 7.67196 - dist: 0.14261 - cons: -0.14581\n",
      "Epoch-31 / recon: 7.65131 - dist: 0.14415 - cons: -0.14461\n",
      "Epoch-32 / recon: 7.63118 - dist: 0.14569 - cons: -0.14344\n",
      "Epoch-33 / recon: 7.61153 - dist: 0.14722 - cons: -0.14231\n",
      "Epoch-34 / recon: 7.59247 - dist: 0.14872 - cons: -0.14121\n",
      "Epoch-35 / recon: 7.57383 - dist: 0.15013 - cons: -0.14016\n",
      "Epoch-36 / recon: 7.55574 - dist: 0.15152 - cons: -0.13914\n",
      "Epoch-37 / recon: 7.53808 - dist: 0.15294 - cons: -0.13815\n",
      "Epoch-38 / recon: 7.52088 - dist: 0.15430 - cons: -0.13720\n",
      "Epoch-39 / recon: 7.50412 - dist: 0.15560 - cons: -0.13628\n",
      "Epoch-40 / recon: 7.48779 - dist: 0.15689 - cons: -0.13538\n",
      "Epoch-41 / recon: 7.47185 - dist: 0.15815 - cons: -0.13451\n",
      "Epoch-42 / recon: 7.45628 - dist: 0.15939 - cons: -0.13367\n",
      "Epoch-43 / recon: 7.44110 - dist: 0.16059 - cons: -0.13286\n",
      "Epoch-44 / recon: 7.42631 - dist: 0.16181 - cons: -0.13207\n",
      "Epoch-45 / recon: 7.41183 - dist: 0.16297 - cons: -0.13131\n",
      "Epoch-46 / recon: 7.39770 - dist: 0.16410 - cons: -0.13057\n",
      "Epoch-47 / recon: 7.38389 - dist: 0.16519 - cons: -0.12985\n",
      "Epoch-48 / recon: 7.37042 - dist: 0.16626 - cons: -0.12915\n",
      "Epoch-49 / recon: 7.35725 - dist: 0.16730 - cons: -0.12848\n",
      "Epoch-50 / recon: 7.34440 - dist: 0.16833 - cons: -0.12783\n",
      "Epoch-51 / recon: 7.33184 - dist: 0.16933 - cons: -0.12719\n",
      "Epoch-52 / recon: 7.31956 - dist: 0.17030 - cons: -0.12658\n",
      "Epoch-53 / recon: 7.30757 - dist: 0.17127 - cons: -0.12598\n",
      "Epoch-54 / recon: 7.29580 - dist: 0.17223 - cons: -0.12540\n",
      "Epoch-55 / recon: 7.28429 - dist: 0.17315 - cons: -0.12484\n",
      "Epoch-56 / recon: 7.27303 - dist: 0.17404 - cons: -0.12428\n",
      "Epoch-57 / recon: 7.26201 - dist: 0.17493 - cons: -0.12374\n",
      "Epoch-58 / recon: 7.25123 - dist: 0.17578 - cons: -0.12322\n",
      "Epoch-59 / recon: 7.24069 - dist: 0.17661 - cons: -0.12271\n",
      "Epoch-60 / recon: 7.23033 - dist: 0.17745 - cons: -0.12222\n",
      "Epoch-61 / recon: 7.22020 - dist: 0.17826 - cons: -0.12174\n",
      "Epoch-62 / recon: 7.21024 - dist: 0.17906 - cons: -0.12127\n",
      "Epoch-63 / recon: 7.20049 - dist: 0.17982 - cons: -0.12082\n",
      "Epoch-64 / recon: 7.19094 - dist: 0.18058 - cons: -0.12037\n",
      "Epoch-65 / recon: 7.18158 - dist: 0.18130 - cons: -0.11994\n",
      "Epoch-66 / recon: 7.17240 - dist: 0.18203 - cons: -0.11951\n",
      "Epoch-67 / recon: 7.16336 - dist: 0.18275 - cons: -0.11910\n",
      "Epoch-68 / recon: 7.15449 - dist: 0.18346 - cons: -0.11870\n",
      "Epoch-69 / recon: 7.14581 - dist: 0.18415 - cons: -0.11831\n",
      "Epoch-70 / recon: 7.13730 - dist: 0.18481 - cons: -0.11792\n",
      "Epoch-71 / recon: 7.12893 - dist: 0.18546 - cons: -0.11754\n",
      "Epoch-72 / recon: 7.12071 - dist: 0.18612 - cons: -0.11718\n",
      "Epoch-73 / recon: 7.11263 - dist: 0.18676 - cons: -0.11682\n",
      "Epoch-74 / recon: 7.10470 - dist: 0.18738 - cons: -0.11647\n",
      "Epoch-75 / recon: 7.09691 - dist: 0.18798 - cons: -0.11613\n",
      "Epoch-76 / recon: 7.08928 - dist: 0.18858 - cons: -0.11579\n",
      "Epoch-77 / recon: 7.08175 - dist: 0.18916 - cons: -0.11546\n",
      "Epoch-78 / recon: 7.07438 - dist: 0.18974 - cons: -0.11514\n",
      "Epoch-79 / recon: 7.06711 - dist: 0.19030 - cons: -0.11483\n",
      "Epoch-80 / recon: 7.05997 - dist: 0.19084 - cons: -0.11452\n",
      "Epoch-81 / recon: 7.05294 - dist: 0.19141 - cons: -0.11422\n",
      "Epoch-82 / recon: 7.04603 - dist: 0.19194 - cons: -0.11392\n",
      "Epoch-83 / recon: 7.03925 - dist: 0.19247 - cons: -0.11364\n",
      "Epoch-84 / recon: 7.03257 - dist: 0.19299 - cons: -0.11335\n",
      "Epoch-85 / recon: 7.02599 - dist: 0.19352 - cons: -0.11308\n",
      "Epoch-86 / recon: 7.01952 - dist: 0.19403 - cons: -0.11281\n",
      "Epoch-87 / recon: 7.01316 - dist: 0.19452 - cons: -0.11254\n",
      "Epoch-88 / recon: 7.00691 - dist: 0.19500 - cons: -0.11228\n",
      "Epoch-89 / recon: 7.00076 - dist: 0.19548 - cons: -0.11203\n",
      "Epoch-90 / recon: 6.99468 - dist: 0.19596 - cons: -0.11178\n",
      "Epoch-91 / recon: 6.98871 - dist: 0.19642 - cons: -0.11153\n",
      "Epoch-92 / recon: 6.98283 - dist: 0.19687 - cons: -0.11129\n",
      "Epoch-93 / recon: 6.97705 - dist: 0.19732 - cons: -0.11105\n",
      "Epoch-94 / recon: 6.97132 - dist: 0.19775 - cons: -0.11082\n",
      "Epoch-95 / recon: 6.96569 - dist: 0.19819 - cons: -0.11059\n",
      "Epoch-96 / recon: 6.96015 - dist: 0.19861 - cons: -0.11037\n",
      "Epoch-97 / recon: 6.95468 - dist: 0.19905 - cons: -0.11015\n",
      "Epoch-98 / recon: 6.94931 - dist: 0.19946 - cons: -0.10993\n",
      "Epoch-99 / recon: 6.94400 - dist: 0.19986 - cons: -0.10972\n",
      "Best Epoch: 98 with NPMI: 0.6160497931341168\n",
      "------- Evaluation results -------\n",
      "topic-0 ['chromosome', 'lmaoooo', 'ghatgpt', 'intelligenceassisted', 'misinformation', 'intelligencequot', 'arithmetic', 'discriminated', 'blatant', 'hoax']\n",
      "topic-1 ['asimov', 'laughed', 'ibs', 'likke', 'juha', 'familiarity', 'turing', 'sideeffect', 'measurekill', 'peoplean']\n",
      "topic-2 ['coin', 'memeai', 'langley', 'bingo', 'tifu', 'largest', 'businessdor', 'fluctuation', 'market', 'proactive']\n",
      "topic-3 ['chatgpt', 'ded', 'desproges', 'evolution', 'chatgptget', 'film', 'obfuscation', 'mung', 'unclear', 'spectacularly']\n",
      "topic-4 ['bot', 'comment', 'discord', 'textdavinci', 'moderator', 'performed', 'compose', 'server', 'repetitive', 'friendly']\n",
      "topic-5 ['ghostwriter', 'blogging', 'montre', 'advertisement', 'mmhmm', 'shrek', 'bunched', 'punctuated', 'copied', 'pdf']\n",
      "topic-6 ['coexist', 'chatbots', 'robot', 'antichrist', 'dhum', 'probatzen', 'robotic', 'bingbot', 'alias', 'christ']\n",
      "topic-7 ['intelligenceassisted', 'money', 'school', 'math', 'universal', 'job', 'teacher', 'income', 'exam', 'people']\n",
      "topic-8 ['doxxing', 'limiting', 'caution', 'moderation', 'reinforcement', 'distraction', 'influencers', 'exploring', 'quotsolvequot', 'privatised']\n",
      "topic-9 ['gtcyanide', 'condiment', 'muchhh', 'piss', 'nutty', 'sara', 'omfg', 'hilarious', 'sweet', 'huggs']\n",
      "topic-10 ['teleporting', 'laughed', 'beat', 'spawn', 'gtplayground', 'fork', 'chaz', 'defeated', 'nxc', 'tik']\n",
      "topic-11 ['gurinchii', 'okaa', 'cheii', 'lmfaooo', 'sonic', 'ew', 'amp', 'chilam', 'opekkhai', 'woooosh']\n",
      "topic-12 ['chatgptget', 'knell', 'lawtech', 'inventor', 'digitalart', 'shaped', 'aitechnology', 'chatgptpredicting', 'intellectual', 'revolutionise']\n",
      "topic-13 ['reigned', 'momma', 'sooner', 'cola', 'coca', 'replaced', 'leadingtech', 'accountant', 'beaver', 'dooes']\n",
      "topic-14 ['ded', 'imminent', 'singularity', 'kiiiing', 'sacrificed', 'fascist', 'feminism', 'feminist', 'liberalism', 'incompetence']\n",
      "topic-15 ['agreed', 'defenetly', 'assessment', 'urged', 'humanistic', 'meritocracy', 'apostrophe', 'circlebr', 'corrects', 'steep']\n",
      "topic-16 ['iamverysmart', 'copypasta', 'pythoning', 'itwasjustajokebro', 'gptjailbreaks', 'recursion', 'dontbrbrthat', 'lazymode', 'quicky', 'mj']\n",
      "topic-17 ['clientele', 'stc', 'kyle', 'denismarktrade', 'deduct', 'msges', 'refunded', 'costco', 'notify', 'dmail']\n",
      "topic-18 ['muppet', 'kekw', 'fungi', 'poopy', 'sacrificed', 'oenai', 'burp', 'muffin', 'flesh', 'wojak']\n",
      "topic-19 ['ptbr', 'chatbing', 'citas', 'cheyi', 'govinda', 'cryptotab', 'poadunga', 'download', 'banate', 'incroyable']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import chi2_contingency \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from coherence import get_topic_coherence\n",
    "\n",
    "args.stage_2_repeat = 1\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    ##수정\n",
    "    twitter_trainloader = create_platform_dataloader(finetuneds, 'twitter', batch_size=bsz, num_workers=0)\n",
    "    reddit_trainloader = create_platform_dataloader(finetuneds, 'reddit', batch_size=bsz, num_workers=0)\n",
    "    youtube_trainloader = create_platform_dataloader(finetuneds, 'youtube', batch_size=bsz, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    ##\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    # 최적 epoch 추적을 위한 변수 초기화\n",
    "    best_npmi = -1\n",
    "    best_epoch = 0\n",
    "    best_model_state = None  # 모델 상태를 저장하기 위한 변수\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        # 각 플랫폼별 DataLoader를 순회\n",
    "        for platform_loader in [twitter_trainloader, reddit_trainloader, youtube_trainloader]:\n",
    "            for batch_idx, batch in enumerate(platform_loader): \n",
    "                _, org_input, pos_input, org_bow, pos_bow, _ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_bow = org_bow.cuda(gpu_ids[0])\n",
    "                pos_input = pos_input.cuda(gpu_ids[0])\n",
    "                pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "                batch_size = org_input.size(0) #org_input_ids.size(0)\n",
    "\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "                # reconstruction loss\n",
    "                # batchmean\n",
    "    #             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "    #             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "\n",
    "    #             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "    #             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "\n",
    "                # 텐서 크기 맞춰줌\n",
    "                org_dists = org_dists[:, :org_bow.size(1)]\n",
    "                pos_dists = pos_dists[:, :pos_bow.size(1)]\n",
    "\n",
    "                recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * (1-org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * (1-pos_bow), axis=1), axis=0)\n",
    "                recons_loss *= 0.5\n",
    "\n",
    "                # consistency loss\n",
    "                pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "                cons_loss = -pos_sim.mean()\n",
    "\n",
    "                # distribution loss\n",
    "                # batchmean\n",
    "    #             distmatch_loss = dist_match_loss(torch.cat((org_topic), dim=0), dirichlet_alpha_2)\n",
    "                distmatch_loss = dist_match_loss(torch.cat((org_topic,), dim=0), dirichlet_alpha_2)\n",
    "\n",
    "\n",
    "                loss = args.coeff_2_recon * recons_loss + \\\n",
    "                       args.coeff_2_cons * cons_loss + \\\n",
    "                       args.coeff_2_dist * distmatch_loss \n",
    "\n",
    "                losses.update(loss.item(), bsz)\n",
    "                closses.update(cons_loss.item(), bsz)\n",
    "                rlosses.update(recons_loss.item(), bsz)\n",
    "                distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "        # Epoch 마다 실행\n",
    "        model.eval()\n",
    "\n",
    "        # 각 토픽에 대한 상위 10개 단어 추출\n",
    "        top_words_per_topic = {}\n",
    "        for topic_idx in range(model.N_topic):\n",
    "            top_words_indices = model.beta[topic_idx].topk(10).indices\n",
    "            top_words = [vocab_dict_reverse[idx.item()] for idx in top_words_indices]\n",
    "            top_words_per_topic[topic_idx] = top_words\n",
    "            \n",
    "        reference_corpus=[doc.split() for doc in valds.preprocess_ctm(valds.nonempty_text)]\n",
    "        topic_words_list = list(top_words_per_topic.values())\n",
    "        result = get_topic_coherence(topic_words_list, reference_corpus)\n",
    "        avg_npmi = result['NPMI']\n",
    "\n",
    "        # 최적의 NPMI와 epoch 추적\n",
    "        if avg_npmi > best_npmi:\n",
    "            best_npmi = avg_npmi\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()  # 현재 모델 상태 저장\n",
    "\n",
    "    print(f\"Best Epoch: {best_epoch} with NPMI: {best_npmi}\")\n",
    "    # 훈련 완료 후, 최적 모델 상태 저장\n",
    "    torch.save(best_model_state, 'our_best_model_state.pth')\n",
    "    model.load_state_dict(torch.load('our_best_model_state.pth'))\n",
    "    \n",
    "    print(\"------- Evaluation results -------\")\n",
    "    #각 토픽당 가지는 워드셋\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(10, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "885f1ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.3756525631944724, 'UCI': 2.836169828792919, 'UMASS': -3.391847991779266, 'CV': 0.7104940248302872, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "topic_words_list = list(all_list.values())\n",
    "reference_corpus=[doc.split() for doc in testds.preprocess_ctm(testds.nonempty_text)]\n",
    "\n",
    "topics=topic_words_list\n",
    "texts=reference_corpus\n",
    "print(get_topic_coherence(topics, texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb71e5",
   "metadata": {},
   "source": [
    "# MI Calulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5ba027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y): 2.854358622943483\n",
      "H(Y|X): 2.960724069021441\n",
      "H(X|Y): 1.0635194901934535\n",
      "Mutual Information (MI): -0.10636544607795795\n",
      "Original Mutual Information Score: 0.03084546522123911\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터 로더 설정\n",
    "testloader = DataLoader(testfinetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "\n",
    "# 토픽 라벨과 플랫폼 라벨 추출\n",
    "ourmodel_test_topic_labels = []\n",
    "test_platform_labels = []\n",
    "\n",
    "for batch in testloader:\n",
    "    _, org_embedding, _, org_bow, _, platform_labels = batch\n",
    "    org_embedding = org_embedding.to(gpu_ids[0])  # gpu_ids[1]는 사용 환경에 따라 다를 수 있습니다.\n",
    "    _, topic_logit = model.decode(org_embedding)\n",
    "    topic_label = torch.argmax(F.softmax(topic_logit, dim=1), dim=1)\n",
    "    ourmodel_test_topic_labels.extend(topic_label.cpu().numpy())\n",
    "    test_platform_labels.extend(platform_labels)  # 수정된 부분\n",
    "\n",
    "# 플랫폼별 토픽 분포 계산\n",
    "topic_dist_df_test = pd.crosstab(pd.Series(ourmodel_test_topic_labels, name='Topic'),\n",
    "                                 pd.Series(test_platform_labels, name='Platform'), normalize='index')\n",
    "\n",
    "# 플랫폼별 및 전체에 대한 토픽 분포를 계산\n",
    "platform_counts = pd.Series(test_platform_labels).value_counts()\n",
    "platform_probabilities = platform_counts / platform_counts.sum()\n",
    "\n",
    "# 전체 데이터셋에 대한 토픽 분포의 엔트로피 계산 (H(Y))\n",
    "topic_probabilities = pd.Series(ourmodel_test_topic_labels).value_counts(normalize=True)\n",
    "H_Y = -np.sum(topic_probabilities * np.log2(topic_probabilities + 1e-10))\n",
    "\n",
    "# 각 플랫폼별 조건부 엔트로피 계산 및 H(Y|X) 계산\n",
    "H_Y_given_X_total = 0\n",
    "for platform in platform_probabilities.index:\n",
    "    # 해당 플랫폼에 대한 토픽 라벨 필터링\n",
    "    platform_indices = [i for i, x in enumerate(test_platform_labels) if x == platform]\n",
    "    platform_topic_labels = [ourmodel_test_topic_labels[i] for i in platform_indices]\n",
    "    platform_topic_prob = pd.Series(platform_topic_labels).value_counts(normalize=True)\n",
    "    \n",
    "    # 플랫폼별 조건부 엔트로피 계산\n",
    "    H_Y_given_X = -np.sum(platform_topic_prob * np.log(platform_topic_prob + 1e-10))\n",
    "    H_Y_given_X_total += platform_probabilities[platform] * H_Y_given_X\n",
    "\n",
    "# Mutual Information (MI) 계산\n",
    "mi = H_Y - H_Y_given_X_total\n",
    "\n",
    "# 각 토픽에서 가장 높은 플랫폼 비율 추출\n",
    "max_platform_distribution = topic_dist_df_test.max(axis=1)\n",
    "\n",
    "# 평균 purity 계산\n",
    "average_purity = max_platform_distribution.mean()\n",
    "\n",
    "print(\"각 토픽에서 가장 높은 플랫폼의 비율 평균 (purity):\", average_purity)\n",
    "print('H(Y):', H_Y)\n",
    "print('H(Y|X):', H_Y_given_X)\n",
    "print('Mutual Information (MI):', mi)\n",
    "\n",
    "# sklearn의 mutual_info_score를 이용한 MI 계산으로 검증\n",
    "mi_score = mutual_info_score(ourmodel_test_topic_labels, test_platform_labels)\n",
    "print(\"Original Mutual Information Score:\", mi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c181c9f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Topic</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Platform</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>0.351759</td>\n",
       "      <td>0.458071</td>\n",
       "      <td>0.258687</td>\n",
       "      <td>0.240572</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.269311</td>\n",
       "      <td>0.296909</td>\n",
       "      <td>0.341987</td>\n",
       "      <td>0.353760</td>\n",
       "      <td>0.361280</td>\n",
       "      <td>0.395793</td>\n",
       "      <td>0.172078</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.286325</td>\n",
       "      <td>0.328261</td>\n",
       "      <td>0.401471</td>\n",
       "      <td>0.306859</td>\n",
       "      <td>0.345266</td>\n",
       "      <td>0.364625</td>\n",
       "      <td>0.335430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>0.319095</td>\n",
       "      <td>0.209644</td>\n",
       "      <td>0.383687</td>\n",
       "      <td>0.534460</td>\n",
       "      <td>0.314670</td>\n",
       "      <td>0.411273</td>\n",
       "      <td>0.318985</td>\n",
       "      <td>0.290564</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.236280</td>\n",
       "      <td>0.217973</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>0.399573</td>\n",
       "      <td>0.297826</td>\n",
       "      <td>0.267647</td>\n",
       "      <td>0.358604</td>\n",
       "      <td>0.322171</td>\n",
       "      <td>0.298419</td>\n",
       "      <td>0.323899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0.329146</td>\n",
       "      <td>0.332285</td>\n",
       "      <td>0.357625</td>\n",
       "      <td>0.224967</td>\n",
       "      <td>0.053751</td>\n",
       "      <td>0.319415</td>\n",
       "      <td>0.384106</td>\n",
       "      <td>0.367449</td>\n",
       "      <td>0.348189</td>\n",
       "      <td>0.402439</td>\n",
       "      <td>0.386233</td>\n",
       "      <td>0.509740</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.314103</td>\n",
       "      <td>0.373913</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.334537</td>\n",
       "      <td>0.332564</td>\n",
       "      <td>0.336957</td>\n",
       "      <td>0.340671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Topic           0         1         2         3         4         5   \\\n",
       "Platform                                                               \n",
       "reddit    0.351759  0.458071  0.258687  0.240572  0.631579  0.269311   \n",
       "twitter   0.319095  0.209644  0.383687  0.534460  0.314670  0.411273   \n",
       "youtube   0.329146  0.332285  0.357625  0.224967  0.053751  0.319415   \n",
       "\n",
       "Topic           6         7         8         9         10        11  \\\n",
       "Platform                                                               \n",
       "reddit    0.296909  0.341987  0.353760  0.361280  0.395793  0.172078   \n",
       "twitter   0.318985  0.290564  0.298050  0.236280  0.217973  0.318182   \n",
       "youtube   0.384106  0.367449  0.348189  0.402439  0.386233  0.509740   \n",
       "\n",
       "Topic           12        13        14        15        16        17  \\\n",
       "Platform                                                               \n",
       "reddit    0.179688  0.286325  0.328261  0.401471  0.306859  0.345266   \n",
       "twitter   0.433594  0.399573  0.297826  0.267647  0.358604  0.322171   \n",
       "youtube   0.386719  0.314103  0.373913  0.330882  0.334537  0.332564   \n",
       "\n",
       "Topic           18        19  \n",
       "Platform                      \n",
       "reddit    0.364625  0.335430  \n",
       "twitter   0.298419  0.323899  \n",
       "youtube   0.336957  0.340671  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist_df_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24802d41",
   "metadata": {},
   "source": [
    "# Seperate platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "003e0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_corpus의 길이 확인\n",
    "total_length = len(reference_corpus)\n",
    "\n",
    "# 각 플랫폼별로 6,000개씩 데이터가 충분한지 확인\n",
    "if total_length >= 18000:\n",
    "    # 트위터 데이터 분할\n",
    "    twitter_texts = reference_corpus[:6000]\n",
    "\n",
    "    # 레딧 데이터 분할\n",
    "    reddit_texts = reference_corpus[6000:12000]\n",
    "\n",
    "    # 유튜브 데이터 분할\n",
    "    youtube_texts = reference_corpus[12000:18000]\n",
    "else:\n",
    "    print(\"데이터가 충분하지 않습니다.\")\n",
    "\n",
    "twitter_dictionary = Dictionary(twitter_texts)\n",
    "twitter_dictionary.add_documents(topic_words_list)\n",
    "\n",
    "reddit_dictionary = Dictionary(reddit_texts)\n",
    "reddit_dictionary.add_documents(topic_words_list)\n",
    "\n",
    "youtube_dictionary = Dictionary(youtube_texts)\n",
    "youtube_dictionary.add_documents(topic_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4558c",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "839fee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.17281190004854755, 'UCI': 1.4734833541766035, 'UMASS': -4.897262862010767, 'CV': 0.7754231691410781, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, twitter_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edd2a9",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e5e213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.46981132807081494, 'UCI': 3.092224774259657, 'UMASS': -2.5446757068500663, 'CV': 0.7607069509740174, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, reddit_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8c453",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f56427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.251779756176881, 'UCI': 2.1096697750568794, 'UMASS': -3.874286438876698, 'CV': 0.7634598786378521, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, youtube_texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kms",
   "language": "python",
   "name": "kms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
