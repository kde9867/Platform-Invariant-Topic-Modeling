{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c355e0",
   "metadata": {},
   "source": [
    "# Setting(dataset, parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a009520",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 ' + \\\n",
    "            '--dataset all --n-word 30000 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 ' + \\\n",
    "            '--n-cluster 20 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c1d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2586781/4253515541.py:23: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from utils import AverageMeter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "import scipy.stats\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset, BertDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "\n",
    "from data import BertDataset, Stage2Dataset\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ab2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bce59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "\n",
    "    #각 stage에서의 batch size\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    #data set정의 \n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    # 클러스터 수와 topic의 수는 20 (k==20)\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    # 단어vocabulary는 2000로 setting\n",
    "    parser.add_argument('--n-word', default=30000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1,2,3], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "   \n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8d1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "# textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5ae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"모든 랜덤 요소에 대한 시드를 고정합니다.\"\"\"\n",
    "    random.seed(seed_value)  \n",
    "    np.random.seed(seed_value)  \n",
    "    torch.manual_seed(seed_value)  \n",
    "    if torch.cuda.is_available():  \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56d5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a80e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 각 데이터셋 초기화(data.py에서 확인 가능)\n",
    "twitter_ds = TwitterDataset()\n",
    "reddit_ds = RedditDataset()\n",
    "youtube_ds = YoutubeDataset()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 전체 데이터를 train과 test로 7:3으로 분할\n",
    "train_twitter_texts, test_twitter_texts, train_twitter_labels, test_twitter_labels = train_test_split(\n",
    "    twitter_ds.texts, twitter_ds.labels, train_size=0.7, random_state=41)\n",
    "\n",
    "train_reddit_texts, test_reddit_texts, train_reddit_labels, test_reddit_labels = train_test_split(\n",
    "    reddit_ds.texts, reddit_ds.labels, train_size=0.7, random_state=41)\n",
    "\n",
    "train_youtube_texts, test_youtube_texts, train_youtube_labels, test_youtube_labels = train_test_split(\n",
    "    youtube_ds.texts, youtube_ds.labels, train_size=0.7, random_state=41)\n",
    "\n",
    "# train 데이터를 다시 train과 val로 9:1로 분할\n",
    "train_twitter_texts, val_twitter_texts, train_twitter_labels, val_twitter_labels = train_test_split(\n",
    "    train_twitter_texts, train_twitter_labels, test_size=0.1, random_state=41)\n",
    "\n",
    "train_reddit_texts, val_reddit_texts, train_reddit_labels, val_reddit_labels = train_test_split(\n",
    "    train_reddit_texts, train_reddit_labels, test_size=0.1, random_state=41)\n",
    "\n",
    "train_youtube_texts, val_youtube_texts, train_youtube_labels, val_youtube_labels = train_test_split(\n",
    "    train_youtube_texts, train_youtube_labels, test_size=0.1, random_state=41)\n",
    "\n",
    "# 각 데이터의 플랫폼을 합침\n",
    "train_total_label = train_twitter_labels + train_reddit_labels + train_youtube_labels\n",
    "train_total_text_list = train_twitter_texts + train_reddit_texts + train_youtube_texts\n",
    "\n",
    "val_total_label = val_twitter_labels + val_reddit_labels + val_youtube_labels\n",
    "val_total_text_list = val_twitter_texts + val_reddit_texts + val_youtube_texts\n",
    "\n",
    "test_total_label = test_twitter_labels + test_reddit_labels + test_youtube_labels\n",
    "test_total_text_list = test_twitter_texts + test_reddit_texts + test_youtube_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9c2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37800/37800 [00:51<00:00, 731.50it/s]\n",
      "100%|██████████| 4200/4200 [00:05<00:00, 839.28it/s]\n",
      "100%|██████████| 18000/18000 [00:23<00:00, 752.20it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=train_total_text_list, platform_label = train_total_label, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "valds = BertDataset(bert=bert_name, text_list=val_total_text_list, platform_label = val_total_label, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds = BertDataset(bert=bert_name, text_list=test_total_text_list, platform_label = test_total_label, N_word=n_word, vectorizer=None, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba29252",
   "metadata": {},
   "source": [
    "# 원본 데이터 Mean pooling 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2229cd",
   "metadata": {},
   "source": [
    "# mean_pooling 함수 정의\n",
    "def mean_pooling(embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb71fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 문장의 평균 풀링 임베딩 계산\n",
    "trainds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in trainds:\n",
    "    trainds_embeddings.append(pooled_embedding)\n",
    "\n",
    "# 평균 풀링된 임베딩 매트릭스 생성\n",
    "train_mean_pooled_embeddings = torch.stack(trainds_embeddings)\n",
    "\n",
    "# 모든 문장의 평균 풀링 임베딩 계산\n",
    "valds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in valds:\n",
    "    valds_embeddings.append(pooled_embedding)\n",
    "\n",
    "# 평균 풀링된 임베딩 매트릭스 생성\n",
    "val_mean_pooled_embeddings = torch.stack(valds_embeddings)\n",
    "\n",
    "# 모든 문장의 평균 풀링 임베딩 계산\n",
    "testds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in testds:\n",
    "    testds_embeddings.append(pooled_embedding)\n",
    "\n",
    "# 평균 풀링된 임베딩 매트릭스 생성\n",
    "test_mean_pooled_embeddings = torch.stack(testds_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd08ab9",
   "metadata": {},
   "source": [
    "# Re_fornulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85826fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97909bc",
   "metadata": {},
   "source": [
    "# Get pos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_cosine_similarity_indices(mean_pooled_embeddings, batch_size=500):\n",
    "    n_rows = mean_pooled_embeddings.shape[0]\n",
    "    max_similarity_indices = np.zeros(n_rows, dtype=np.int64)\n",
    "\n",
    "    for start_idx in range(0, n_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_rows)\n",
    "        batch_data = mean_pooled_embeddings[start_idx:end_idx]\n",
    "\n",
    "        batch_similarity = cosine_similarity(batch_data, mean_pooled_embeddings)\n",
    "\n",
    "        # 자기 자신과의 유사도를 -1로 설정\n",
    "        for i, original_idx in enumerate(range(start_idx, end_idx)):\n",
    "            batch_similarity[i, original_idx] = -1\n",
    "\n",
    "        # 각 행에서 가장 큰 값을 가진 인덱스 찾기\n",
    "        max_indices = np.argmax(batch_similarity, axis=1)\n",
    "        max_similarity_indices[start_idx:end_idx] = max_indices\n",
    "\n",
    "        #logger.info(f\"{end_idx}/{n_rows} 데이터 처리 완료\")\n",
    "\n",
    "    return max_similarity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3016fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_pooled_embeddings를 2차원 형태로 변환\n",
    "train_mean_pooled_embeddings_2d = train_mean_pooled_embeddings.squeeze()\n",
    "val_mean_pooled_embeddings_2d = val_mean_pooled_embeddings.squeeze()\n",
    "test_mean_pooled_embeddings_2d = test_mean_pooled_embeddings.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc96d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 매트릭스 계산\n",
    "train_similarity_matrix = compute_max_cosine_similarity_indices(train_mean_pooled_embeddings_2d)\n",
    "val_similarity_matrix = compute_max_cosine_similarity_indices(val_mean_pooled_embeddings_2d)\n",
    "test_similarity_matrix = compute_max_cosine_similarity_indices(test_mean_pooled_embeddings_2d)  # 테스트 데이터셋에 대해서도 동일하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745200ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37800,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12fdaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3abcb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37800/37800 [00:31<00:00, 1210.40it/s]\n",
      "100%|██████████| 37800/37800 [20:07<00:00, 31.29it/s]\n",
      "100%|██████████| 4200/4200 [00:02<00:00, 1602.67it/s]\n",
      "100%|██████████| 4200/4200 [02:14<00:00, 31.28it/s]\n",
      "100%|██████████| 18000/18000 [00:13<00:00, 1307.45it/s]\n",
      "  9%|▉         | 1673/18000 [00:58<09:03, 30.04it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 18000/18000 [09:52<00:00, 30.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, train_similarity_matrix, n_word, lemmatize=True)\n",
    "\n",
    "valfinetuneds = Stage2Dataset(model.encoder, valds, val_similarity_matrix, n_word, lemmatize=True) \n",
    "\n",
    "testfinetuneds = Stage2Dataset(model.encoder, testds, test_similarity_matrix, n_word, lemmatize=True) \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06d9e537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(finetuneds.embedding_list)\n",
    "len(finetuneds.bow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daded5c",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a3e6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9818d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_measure_hungarian = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86eb85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d91e4",
   "metadata": {},
   "source": [
    "# Seperate Platform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df7a3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from collections import defaultdict\n",
    "\n",
    "# 커스텀 샘플러 구현\n",
    "class PlatformSampler(Sampler):\n",
    "    def __init__(self, dataset, platform_label):\n",
    "        self.indices = [i for i, label in enumerate(dataset.platform_label_list) if label == platform_label]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e66859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 플랫폼별 데이터로더 생성 함수\n",
    "def create_platform_dataloader(dataset, platform_label, batch_size=32):\n",
    "    sampler = PlatformSampler(dataset, platform_label)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    return dataloader\n",
    "\n",
    "def create_platform_dataloader(dataset, platform_label, batch_size=32, num_workers=0):\n",
    "    # PlatformSampler는 플랫폼 라벨에 따라 데이터셋에서 샘플링하는 커스텀 샘플러입니다.\n",
    "    # 이 샘플러는 여기서 정의하거나, 필요에 따라 다른 샘플링 로직을 구현할 수 있습니다.\n",
    "    sampler = PlatformSampler(dataset, platform_label)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01506b05",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd17d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 9.18415 - dist: 0.15204 - cons: -0.08972\n",
      "Epoch-1 / recon: 8.94334 - dist: 0.14287 - cons: -0.12167\n",
      "Epoch-2 / recon: 8.79036 - dist: 0.13551 - cons: -0.14182\n",
      "Epoch-3 / recon: 8.67513 - dist: 0.12803 - cons: -0.15453\n",
      "Epoch-4 / recon: 8.58187 - dist: 0.12241 - cons: -0.16281\n",
      "Epoch-5 / recon: 8.50325 - dist: 0.11865 - cons: -0.16878\n",
      "Epoch-6 / recon: 8.43566 - dist: 0.11589 - cons: -0.17275\n",
      "Epoch-7 / recon: 8.37593 - dist: 0.11411 - cons: -0.17523\n",
      "Epoch-8 / recon: 8.32235 - dist: 0.11319 - cons: -0.17657\n",
      "Epoch-9 / recon: 8.27357 - dist: 0.11290 - cons: -0.17710\n",
      "Epoch-10 / recon: 8.22845 - dist: 0.11307 - cons: -0.17697\n",
      "Epoch-11 / recon: 8.18635 - dist: 0.11347 - cons: -0.17642\n",
      "Epoch-12 / recon: 8.14665 - dist: 0.11431 - cons: -0.17549\n",
      "Epoch-13 / recon: 8.10907 - dist: 0.11539 - cons: -0.17430\n",
      "Epoch-14 / recon: 8.07312 - dist: 0.11664 - cons: -0.17290\n",
      "Epoch-15 / recon: 8.03877 - dist: 0.11798 - cons: -0.17141\n",
      "Epoch-16 / recon: 8.00577 - dist: 0.11955 - cons: -0.16982\n",
      "Epoch-17 / recon: 7.97409 - dist: 0.12118 - cons: -0.16816\n",
      "Epoch-18 / recon: 7.94364 - dist: 0.12286 - cons: -0.16649\n",
      "Epoch-19 / recon: 7.91408 - dist: 0.12461 - cons: -0.16479\n",
      "Epoch-20 / recon: 7.88565 - dist: 0.12637 - cons: -0.16310\n",
      "Epoch-21 / recon: 7.85809 - dist: 0.12823 - cons: -0.16144\n",
      "Epoch-22 / recon: 7.83139 - dist: 0.13006 - cons: -0.15979\n",
      "Epoch-23 / recon: 7.80557 - dist: 0.13187 - cons: -0.15817\n",
      "Epoch-24 / recon: 7.78052 - dist: 0.13365 - cons: -0.15659\n",
      "Epoch-25 / recon: 7.75638 - dist: 0.13546 - cons: -0.15505\n",
      "Epoch-26 / recon: 7.73285 - dist: 0.13723 - cons: -0.15356\n",
      "Epoch-27 / recon: 7.71008 - dist: 0.13900 - cons: -0.15210\n",
      "Epoch-28 / recon: 7.68800 - dist: 0.14070 - cons: -0.15068\n",
      "Epoch-29 / recon: 7.66645 - dist: 0.14240 - cons: -0.14930\n",
      "Epoch-30 / recon: 7.64555 - dist: 0.14407 - cons: -0.14796\n",
      "Epoch-31 / recon: 7.62517 - dist: 0.14576 - cons: -0.14666\n",
      "Epoch-32 / recon: 7.60538 - dist: 0.14741 - cons: -0.14540\n",
      "Epoch-33 / recon: 7.58615 - dist: 0.14898 - cons: -0.14419\n",
      "Epoch-34 / recon: 7.56739 - dist: 0.15055 - cons: -0.14301\n",
      "Epoch-35 / recon: 7.54913 - dist: 0.15212 - cons: -0.14188\n",
      "Epoch-36 / recon: 7.53134 - dist: 0.15360 - cons: -0.14077\n",
      "Epoch-37 / recon: 7.51403 - dist: 0.15506 - cons: -0.13970\n",
      "Epoch-38 / recon: 7.49717 - dist: 0.15651 - cons: -0.13866\n",
      "Epoch-39 / recon: 7.48069 - dist: 0.15788 - cons: -0.13765\n",
      "Epoch-40 / recon: 7.46465 - dist: 0.15929 - cons: -0.13668\n",
      "Epoch-41 / recon: 7.44899 - dist: 0.16063 - cons: -0.13573\n",
      "Epoch-42 / recon: 7.43370 - dist: 0.16199 - cons: -0.13481\n",
      "Epoch-43 / recon: 7.41880 - dist: 0.16329 - cons: -0.13392\n",
      "Epoch-44 / recon: 7.40419 - dist: 0.16454 - cons: -0.13306\n",
      "Epoch-45 / recon: 7.38993 - dist: 0.16582 - cons: -0.13222\n",
      "Epoch-46 / recon: 7.37600 - dist: 0.16705 - cons: -0.13140\n",
      "Epoch-47 / recon: 7.36237 - dist: 0.16822 - cons: -0.13061\n",
      "Epoch-48 / recon: 7.34909 - dist: 0.16939 - cons: -0.12985\n",
      "Epoch-49 / recon: 7.33611 - dist: 0.17053 - cons: -0.12911\n",
      "Epoch-50 / recon: 7.32339 - dist: 0.17164 - cons: -0.12839\n",
      "Epoch-51 / recon: 7.31095 - dist: 0.17274 - cons: -0.12769\n",
      "Epoch-52 / recon: 7.29881 - dist: 0.17382 - cons: -0.12701\n",
      "Epoch-53 / recon: 7.28687 - dist: 0.17488 - cons: -0.12635\n",
      "Epoch-54 / recon: 7.27525 - dist: 0.17589 - cons: -0.12570\n",
      "Epoch-55 / recon: 7.26387 - dist: 0.17691 - cons: -0.12507\n",
      "Epoch-56 / recon: 7.25268 - dist: 0.17790 - cons: -0.12446\n",
      "Epoch-57 / recon: 7.24174 - dist: 0.17886 - cons: -0.12387\n",
      "Epoch-58 / recon: 7.23103 - dist: 0.17980 - cons: -0.12329\n",
      "Epoch-59 / recon: 7.22056 - dist: 0.18072 - cons: -0.12273\n",
      "Epoch-60 / recon: 7.21028 - dist: 0.18162 - cons: -0.12218\n",
      "Epoch-61 / recon: 7.20019 - dist: 0.18251 - cons: -0.12165\n",
      "Epoch-62 / recon: 7.19032 - dist: 0.18338 - cons: -0.12113\n",
      "Epoch-63 / recon: 7.18063 - dist: 0.18423 - cons: -0.12062\n",
      "Epoch-64 / recon: 7.17115 - dist: 0.18506 - cons: -0.12012\n",
      "Epoch-65 / recon: 7.16184 - dist: 0.18588 - cons: -0.11964\n",
      "Epoch-66 / recon: 7.15270 - dist: 0.18668 - cons: -0.11917\n",
      "Epoch-67 / recon: 7.14377 - dist: 0.18747 - cons: -0.11871\n",
      "Epoch-68 / recon: 7.13500 - dist: 0.18824 - cons: -0.11826\n",
      "Epoch-69 / recon: 7.12640 - dist: 0.18899 - cons: -0.11782\n",
      "Epoch-70 / recon: 7.11793 - dist: 0.18975 - cons: -0.11739\n",
      "Epoch-71 / recon: 7.10960 - dist: 0.19050 - cons: -0.11697\n",
      "Epoch-72 / recon: 7.10143 - dist: 0.19122 - cons: -0.11656\n",
      "Epoch-73 / recon: 7.09341 - dist: 0.19193 - cons: -0.11616\n",
      "Epoch-74 / recon: 7.08553 - dist: 0.19263 - cons: -0.11576\n",
      "Epoch-75 / recon: 7.07780 - dist: 0.19331 - cons: -0.11538\n",
      "Epoch-76 / recon: 7.07020 - dist: 0.19398 - cons: -0.11500\n",
      "Epoch-77 / recon: 7.06273 - dist: 0.19464 - cons: -0.11464\n",
      "Epoch-78 / recon: 7.05541 - dist: 0.19528 - cons: -0.11428\n",
      "Epoch-79 / recon: 7.04820 - dist: 0.19593 - cons: -0.11392\n",
      "Epoch-80 / recon: 7.04113 - dist: 0.19655 - cons: -0.11358\n",
      "Epoch-81 / recon: 7.03414 - dist: 0.19716 - cons: -0.11324\n",
      "Epoch-82 / recon: 7.02727 - dist: 0.19777 - cons: -0.11291\n",
      "Epoch-83 / recon: 7.02051 - dist: 0.19836 - cons: -0.11259\n",
      "Epoch-84 / recon: 7.01388 - dist: 0.19895 - cons: -0.11227\n",
      "Epoch-85 / recon: 7.00732 - dist: 0.19954 - cons: -0.11196\n",
      "Epoch-86 / recon: 7.00090 - dist: 0.20012 - cons: -0.11165\n",
      "Epoch-87 / recon: 6.99459 - dist: 0.20068 - cons: -0.11135\n",
      "Epoch-88 / recon: 6.98837 - dist: 0.20123 - cons: -0.11106\n",
      "Epoch-89 / recon: 6.98222 - dist: 0.20177 - cons: -0.11077\n",
      "Epoch-90 / recon: 6.97617 - dist: 0.20230 - cons: -0.11049\n",
      "Epoch-91 / recon: 6.97023 - dist: 0.20284 - cons: -0.11021\n",
      "Epoch-92 / recon: 6.96437 - dist: 0.20335 - cons: -0.10994\n",
      "Epoch-93 / recon: 6.95862 - dist: 0.20386 - cons: -0.10967\n",
      "Epoch-94 / recon: 6.95293 - dist: 0.20436 - cons: -0.10941\n",
      "Epoch-95 / recon: 6.94734 - dist: 0.20485 - cons: -0.10915\n",
      "Epoch-96 / recon: 6.94182 - dist: 0.20535 - cons: -0.10890\n",
      "Epoch-97 / recon: 6.93640 - dist: 0.20582 - cons: -0.10865\n",
      "Epoch-98 / recon: 6.93105 - dist: 0.20630 - cons: -0.10840\n",
      "Epoch-99 / recon: 6.92577 - dist: 0.20676 - cons: -0.10816\n",
      "Best Epoch: 97 with NPMI: 0.9052001885558077\n",
      "------- Evaluation results -------\n",
      "topic-0 ['amp', 'crazygpt', 'incognito', 'chromeedgefirefox', 'rickroll', 'chromium', 'omegle', 'sunglass', 'dithering', 'bingbot']\n",
      "topic-1 ['copypasta', 'clock', 'reload', 'occasionally', 'paste', 'linebrchatgpt', 'rewritten', 'copy', 'backing', 'mail']\n",
      "topic-2 ['mentor', 'outsized', 'thankyou', 'payout', 'esl', 'marginalized', 'supplement', 'niiice', 'fantastic', 'intervention']\n",
      "topic-3 ['singularity', 'defenetly', 'imminent', 'godlike', 'rocked', 'declaration', 'polygamy', 'durden', 'paradox', 'charlemagne']\n",
      "topic-4 ['discord', 'request', 'friendly', 'performed', 'moderator', 'subreddit', 'repetitive', 'comment', 'experiment', 'compose']\n",
      "topic-5 ['quotive', 'asi', 'assamese', 'commodor', 'withquot', 'ami', 'osm', 'unclear', 'chatgptget', 'ltanswers']\n",
      "topic-6 ['momma', 'crazygpt', 'regurarly', 'brutal', 'shady', 'closet', 'gtmostly', 'bloody', 'ideologically', 'madness']\n",
      "topic-7 ['chatgptgonewild', 'chatgptcrypto', 'palki', 'interviewed', 'brsimple', 'overview', 'kipp', 'coerced', 'mimir', 'chatbotlife']\n",
      "topic-8 ['muppet', 'calculator', 'essaywriting', 'melody', 'rhyme', 'chord', 'mathematica', 'comprehension', 'dissertation', 'urdu']\n",
      "topic-9 ['zesty', 'tshirt', 'loli', 'loool', 'huggs', 'gottemmm', 'ok', 'bingo', 'dad', 'muchhh']\n",
      "topic-10 ['textdavinci', 'moderator', 'discord', 'bot', 'server', 'update', 'performed', 'friendly', 'comment', 'prevent']\n",
      "topic-11 ['cola', 'coca', 'poopy', 'knight', 'tossing', 'monocle', 'manly', 'bottle', 'soda', 'castled']\n",
      "topic-12 ['zune', 'chatgptowner', 'jobsbrbr', 'valued', 'mcdonalds', 'stake', 'investment', 'sooner', 'walmart', 'isolationism']\n",
      "topic-13 ['dying', 'goodnight', 'bingbot', 'soulless', 'surrender', 'infinitybots', 'backfire', 'lifehack', 'chatbots', 'dyslexic']\n",
      "topic-14 ['vai', 'cai', 'aro', 'commandment', 'cryptoai', 'degenai', 'kana', 'mlart', 'aesthetic', 'digitalart']\n",
      "topic-15 ['despair', 'unnecessarily', 'fabricates', 'morely', 'routinely', 'comptia', 'pentest', 'outmeasures', 'progressively', 'ooookkurrrr']\n",
      "topic-16 ['brhoweveri', 'abruptly', 'midword', 'posterity', 'crashed', 'broke', 'disagrees', 'mikrofon', 'resolve', 'peshawarattack']\n",
      "topic-17 ['incroyable', 'cdc', 'hehelets', 'cameroon', 'clinton', 'lopotmized', 'recomended', 'puro', 'avaliable', 'jarves']\n",
      "topic-18 ['prohibitive', 'entail', 'dataengineering', 'lessbrwhat', 'buffed', 'exploration', 'lossy', 'batch', 'longshort', 'terrify']\n",
      "topic-19 ['woooosh', 'plank', 'tau', 'woosh', 'lmfaooo', 'kiiiing', 'lmaoooo', 'shalom', 'kekw', 'sacrificed']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import chi2_contingency \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from coherence import get_topic_coherence\n",
    "\n",
    "args.stage_2_repeat = 1\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    ##수정\n",
    "    twitter_trainloader = create_platform_dataloader(finetuneds, 'twitter', batch_size=bsz, num_workers=0)\n",
    "    reddit_trainloader = create_platform_dataloader(finetuneds, 'reddit', batch_size=bsz, num_workers=0)\n",
    "    youtube_trainloader = create_platform_dataloader(finetuneds, 'youtube', batch_size=bsz, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    ##\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    # 최적 epoch 추적을 위한 변수 초기화\n",
    "    best_npmi = -1\n",
    "    best_epoch = 0\n",
    "    best_model_state = None  # 모델 상태를 저장하기 위한 변수\n",
    "    \n",
    "    # 각 플랫폼별 DataLoader의 이터레이터 생성\n",
    "    twitter_iter = iter(twitter_trainloader)\n",
    "    reddit_iter = iter(reddit_trainloader)\n",
    "    youtube_iter = iter(youtube_trainloader)\n",
    "\n",
    "    # 가장 긴 DataLoader의 길이를 계산하여 학습 루프의 범위를 정함\n",
    "    max_length = max(len(twitter_trainloader), len(reddit_trainloader), len(youtube_trainloader))\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # 플랫폼별 DataLoader에서 배치를 순차적으로 가져오기\n",
    "            try:\n",
    "                twitter_batch = next(twitter_iter)\n",
    "            except StopIteration:\n",
    "                # Twitter DataLoader가 끝에 도달하면, 이터레이터를 다시 시작함\n",
    "                twitter_iter = iter(twitter_trainloader)\n",
    "                twitter_batch = next(twitter_iter)\n",
    "\n",
    "            try:\n",
    "                reddit_batch = next(reddit_iter)\n",
    "            except StopIteration:\n",
    "                reddit_iter = iter(reddit_trainloader)\n",
    "                reddit_batch = next(reddit_iter)\n",
    "\n",
    "            try:\n",
    "                youtube_batch = next(youtube_iter)\n",
    "            except StopIteration:\n",
    "                youtube_iter = iter(youtube_trainloader)\n",
    "                youtube_batch = next(youtube_iter)\n",
    "\n",
    "            # 각 배치에 대한 학습 로직 구현\n",
    "            for batch in [twitter_batch, reddit_batch, youtube_batch]:\n",
    "                _, org_input, pos_input, org_bow, pos_bow, _ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_bow = org_bow.cuda(gpu_ids[0])\n",
    "                pos_input = pos_input.cuda(gpu_ids[0])\n",
    "                pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "                batch_size = org_input.size(0) #org_input_ids.size(0)\n",
    "\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "                # reconstruction loss\n",
    "                # batchmean\n",
    "    #             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "    #             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "\n",
    "    #             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "    #             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "\n",
    "                # 텐서 크기 맞춰줌\n",
    "                org_dists = org_dists[:, :org_bow.size(1)]\n",
    "                pos_dists = pos_dists[:, :pos_bow.size(1)]\n",
    "\n",
    "                recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * (1-org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * (1-pos_bow), axis=1), axis=0)\n",
    "                recons_loss *= 0.5\n",
    "\n",
    "                # consistency loss\n",
    "                pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "                cons_loss = -pos_sim.mean()\n",
    "\n",
    "                # distribution loss\n",
    "                # batchmean\n",
    "    #             distmatch_loss = dist_match_loss(torch.cat((org_topic), dim=0), dirichlet_alpha_2)\n",
    "                distmatch_loss = dist_match_loss(torch.cat((org_topic,), dim=0), dirichlet_alpha_2)\n",
    "\n",
    "\n",
    "                loss = args.coeff_2_recon * recons_loss + \\\n",
    "                       args.coeff_2_cons * cons_loss + \\\n",
    "                       args.coeff_2_dist * distmatch_loss \n",
    "\n",
    "                losses.update(loss.item(), bsz)\n",
    "                closses.update(cons_loss.item(), bsz)\n",
    "                rlosses.update(recons_loss.item(), bsz)\n",
    "                distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "        # Epoch 마다 실행\n",
    "        model.eval()\n",
    "\n",
    "        # 각 토픽에 대한 상위 10개 단어 추출\n",
    "        top_words_per_topic = {}\n",
    "        for topic_idx in range(model.N_topic):\n",
    "            top_words_indices = model.beta[topic_idx].topk(10).indices\n",
    "            top_words = [vocab_dict_reverse[idx.item()] for idx in top_words_indices]\n",
    "            top_words_per_topic[topic_idx] = top_words\n",
    "            \n",
    "        reference_corpus=[doc.split() for doc in valds.preprocess_ctm(valds.nonempty_text)]\n",
    "        topic_words_list = list(top_words_per_topic.values())\n",
    "        result = get_topic_coherence(topic_words_list, reference_corpus)\n",
    "        avg_npmi = result['NPMI']\n",
    "\n",
    "        # 최적의 NPMI와 epoch 추적\n",
    "        if avg_npmi > best_npmi:\n",
    "            best_npmi = avg_npmi\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()  # 현재 모델 상태 저장\n",
    "\n",
    "    print(f\"Best Epoch: {best_epoch} with NPMI: {best_npmi}\")\n",
    "    # 훈련 완료 후, 최적 모델 상태 저장\n",
    "    torch.save(best_model_state, 'our_best_model_state.pth')\n",
    "    model.load_state_dict(torch.load('our_best_model_state.pth'))\n",
    "    \n",
    "    print(\"------- Evaluation results -------\")\n",
    "    #각 토픽당 가지는 워드셋\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(10, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "885f1ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.7839543630417701, 'UCI': 6.021049220909073, 'UMASS': -0.8532157191386374, 'CV': 0.7447828058413485, 'Topic_Diversity': 0.965}\n"
     ]
    }
   ],
   "source": [
    "topic_words_list = list(all_list.values())\n",
    "reference_corpus=[doc.split() for doc in testds.preprocess_ctm(testds.nonempty_text)]\n",
    "\n",
    "topics=topic_words_list\n",
    "texts=reference_corpus\n",
    "print(get_topic_coherence(topics, texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb71e5",
   "metadata": {},
   "source": [
    "# MI Calulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5ba027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 토픽에서 가장 높은 플랫폼의 비율 평균 (purity): 0.45250493463653774\n",
      "H(Y): 4.151476999465531\n",
      "H(Y|X): 2.8135437025870846\n",
      "Mutual Information (MI): 1.3086207820189446\n",
      "Original Mutual Information Score: 0.034728359892409036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터 로더 설정\n",
    "testloader = DataLoader(testfinetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "\n",
    "# 토픽 라벨과 플랫폼 라벨 추출\n",
    "ourmodel_test_topic_labels = []\n",
    "test_platform_labels = []\n",
    "\n",
    "for batch in testloader:\n",
    "    _, org_embedding, _, org_bow, _, platform_labels = batch\n",
    "    org_embedding = org_embedding.to(gpu_ids[0])  # gpu_ids[1]는 사용 환경에 따라 다를 수 있습니다.\n",
    "    _, topic_logit = model.decode(org_embedding)\n",
    "    topic_label = torch.argmax(F.softmax(topic_logit, dim=1), dim=1)\n",
    "    ourmodel_test_topic_labels.extend(topic_label.cpu().numpy())\n",
    "    test_platform_labels.extend(platform_labels)  # 수정된 부분\n",
    "\n",
    "# 플랫폼별 토픽 분포 계산\n",
    "topic_dist_df_test = pd.crosstab(pd.Series(ourmodel_test_topic_labels, name='Topic'),\n",
    "                                 pd.Series(test_platform_labels, name='Platform'), normalize='index')\n",
    "\n",
    "# 플랫폼별 및 전체에 대한 토픽 분포를 계산\n",
    "platform_counts = pd.Series(test_platform_labels).value_counts()\n",
    "platform_probabilities = platform_counts / platform_counts.sum()\n",
    "\n",
    "# 전체 데이터셋에 대한 토픽 분포의 엔트로피 계산 (H(Y))\n",
    "topic_probabilities = pd.Series(ourmodel_test_topic_labels).value_counts(normalize=True)\n",
    "H_Y = -np.sum(topic_probabilities * np.log2(topic_probabilities + 1e-10))\n",
    "\n",
    "# 각 플랫폼별 조건부 엔트로피 계산 및 H(Y|X) 계산\n",
    "H_Y_given_X_total = 0\n",
    "for platform in platform_probabilities.index:\n",
    "    # 해당 플랫폼에 대한 토픽 라벨 필터링\n",
    "    platform_indices = [i for i, x in enumerate(test_platform_labels) if x == platform]\n",
    "    platform_topic_labels = [ourmodel_test_topic_labels[i] for i in platform_indices]\n",
    "    platform_topic_prob = pd.Series(platform_topic_labels).value_counts(normalize=True)\n",
    "    \n",
    "    # 플랫폼별 조건부 엔트로피 계산\n",
    "    H_Y_given_X = -np.sum(platform_topic_prob * np.log(platform_topic_prob + 1e-10))\n",
    "    H_Y_given_X_total += platform_probabilities[platform] * H_Y_given_X\n",
    "\n",
    "# Mutual Information (MI) 계산\n",
    "mi = H_Y - H_Y_given_X_total\n",
    "\n",
    "# 각 토픽에서 가장 높은 플랫폼 비율 추출\n",
    "max_platform_distribution = topic_dist_df_test.max(axis=1)\n",
    "\n",
    "# 평균 purity 계산\n",
    "average_purity = max_platform_distribution.mean()\n",
    "\n",
    "print(\"각 토픽에서 가장 높은 플랫폼의 비율 평균 (purity):\", average_purity)\n",
    "print('H(Y):', H_Y)\n",
    "print('H(Y|X):', H_Y_given_X)\n",
    "print('Mutual Information (MI):', mi)\n",
    "\n",
    "# sklearn의 mutual_info_score를 이용한 MI 계산으로 검증\n",
    "mi_score = mutual_info_score(ourmodel_test_topic_labels, test_platform_labels)\n",
    "print(\"Original Mutual Information Score:\", mi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c181c9f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Topic</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Platform</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>0.249587</td>\n",
       "      <td>0.296832</td>\n",
       "      <td>0.209177</td>\n",
       "      <td>0.270370</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.202985</td>\n",
       "      <td>0.363782</td>\n",
       "      <td>0.278340</td>\n",
       "      <td>0.302869</td>\n",
       "      <td>0.343797</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.321317</td>\n",
       "      <td>0.285996</td>\n",
       "      <td>0.298319</td>\n",
       "      <td>0.292770</td>\n",
       "      <td>0.442085</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.342537</td>\n",
       "      <td>0.355664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>0.408264</td>\n",
       "      <td>0.378544</td>\n",
       "      <td>0.360324</td>\n",
       "      <td>0.272222</td>\n",
       "      <td>0.106312</td>\n",
       "      <td>0.674627</td>\n",
       "      <td>0.280449</td>\n",
       "      <td>0.404858</td>\n",
       "      <td>0.429330</td>\n",
       "      <td>0.288490</td>\n",
       "      <td>0.240093</td>\n",
       "      <td>0.214734</td>\n",
       "      <td>0.353057</td>\n",
       "      <td>0.262605</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.244009</td>\n",
       "      <td>0.331844</td>\n",
       "      <td>0.341674</td>\n",
       "      <td>0.301462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0.342149</td>\n",
       "      <td>0.324625</td>\n",
       "      <td>0.430499</td>\n",
       "      <td>0.457407</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>0.122388</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.316802</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.367713</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.463950</td>\n",
       "      <td>0.360947</td>\n",
       "      <td>0.439076</td>\n",
       "      <td>0.313529</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.305011</td>\n",
       "      <td>0.351955</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.342875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Topic           0         1         2         3         4         5   \\\n",
       "Platform                                                               \n",
       "reddit    0.249587  0.296832  0.209177  0.270370  0.837209  0.202985   \n",
       "twitter   0.408264  0.378544  0.360324  0.272222  0.106312  0.674627   \n",
       "youtube   0.342149  0.324625  0.430499  0.457407  0.056478  0.122388   \n",
       "\n",
       "Topic           6         7         8         9         10        11  \\\n",
       "Platform                                                               \n",
       "reddit    0.363782  0.278340  0.302869  0.343797  0.696970  0.321317   \n",
       "twitter   0.280449  0.404858  0.429330  0.288490  0.240093  0.214734   \n",
       "youtube   0.355769  0.316802  0.267800  0.367713  0.062937  0.463950   \n",
       "\n",
       "Topic           12        13        14        15        16        17  \\\n",
       "Platform                                                               \n",
       "reddit    0.285996  0.298319  0.292770  0.442085  0.450980  0.316201   \n",
       "twitter   0.353057  0.262605  0.393701  0.233591  0.244009  0.331844   \n",
       "youtube   0.360947  0.439076  0.313529  0.324324  0.305011  0.351955   \n",
       "\n",
       "Topic           18        19  \n",
       "Platform                      \n",
       "reddit    0.342537  0.355664  \n",
       "twitter   0.341674  0.301462  \n",
       "youtube   0.315789  0.342875  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist_df_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24802d41",
   "metadata": {},
   "source": [
    "# Seperate platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "003e0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_corpus의 길이 확인\n",
    "total_length = len(reference_corpus)\n",
    "\n",
    "# 각 플랫폼별로 6,000개씩 데이터가 충분한지 확인\n",
    "if total_length >= 18000:\n",
    "    # 트위터 데이터 분할\n",
    "    twitter_texts = reference_corpus[:6000]\n",
    "\n",
    "    # 레딧 데이터 분할\n",
    "    reddit_texts = reference_corpus[6000:12000]\n",
    "\n",
    "    # 유튜브 데이터 분할\n",
    "    youtube_texts = reference_corpus[12000:18000]\n",
    "else:\n",
    "    print(\"데이터가 충분하지 않습니다.\")\n",
    "\n",
    "twitter_dictionary = Dictionary(twitter_texts)\n",
    "twitter_dictionary.add_documents(topic_words_list)\n",
    "\n",
    "reddit_dictionary = Dictionary(reddit_texts)\n",
    "reddit_dictionary.add_documents(topic_words_list)\n",
    "\n",
    "youtube_dictionary = Dictionary(youtube_texts)\n",
    "youtube_dictionary.add_documents(topic_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4558c",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "839fee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.7030746348415198, 'UCI': 5.662690397743228, 'UMASS': -0.9145817258008636, 'CV': 0.7966546888516141, 'Topic_Diversity': 0.965}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, twitter_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edd2a9",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e5e213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.824643497248931, 'UCI': 5.247322113243571, 'UMASS': -0.6631570993922827, 'CV': 0.7992682566855496, 'Topic_Diversity': 0.965}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, reddit_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8c453",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f56427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.4540206410780545, 'UCI': 3.337720540702274, 'UMASS': -1.7670120099356408, 'CV': 0.7997852904911561, 'Topic_Diversity': 0.965}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, youtube_texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kms",
   "language": "python",
   "name": "kms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
