{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c355e0",
   "metadata": {},
   "source": [
    "# Setting(dataset, parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32cb1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 원하는 저장 경로를 변수에 저장 \n",
    "desired_cache_path = \"/mnt/ssd1/don_ssd1/twitter_crawling_don/UTopic/hub\" # 본인 mnt 디렉토리 주소 입력\n",
    "\n",
    "# TRANSFORMERS_CACHE 환경변수 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = desired_cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a009520",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 ' + \\\n",
    "            '--dataset all --n-word 30000 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 ' + \\\n",
    "            '--n-cluster 3 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c1d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/don12/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/don12/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/don12/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from utils import AverageMeter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "import scipy.stats\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset, BertDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "\n",
    "from data import BertDataset, Stage2Dataset\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dd2c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 14 23:06:22 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:18:00.0 Off |                  Off |\n",
      "| 39%   61C    P2   154W / 300W |  45715MiB / 49140MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:3B:00.0 Off |                  Off |\n",
      "| 40%   68C    P2   221W / 300W |  25373MiB / 49140MiB |     55%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:86:00.0 Off |                  Off |\n",
      "| 43%   70C    P2   190W / 300W |  33429MiB / 49140MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:AF:00.0 Off |                  Off |\n",
      "| 42%   71C    P2   193W / 300W |  33383MiB / 49140MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1554450      C   /usr/bin/python3                 1461MiB |\n",
      "|    0   N/A  N/A   1656318      C   /usr/bin/python3                  893MiB |\n",
      "|    0   N/A  N/A   1680370      C   /usr/bin/python3                 1275MiB |\n",
      "|    0   N/A  N/A   1734757      C   python                          11617MiB |\n",
      "|    0   N/A  N/A   1769868      C   /usr/bin/python3                 1467MiB |\n",
      "|    0   N/A  N/A   1825649      C   /usr/bin/python3                 8673MiB |\n",
      "|    0   N/A  N/A   2094650      C   python                          20325MiB |\n",
      "|    1   N/A  N/A   1533592      C   .../streamlit/bin/python3.11     1871MiB |\n",
      "|    1   N/A  N/A   1553645      C   /usr/bin/python3                  841MiB |\n",
      "|    1   N/A  N/A   1825649      C   /usr/bin/python3                  839MiB |\n",
      "|    1   N/A  N/A   2094650      C   python                          20313MiB |\n",
      "|    1   N/A  N/A   2193850      C   ...onda3/envs/api/bin/python     1505MiB |\n",
      "|    2   N/A  N/A   1715842      C   /usr/bin/python3                33427MiB |\n",
      "|    3   N/A  N/A   1715842      C   /usr/bin/python3                33381MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ab2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bce59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "\n",
    "    #각 stage에서의 batch size\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    #data set정의 \n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    # 클러스터 수와 topic의 수는 20 (k==20)\n",
    "    parser.add_argument('--n-cluster', default=3, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    # 단어vocabulary는 2000로 setting\n",
    "    parser.add_argument('--n-word', default=30000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1,2,3], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "   \n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6021b3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "--------------------------------------------------------------------------------\n",
      "comp.graphics\n",
      "--------------------------------------------------------------------------------\n",
      "comp.os.ms-windows.misc\n",
      "--------------------------------------------------------------------------------\n",
      "comp.sys.ibm.pc.hardware\n",
      "--------------------------------------------------------------------------------\n",
      "comp.sys.mac.hardware\n",
      "--------------------------------------------------------------------------------\n",
      "comp.windows.x\n",
      "--------------------------------------------------------------------------------\n",
      "misc.forsale\n",
      "--------------------------------------------------------------------------------\n",
      "rec.autos\n",
      "--------------------------------------------------------------------------------\n",
      "rec.motorcycles\n",
      "--------------------------------------------------------------------------------\n",
      "rec.sport.baseball\n",
      "--------------------------------------------------------------------------------\n",
      "rec.sport.hockey\n",
      "--------------------------------------------------------------------------------\n",
      "sci.crypt\n",
      "--------------------------------------------------------------------------------\n",
      "sci.electronics\n",
      "--------------------------------------------------------------------------------\n",
      "sci.med\n",
      "--------------------------------------------------------------------------------\n",
      "sci.space\n",
      "--------------------------------------------------------------------------------\n",
      "soc.religion.christian\n",
      "--------------------------------------------------------------------------------\n",
      "talk.politics.guns\n",
      "--------------------------------------------------------------------------------\n",
      "talk.politics.mideast\n",
      "--------------------------------------------------------------------------------\n",
      "talk.politics.misc\n",
      "--------------------------------------------------------------------------------\n",
      "talk.religion.misc\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# 20newsgroups 데이터셋 불러오기\n",
    "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "\n",
    "documents = dataset.data\n",
    "targets = dataset.target\n",
    "target_names = dataset.target_names\n",
    "\n",
    "# 각 주제별로 한 개의 문서를 선택하여 출력\n",
    "for i in range(len(target_names)):\n",
    "    # 현재 주제와 일치하는 문서들의 인덱스 찾기\n",
    "    doc_indices = np.where(targets == i)[0]\n",
    "    \n",
    "    # 첫 번째 문서의 인덱스 선택\n",
    "    doc_idx = doc_indices[0]\n",
    "    \n",
    "    # 해당 문서와 주제 이름 출력\n",
    "    print(f\"{target_names[i]}\")\n",
    "    #print(f\"문서 예시:\\n{documents[doc_idx][:500]}\")  # 첫 500자만 출력\n",
    "    print(\"-\" * 80)\n",
    "# 20newsgroups 데이터셋의 주제들을 'politics', 'sport', 'tech'로 매핑하는 함수를 작성합니다.\n",
    "\n",
    "def map_topics_to_labels(targets, target_names):\n",
    "    # 주제를 라벨로 매핑하는 딕셔너리\n",
    "    topic_to_label = {\n",
    "        'talk.politics.guns': 'politics',\n",
    "        'talk.politics.mideast': 'politics',\n",
    "        'talk.politics.misc': 'politics',\n",
    "        'rec.sport.baseball': 'sport',\n",
    "        'rec.sport.hockey': 'sport',\n",
    "        'comp.os.ms-windows.misc': 'tech',\n",
    "        'comp.sys.ibm.pc.hardware': 'tech',\n",
    "        'comp.sys.mac.hardware': 'tech',\n",
    "        'comp.windows.x': 'tech'\n",
    "    }\n",
    "\n",
    "    # 타겟(주제) ID를 라벨로 변환\n",
    "    labels = np.array([topic_to_label.get(target_names[target], \"other\") for target in targets])\n",
    "    return labels\n",
    "\n",
    "# 20newsgroups 데이터셋의 타겟을 'politics', 'sport', 'tech' 라벨로 매핑\n",
    "mapped_labels = map_topics_to_labels(targets, target_names)\n",
    "\n",
    "# 결과 확인을 위해 첫 10개의 매핑된 라벨을 출력\n",
    "mapped_labels[:10]\n",
    "# 'politics', 'sport', 'tech' 라벨을 가진 문서만 필터링하는 함수\n",
    "\n",
    "def filter_documents(documents, labels, valid_labels):\n",
    "    # 유효한 라벨을 가진 문서의 인덱스를 찾습니다.\n",
    "    valid_indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "\n",
    "    # 해당 인덱스의 문서만 필터링합니다.\n",
    "    filtered_documents = [documents[i] for i in valid_indices]\n",
    "    filtered_labels = [labels[i] for i in valid_indices]\n",
    "\n",
    "    return filtered_documents, filtered_labels\n",
    "\n",
    "# 'politics', 'sport', 'tech' 라벨을 가진 문서 필터링\n",
    "valid_labels = ['politics', 'sport', 'tech']\n",
    "filtered_documents, filtered_labels = filter_documents(documents, mapped_labels, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f0c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_df = pd.read_csv('nyt-articles-2020.csv')\n",
    "\n",
    "newsdesk_to_label = {\n",
    "    'Science': 'tech',\n",
    "    'Technology': 'tech',\n",
    "    'Business': 'tech',\n",
    "    'Sports': 'sport',\n",
    "    'OpEd': 'politics',\n",
    "    'U.S.': 'politics',\n",
    "    'New York': 'politics',\n",
    "    'Politics': 'politics'\n",
    "}\n",
    "\n",
    "nyt_df['label'] = nyt_df['newsdesk'].map(newsdesk_to_label)\n",
    "filtered_nyt_df = nyt_df[nyt_df['label'].isin(['politics', 'sport', 'tech'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6b2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_folder_path = '/mnt/ssd1/don_ssd1/twitter_crawling_don/UTopic/data/bbc'\n",
    "labels = ['politics', 'sport', 'tech']\n",
    "\n",
    "data = []\n",
    "for label in labels:\n",
    "    folder_path = os.path.join(data_folder_path, label)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            data.append({'label': label, 'document': content})\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_data = list(zip(filtered_documents, filtered_labels))\n",
    "nyt_data = list(zip(filtered_nyt_df['abstract'].tolist(), filtered_nyt_df['label'].tolist()))\n",
    "bbc_data = list(zip(df['document'].tolist(), df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54ae0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 샘플링 사이즈와 라벨 설정\n",
    "total_sample_size = 3600\n",
    "labels = ['politics', 'sport', 'tech']\n",
    "platforms = ['20newsgroups', 'nyt', 'bbc']\n",
    "samples_per_label = total_sample_size // len(labels)\n",
    "\n",
    "# 데이터와 라벨, 플랫폼을 라벨별로 분리하여 저장하는 함수\n",
    "def sample_by_label(data, label, num_samples, platform):\n",
    "    filtered_data = [(text, lbl, platform) for text, lbl in data if lbl == label]\n",
    "    return random.sample(filtered_data, min(num_samples, len(filtered_data)))\n",
    "\n",
    "# 데이터를 훈련과 테스트로 분할하는 함수\n",
    "def split_train_test(data, train_ratio=0.8):\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    return data[:train_size], data[train_size:]\n",
    "\n",
    "# 훈련 데이터에서 검증 데이터 분리하는 함수\n",
    "def split_train_valid(data, valid_ratio_from_train=0.1):\n",
    "    valid_size = int(len(data) * valid_ratio_from_train)\n",
    "    train_size = len(data) - valid_size\n",
    "    return data[:train_size], data[train_size:]\n",
    "\n",
    "train_total_text_list = []\n",
    "train_total_label_list = []\n",
    "valid_total_text_list = []\n",
    "valid_total_label_list = []\n",
    "test_total_text_list = []\n",
    "test_total_label_list = []\n",
    "train_total_platform_list = []\n",
    "valid_total_platform_list = []\n",
    "test_total_platform_list = []\n",
    "\n",
    "# 각 데이터셋에 대해 라벨별로 샘플링\n",
    "data_sources = [(newsgroups_data, '20newsgroups'), (nyt_data, 'nyt'), (bbc_data, 'bbc')]\n",
    "\n",
    "for label in labels:\n",
    "    for data, platform in data_sources:\n",
    "        sampled_data = sample_by_label(data, label, samples_per_label // len(labels), platform)\n",
    "        # 훈련 및 테스트 데이터 분할\n",
    "        train_data, test_data = split_train_test(sampled_data)\n",
    "        # 훈련 데이터에서 검증 데이터 분리\n",
    "        train_data, valid_data = split_train_valid(train_data)\n",
    "\n",
    "        train_total_text_list.extend([text for text, lbl, plt in train_data])\n",
    "        train_total_label_list.extend([lbl for text, lbl, plt in train_data])\n",
    "        train_total_platform_list.extend([plt for text, lbl, plt in train_data])\n",
    "        valid_total_text_list.extend([text for text, lbl, plt in valid_data])\n",
    "        valid_total_label_list.extend([lbl for text, lbl, plt in valid_data])\n",
    "        valid_total_platform_list.extend([plt for text, lbl, plt in valid_data])\n",
    "        test_total_text_list.extend([text for text, lbl, plt in test_data])\n",
    "        test_total_label_list.extend([lbl for text, lbl, plt in test_data])\n",
    "        test_total_platform_list.extend([plt for text, lbl, plt in test_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7183cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012842e",
   "metadata": {},
   "source": [
    "len(set(['coinex', 'announces', 'seos', 'chatgptpowered', 'launches', 'bigdata', 'unveils', 'openaichatgpt', 'tags', 'hn', 'stablediffusion', 'chatgptstyle', 'reportedly', 'mba', 'marketers', 'baidu', 'technews', 'fintech', 'chatgptlike', 'elonmusk', 'notion', 'goog', 'googleai', 'digitalmarketing', 'artificalintelligence', 'rt', 'googl', 'bardai', 'edtech', 'malware', 'wharton', 'agix', 'chatgptplus', 'datascience', 'deeplearning', 'msft', 'weirdness', 'tweets', 'amid', 'aitools', 'cybersecurity', 'airdrop', 'cc', 'valentines', 'startups', 'snapchat', 'generativeai', 'buzzfeed', 'fastestgrowing', 'anthropic', 'maker', 'rival', 'techcrunch', 'aiart', 'nocode', 'invests', 'cybercriminals', 'abstracts', 'nyc', 'webinar', 'retweet', 'educators', 'brilliance', 'rescue', 'daysofcode', 'gm', 'rtechnology', 'linkedin', 'licensing', 'copywriting', 'copywriters', 'contentmarketing', 'revolutionizing', 'technologynews', 'warns', 'metaverse', 'cofounder', 'trending', 'founders', 'aipowered', 'openaichat', 'releases', 'microsofts', 'chinas', 'infosec', 'launching', 'jasper', 'nfts', 'newsletter', 'chatgptgod', 'futureofwork', 'digitaltransformation', 'founder', 'feb', 'buzz', 'rn', 'ux', 'courtesy', 'nick', 'claude',\n",
    "        'remindme', 'giphy', 'gif', 'deleted', 'giphydownsized', 'chadgpt', 'removed', 'patched', 'nerfed', 'yup', 'waitlist', 'refresh', 'sydney', 'mods', 'nsfw', 'characterai', 'screenshot', 'downvoted', 'youcom', 'meth', 'ascii', 'karma', 'hahaha', 'hangman', 'chatopenaicom', 'emojis', 'porn', 'redditor', 'vpn', 'upvotes', 'blah', 'upvote', 'violated', 'yep', 'joking', 'nope', 'offended', 'mod', 'bruh', 'roleplay', 'ops', 'bob', 'dans', 'redditors', 'nerf', 'firefox', 'trolling', 'sarcastic', 'huh', 'turbo', 'troll', 'patch', 'tag', 'url', 'sus', 'erotica', 'chad', 'gotcha', 'basilisk', 'login', 'lmfao', 'temperature', 'poll', 'emoji', 'rick', 'dm', 'jailbreak', 'orange', 'sub', 'quack', 'davinci', 'uh', 'flagged', 'op', 'markdown', 'flair', 'cares', 'refreshing', 'hitler', 'cookies', 'hmm', 'yikes', 'erotic', 'gti', 'paywall', 'elaborate', 'yea', 'ah', 'uncensored', 'rude', 'colour', 'bitch', 'therapy', 'neutered', 'deny', 'chats', 'jailbroken', 'cake', 'dungeon', 'dang',\n",
    "        'zronx', 'tuce', 'jontron', 'levy', 'bishop', 'rook', 'thumbnail', 'quotquot', 'jon', 'linus', 'hrefaboutinvalidzcsafeza', 'beluga', 'vid', 'bhai', 'gemx', 'raid', 'ohio', 'circle', 'subscribed', 'anna', 'stare', 'canva', 'napster', 'shapiro', 'sponsor', 'broker', 'websiteapp', 'manoj', 'subscriber', 'bluewillow', 'alex', 'vids', 'legends', 'ryan', 'shes', 'hackbanzer', 'quotoquot', 'pictory', 'youtuber', 'profitable', 'pawn', 'joma', 'folders', 'lifechanging', 'thomas', 'ur', 'plz', 'mike', 'scott', 'casey', 'adrian', 'enjoyed', 'stockfish', 'invideo', 'shortlisted', 'hikaru', 'bless', 'corpsb', 'chatgbt', 'bfuture', 'curve', 'accent', 'amc', 'tutorials', 'gotham', 'mrs', 'earning', 'bra', 'elo', 'oliver', 'youtubers', 'quotcontinuequot', 'membership', 'labels', 'dagogo', 'eonr', 'hai', 'quotai', 'affiliate', 'congratulationsbryou', 'subscribers', 'thumbnails', 'azn', 'beast', 'tom', 'trader', 'garetz', 'quot', 'subbed', 'pls', 'quotchatgpt', 'gtp', 'machina', 'quoti', 'bret', 'terminator', 'watchingbrdm', 'quothow', 'nowi', 'mint']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff82213",
   "metadata": {},
   "source": [
    "len(set(['chatgptpowered', 'announces', 'seos', 'launches', 'unveils', 'bigdata', 'stablediffusion', 'coinex', 'chatgptstyle', 'tags', 'mba', 'baidu', 'openaichatgpt', 'edtech', 'technews', 'reportedly', 'notion', 'wharton', 'chatgptlike', 'hn', 'fintech', 'artificalintelligence', 'googleai', 'marketers', 'bardai', 'elonmusk', 'goog', 'googl', 'msft', 'abstracts', 'chatgptplus', 'rescue', 'rt', 'tweets', 'deeplearning', 'datascience', 'malware', 'generativeai', 'buzzfeed', 'digitalmarketing', 'agix', 'invests', 'amid', 'airdrop', 'anthropic', 'retweet', 'rn', 'cybercriminals', 'coauthor', 'rival', 'educators', 'forbes', 'startups', 'chatgptgod', 'snapchat', 'infosec', 'aiart', 'cybersecurity', 'valentines', 'technologynews', 'copywriting', 'courtesy', 'chinas', 'newsletter', 'gm', 'maker', 'aipowered', 'gamechanger', 'cc', 'daysofcode', 'nfts', 'nocode', 'techcrunch', 'ux', 'linkedin', 'metaverse', 'weirdness', 'webinar', 'futureofwork', 'blockchain', 'bullish', 'socialmedia', 'edchat', 'ernie', 'azure', 'fastestgrowing', 'launching', 'cofounder', 'tweeting', 'hackers', 'trending', 'bloomberg', 'ftw', 'licensing', 'competitor', 'chatgptai', 'classrooms', 'integrating', 'gtgt', 'openaichat', 'brilliance', 'chatgptdown', 'microsofts', 'warns', 'jpeg', 'chatsonic', 'viral', 'aitools', 'frenzy', 'claude', 'alibaba', 'wsj', 'bings', 'fet', 'founders', 'aiwritten', 'layoffs', 'contentmarketing', 'opera', 'iot', 'digitaltransformation', 'jasper', 'ethereum', 'mindblowing', 'classroom', 'everyones', 'cnet', 'phishing', 'haiku', 'chatgptmaker', 'yall', 'revolutionize', 'disrupt', 'kenyan', 'googlebard', 'releases', 'feb', 'buzz', 'passes', 'yearold', 'march', 'storm', 'bb', 'dey', 'introduces', 'firms', 'nft', 'startup', 'atlantic', 'valuation', 'chatgptgenerated', 'qampa', 'mem', 'rtechnology', 'insider', 'jpmorgan', 'craze', 'revolutionizing', 'bans', 'aichatgpt', 'whatsapp', 'digitalhealth', 'blurry', 'ens', 'davos', 'officially', 'analytics', 'york', 'nyc', 'weekend', 'aichatbot', 'obsessed', 'bingchatgpt', 'trends', 'leveraging', 'tipping', 'boost', 'killer', 'november', 'pros', 'unroll', 'introducing', 'integrates', 'nlp', 'tweet', 'gainer', 'launched', 'imgnai', 'cave', 'changer', 'highered', 'copywriters', 'agrees', 'multibilliondollar', 'nick', 'fastest', 'contentcreation', 'ht', 'founder', 'fluent',\n",
    "'deleted', 'giphy', 'remindme', 'gif', 'removed', 'giphydownsized', 'patched', 'downvoted', 'nerfed', 'sydney', 'nsfw', 'youcom', 'characterai', 'blah', 'chadgpt', 'yup', 'refresh', 'ascii', 'waitlist', 'hangman', 'redditor', 'meth', 'huh', 'vpn', 'porn', 'nope', 'screenshot', 'quack', 'upvote', 'emojis', 'nerf', 'mods', 'uncensored', 'karma', 'firefox', 'hahaha', 'archived', 'fucked', 'temperature', 'cringe', 'yep', 'chatopenaicom', 'davinci', 'violated', 'rick', 'cookies', 'upvotes', 'hitler', 'offended', 'roleplay', 'cows', 'sus', 'woah', 'trolling', 'chad', 'erotic', 'bud', 'ampd', 'patch', 'sub', 'troll', 'lobotomized', 'bruh', 'sarcastic', 'turbo', 'jailbreaking', 'moderation', 'deny', 'poll', 'orange', 'jailbreak', 'complaining', 'flagged', 'smut', 'bitch', 'op', 'url', 'logged', 'neutered', 'tag', 'ah', 'cake', 'chats', 'wokegpt', 'lmfao', 'asshole', 'erotica', 'delete', 'ops', 'downvote', 'cache', 'gay', 'yea', 'joking', 'lame', 'dm', 'unrestricted', 'login', 'gti', 'rude', 'markdown', 'therapist', 'mod', 'redditors', 'inspect', 'refreshing', 'bypass', 'colour', 'rip', 'workaround', 'retarded', 'yikes', 'sarcasm', 'lasted', 'moron', 'forgets', 'username', 'regenerate', 'basilisk', 'triggered', 'uh', 'hourly', 'jailbroken', 'youchat', 'playground', 'bob', 'flair', 'oof', 'vram', 'repost', 'counting', 'idiot', 'fart', 'eh', 'emoji', 'edgy', 'censoring', 'nah', 'dick', 'morty', 'tos', 'cgpt', 'screenshots', 'tldr', 'vietnam', 'wtf', 'guessing', 'automoderator', 'chatgptx', 'formatting', 'haikusbot', 'hmm', 'angry', 'clinicalillusionist', 'poop', 'nazi', 'insult', 'svg', 'censored', 'filters', 'nerfing', 'cutoff', 'nevermind', 'dungeon', 'automod', 'goddamn', 'shitty', 'polite', 'logging', 'wdym', 'filtered', 'iq', 'censor', 'ip', 'chicken', 'novelai', 'dans', 'sex', 'milk', 'gotcha', 'reload', 'sassy', 'dnd', 'blocked', 'retry', 'odd', 'batman', 'locally', 'af', 'donald', 'satire', 'gtit', 'kanye', 'log', 'drugs', 'upvoted', 'closedai', 'annoyed', 'restricted', 'umm',\n",
    "'tuce', 'zronx', 'quotquot', 'jontron', 'levy', 'jon', 'rook', 'bishop', 'beluga', 'thumbnail', 'ohio', 'linus', 'gemx', 'vid', 'hrefaboutinvalidzcsafeza', 'bhai', 'raid', 'stare', 'napster', 'pictory', 'subscribed', 'anna', 'circle', 'ur', 'pawn', 'stockfish', 'websiteapp', 'shapiro', 'ryan', 'gotham', 'manoj', 'subscriber', 'broker', 'folders', 'sponsor', 'youtubers', 'hikaru', 'bluewillow', 'ltlets', 'canva', 'joma', 'shorts', 'legends', 'lifechanging', 'hackbanzer', 'labels', 'vids', 'membership', 'profitable', 'scott', 'mrs', 'shes', 'adrian', 'bless', 'earning', 'maher', 'quothow', 'chatgbt', 'affiliate', 'oliver', 'thomas', 'shortlisted', 'subscribers', 'elo', 'alex', 'quotoquot', 'plz', 'jim', 'invideo', 'corpsb', 'bfuture', 'hai', 'enjoyed', 'mike', 'terminator', 'thx', 'trader', 'quot', 'gtp', 'quotchatgpt', 'amc', 'youtuber', 'tom', 'quoti', 'quotai', 'greg', 'accent', 'antichrist', 'subs', 'yt', 'gbt', 'curve', 'brother', 'tutorials', 'ben', 'shadow', 'nowi', 'quotcontinuequot', 'congratulationsbryou', 'ka', 'watchingbrdm', 'dagogo', 'pls', 'fx', 'garetz', 'bret', 'azn', 'uploaded', 'funds', 'silver', 'ring', 'intro', 'anlt', 'telegram', 'mint', 'ambulance', 'terrifying', 'casey', 'ke', 'brthanks', 'bra', 'bhi', 'machina', 'thankyou', 'vanoss', 'aa', 'kya', 'dread', 'harry', 'mate', 'idk', 'portfolio', 'leila', 'upto', 'legend', 'subbed', 'magnus', 'beast', 'earned', 'mosh', 'checkmate', 'quotit', 'quotim', 'brbr', 'mittens', 'madan', 'quotthe', 'ho', 'ltt', 'sigmoid', 'quotdont', 'helpdesk', 'clip', 'eonr', 'quotyou', 'monique', 'youquot', 'brrepent', 'tutorial', 'rooks', 'grant', 'upload', 'moves', 'laughed', 'shouldve', 'reynolds', 'delirious', 'informative', 'funniest', 'subscribe', 'ki', 'awzx', 'imperative', 'brthank', 'tho', 'quotwhat', 'bri', 'bhaiya', 'xd', 'screwed', 'logo', 'profits', 'quotthis', 'renders', 'channels', 'jarvis', 'hexagon', 'earn', 'heavens', 'lucid', 'kar', 'quotits', 'roblox', 'giveaway', 'knight', 'brit', 'hamish', 'skynet', 'bye', 'couldve'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8440c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, bert, text_list, platform_label, label_list, N_word, vectorizer=None, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.nonempty_text = [text for text in text_list if len(text) > 0]\n",
    "        \n",
    "        # Remove new lines\n",
    "        self.nonempty_text = [re.sub(\"\\n\",\" \", sent) for sent in self.nonempty_text]\n",
    "                \n",
    "        # Remove Emails\n",
    "        self.nonempty_text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove new line characters\n",
    "        self.nonempty_text = [re.sub('\\s+', ' ', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove distracting single quotes\n",
    "        self.nonempty_text = [re.sub(\"\\'\", \"\", sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        self.jargons = set(['homosexual', 'simms', 'scsi', 'dma', 'ulf', 'color', 'armenian', 'widget', 'ide', 'centris', 'braves', 'leafs', 'faq', 'simm', 'gm', 'ethernet', 'stats', 'font', 'espn', 'meg', 'rgb', 'vram', 'buffalo', 'batf', 'printer', 'cpu', 'com', 'bruins', 'toronto', 'armenians', 'gant', 'lebanese', 'pitching', 'koresh', 'icon', 'dos', 'baerga', 'arabs', 'sabres', 'mb', 'dx', 'ottawa', 'azerbaijan', 'mailing', 'bitmap', 'norton', 'jews', 'gif', 'slave', 'pocklington', 'mhz', 'rectum', 'don', 'cdrom', 'ihr', 'irq', 'gld', 'cleveland', 'quadra', 'cursor', 'flames', 'bios', 'runs', 'wings', 'dl', 'penguins', 'motherboard', 'hp', 'gay', 'alomar', 'davidians', 'dale', 'colormap', 'israel', 'shrill', 'mattingly', 'pitcher', 'xr', 'chipset', 'diamond', 'cubs', 'detroit', 'fuhr', 'ordonly', 'floppy', 'gritz', 'sandberg', 'yankees', 'arab', 'clinton', 'controller', 'oh', 'vga', 'xman', 'hey', 'blasphemy', 'farrs', 'bosnia', 'wondering', 'libxmulibxmua', 'trumps', 'covid', 'biden', 'trump', 'vaccine', 'republicans', 'stock', 'outbreak', 'quarterback', 'chiefs', 'nfl', 'bidens', 'quarantine', 'lockdown', 'twitter', 'patriots', 'bowl', 'cancellation', 'facebook', 'ravens', 'steelers', 'canceled', 'soccer', 'woods', 'progressive', 'immune', 'pandemic', 'nba', 'colts', 'infection', 'gauff', 'vicepresidential', 'hospitalized', 'caucus', 'lawmaker', 'buttigieg', 'antibody', 'basketball', 'postseason', 'viral', 'blessing', 'marshmallow', 'ncaa', 'genetic', 'bernie', 'cheating', 'confront', 'djokovic', 'kentucky', 'bloombergs', 'pelicans', 'mate', 'distancing', 'floyds', 'patient', 'seahawks', 'pga', 'nomination', 'mds', 'tokyo', 'upended', 'biotech', 'lakers', 'immunity', 'shutdown', 'clinical', 'presidentelect', 'obama', 'scientist', 'readers', 'presidential', 'mlb', 'columnist', 'mask', 'furlough', 'republican', 'obsessing', 'donald', 'sanders', 'narrative', 'indistinguishable', 'astros', 'gym', 'coronavirus', 'apex', 'uncertainty', 'normalcy', 'miami', 'tennessee', 'rays', 'jacksonville', 'nominee', 'insect', 'alarmist', 'bee', 'safely', 'instagram', 'michigan', 'expanded', 'amazon', 'ukip', 'blog', 'patent', 'kenteris', 'mourinho', 'henman', 'moya', 'kilroysilk', 'gerrard', 'mirza', 'spam', 'thanou', 'wenger', 'souness', 'commodore', 'uwb', 'holmes', 'benitez', 'conte', 'bittorrent', 'blunkett', 'mock', 'spyware', 'ds', 'liverpool', 'robot', 'hague', 'iaaf', 'domain', 'parry', 'roddick', 'eu', 'bluray', 'agassi', 'nintendo', 'bt', 'aragones', 'screensaver', 'asylum', 'lycos', 'argonaut', 'hunting', 'hewitt', 'capriati', 'donation', 'firefox', 'sec', 'cabir', 'bnp', 'oleary', 'wifi', 'ferguson', 'safin', 'anelka', 'davenport', 'bellamy', 'blackpool', 'hiphop', 'robben', 'rfid', 'uefa', 'federer', 'drinking', 'skype', 'psp', 'radcliffe', 'cunningham', 'lions', 'hingis', 'straw', 'directive', 'dvd', 'ogara', 'ruddock', 'seafarer', 'hunt', 'browser', 'livingstone', 'seed', 'southampton', 'mcletchie', 'award', 'jaynes', 'poster', 'balco', 'referendum', 'edinburgh', 'hdtv', 'pension', 'indoor', 'mini', 'simonetti', 'grid', 'chepkemei', 'wilkinson', 'robinson', 'marathon', 'ink', 'dvds', 'chelsea'])\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert)\n",
    "        self.model = AutoModel.from_pretrained(bert)\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        self.N_word = N_word\n",
    "        \n",
    "        if vectorizer == None:\n",
    "            self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "            self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text))\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            \n",
    "        self.org_list = []\n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            org_input = self.tokenizer(sent, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "            org_input['input_ids'] = torch.squeeze(org_input['input_ids'])\n",
    "            org_input['attention_mask'] = torch.squeeze(org_input['attention_mask'])\n",
    "            self.org_list.append(org_input)\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "        ## platform label    \n",
    "        self.platform_label_list = platform_label\n",
    "        self.label_list = label_list\n",
    "            \n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray()\n",
    "        vectorized_input = vectorized_input.astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                             for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp    \n",
    "    \n",
    "    # mean_pooling 함수 정의\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()\n",
    "        sum_embeddings = torch.sum(model_output * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nonempty_text)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.nonempty_text[idx]\n",
    "        encoded_input = self.tokenizer(sentence, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        # mean_pooling 함수를 사용하여 문장 임베딩의 평균을 계산\n",
    "        pooled_embedding = self.mean_pooling(model_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "\n",
    "        return self.org_list[idx], self.bow_list[idx], pooled_embedding, self.platform_label_list[idx], self.label_list[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8d1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "# textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac5ae4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"모든 랜덤 요소에 대한 시드를 고정합니다.\"\"\"\n",
    "    random.seed(seed_value)  \n",
    "    np.random.seed(seed_value)  \n",
    "    torch.manual_seed(seed_value)  \n",
    "    if torch.cuda.is_available():  \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a56d5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb9c2718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 08:00:43.216871: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-15 08:00:43.377873: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-15 08:00:44.051537: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2024-03-15 08:00:44.051627: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2024-03-15 08:00:44.051635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 2583/2583 [00:06<00:00, 391.89it/s]\n",
      "100%|██████████| 287/287 [00:00<00:00, 433.19it/s]\n",
      "100%|██████████| 713/713 [00:01<00:00, 442.54it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=train_total_text_list, platform_label=train_total_platform_list, label_list=train_total_label_list, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "valds = BertDataset(bert=bert_name, text_list=valid_total_text_list, platform_label=valid_total_platform_list, label_list=valid_total_label_list, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds = BertDataset(bert=bert_name, text_list=test_total_text_list, platform_label=test_total_platform_list, label_list=test_total_label_list, N_word=n_word, vectorizer=None, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba29252",
   "metadata": {},
   "source": [
    "# 원본 데이터 Mean pooling 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2229cd",
   "metadata": {},
   "source": [
    "# mean_pooling 함수 정의\n",
    "def mean_pooling(embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbb71fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainds\n",
    "trainds_embeddings = []\n",
    "for item in trainds:\n",
    "    _, _, pooled_embedding, _, _ = item \n",
    "    trainds_embeddings.append(pooled_embedding)\n",
    "\n",
    "train_mean_pooled_embeddings = torch.stack(trainds_embeddings)\n",
    "\n",
    "# valds\n",
    "valds_embeddings = []\n",
    "for item in valds:\n",
    "    _, _, pooled_embedding, _, _ = item \n",
    "    valds_embeddings.append(pooled_embedding)\n",
    "\n",
    "val_mean_pooled_embeddings = torch.stack(valds_embeddings)\n",
    "\n",
    "# testds\n",
    "testds_embeddings = []\n",
    "for item in testds:\n",
    "    _, _, pooled_embedding, _, _ = item  \n",
    "    testds_embeddings.append(pooled_embedding)\n",
    "\n",
    "test_mean_pooled_embeddings = torch.stack(testds_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd08ab9",
   "metadata": {},
   "source": [
    "# Re_fornulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85826fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97909bc",
   "metadata": {},
   "source": [
    "# Get pos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89a726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_cosine_similarity_indices(mean_pooled_embeddings, batch_size=500):\n",
    "    n_rows = mean_pooled_embeddings.shape[0]\n",
    "    max_similarity_indices = np.zeros(n_rows, dtype=np.int64)\n",
    "\n",
    "    for start_idx in range(0, n_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_rows)\n",
    "        batch_data = mean_pooled_embeddings[start_idx:end_idx]\n",
    "\n",
    "        batch_similarity = cosine_similarity(batch_data, mean_pooled_embeddings)\n",
    "\n",
    "        # 자기 자신과의 유사도를 -1로 설정\n",
    "        for i, original_idx in enumerate(range(start_idx, end_idx)):\n",
    "            batch_similarity[i, original_idx] = -1\n",
    "\n",
    "        # 각 행에서 가장 큰 값을 가진 인덱스 찾기\n",
    "        max_indices = np.argmax(batch_similarity, axis=1)\n",
    "        max_similarity_indices[start_idx:end_idx] = max_indices\n",
    "\n",
    "    return max_similarity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3016fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_pooled_embeddings를 2차원 형태로 변환\n",
    "train_mean_pooled_embeddings_2d = train_mean_pooled_embeddings.squeeze()\n",
    "val_mean_pooled_embeddings_2d = val_mean_pooled_embeddings.squeeze()\n",
    "test_mean_pooled_embeddings_2d = test_mean_pooled_embeddings.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc96d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 매트릭스 계산\n",
    "train_similarity_matrix = compute_max_cosine_similarity_indices(train_mean_pooled_embeddings_2d)\n",
    "val_similarity_matrix = compute_max_cosine_similarity_indices(val_mean_pooled_embeddings_2d)\n",
    "test_similarity_matrix = compute_max_cosine_similarity_indices(test_mean_pooled_embeddings_2d)  # 테스트 데이터셋에 대해서도 동일하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12fdaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab048a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, similarity_matrix, N_word, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        self.N_word = N_word\n",
    "        self.jargons =  set(['homosexual', 'simms', 'scsi', 'dma', 'ulf', 'color', 'armenian', 'widget', 'ide', 'centris', 'braves', 'leafs', 'faq', 'simm', 'gm', 'ethernet', 'stats', 'font', 'espn', 'meg', 'rgb', 'vram', 'buffalo', 'batf', 'printer', 'cpu', 'com', 'bruins', 'toronto', 'armenians', 'gant', 'lebanese', 'pitching', 'koresh', 'icon', 'dos', 'baerga', 'arabs', 'sabres', 'mb', 'dx', 'ottawa', 'azerbaijan', 'mailing', 'bitmap', 'norton', 'jews', 'gif', 'slave', 'pocklington', 'mhz', 'rectum', 'don', 'cdrom', 'ihr', 'irq', 'gld', 'cleveland', 'quadra', 'cursor', 'flames', 'bios', 'runs', 'wings', 'dl', 'penguins', 'motherboard', 'hp', 'gay', 'alomar', 'davidians', 'dale', 'colormap', 'israel', 'shrill', 'mattingly', 'pitcher', 'xr', 'chipset', 'diamond', 'cubs', 'detroit', 'fuhr', 'ordonly', 'floppy', 'gritz', 'sandberg', 'yankees', 'arab', 'clinton', 'controller', 'oh', 'vga', 'xman', 'hey', 'blasphemy', 'farrs', 'bosnia', 'wondering', 'libxmulibxmua', 'trumps', 'covid', 'biden', 'trump', 'vaccine', 'republicans', 'stock', 'outbreak', 'quarterback', 'chiefs', 'nfl', 'bidens', 'quarantine', 'lockdown', 'twitter', 'patriots', 'bowl', 'cancellation', 'facebook', 'ravens', 'steelers', 'canceled', 'soccer', 'woods', 'progressive', 'immune', 'pandemic', 'nba', 'colts', 'infection', 'gauff', 'vicepresidential', 'hospitalized', 'caucus', 'lawmaker', 'buttigieg', 'antibody', 'basketball', 'postseason', 'viral', 'blessing', 'marshmallow', 'ncaa', 'genetic', 'bernie', 'cheating', 'confront', 'djokovic', 'kentucky', 'bloombergs', 'pelicans', 'mate', 'distancing', 'floyds', 'patient', 'seahawks', 'pga', 'nomination', 'mds', 'tokyo', 'upended', 'biotech', 'lakers', 'immunity', 'shutdown', 'clinical', 'presidentelect', 'obama', 'scientist', 'readers', 'presidential', 'mlb', 'columnist', 'mask', 'furlough', 'republican', 'obsessing', 'donald', 'sanders', 'narrative', 'indistinguishable', 'astros', 'gym', 'coronavirus', 'apex', 'uncertainty', 'normalcy', 'miami', 'tennessee', 'rays', 'jacksonville', 'nominee', 'insect', 'alarmist', 'bee', 'safely', 'instagram', 'michigan', 'expanded', 'amazon', 'ukip', 'blog', 'patent', 'kenteris', 'mourinho', 'henman', 'moya', 'kilroysilk', 'gerrard', 'mirza', 'spam', 'thanou', 'wenger', 'souness', 'commodore', 'uwb', 'holmes', 'benitez', 'conte', 'bittorrent', 'blunkett', 'mock', 'spyware', 'ds', 'liverpool', 'robot', 'hague', 'iaaf', 'domain', 'parry', 'roddick', 'eu', 'bluray', 'agassi', 'nintendo', 'bt', 'aragones', 'screensaver', 'asylum', 'lycos', 'argonaut', 'hunting', 'hewitt', 'capriati', 'donation', 'firefox', 'sec', 'cabir', 'bnp', 'oleary', 'wifi', 'ferguson', 'safin', 'anelka', 'davenport', 'bellamy', 'blackpool', 'hiphop', 'robben', 'rfid', 'uefa', 'federer', 'drinking', 'skype', 'psp', 'radcliffe', 'cunningham', 'lions', 'hingis', 'straw', 'directive', 'dvd', 'ogara', 'ruddock', 'seafarer', 'hunt', 'browser', 'livingstone', 'seed', 'southampton', 'mcletchie', 'award', 'jaynes', 'poster', 'balco', 'referendum', 'edinburgh', 'hdtv', 'pension', 'indoor', 'mini', 'simonetti', 'grid', 'chepkemei', 'wilkinson', 'robinson', 'marathon', 'ink', 'dvds', 'chelsea'])\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "\n",
    "        self.pos_dict = similarity_matrix\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "        self.platform_label_list = self.ds.platform_label_list\n",
    "        self.label_list = self.ds.label_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터셋의 크기를 초과하지 않도록 idx 값을 조정\n",
    "        if idx >= len(self.org_list):\n",
    "            idx = len(self.org_list) - 1  \n",
    "        pos_idx = self.pos_dict[idx]\n",
    "\n",
    "        return idx, self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx], self.platform_label_list[idx], self.label_list[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3abcb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2583/2583 [00:04<00:00, 546.59it/s]\n",
      "100%|██████████| 2583/2583 [45:15<00:00,  1.05s/it] \n",
      "100%|██████████| 287/287 [00:00<00:00, 579.77it/s]\n",
      "100%|██████████| 287/287 [01:10<00:00,  4.09it/s]\n",
      "100%|██████████| 713/713 [00:01<00:00, 613.78it/s]\n",
      "100%|██████████| 713/713 [06:56<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, train_similarity_matrix, n_word, lemmatize=True)\n",
    "valfinetuneds = Stage2Dataset(model.encoder, valds, val_similarity_matrix, n_word, lemmatize=True) \n",
    "testfinetuneds = Stage2Dataset(model.encoder, testds, test_similarity_matrix, n_word, lemmatize=True) \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "745200ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2583,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06d9e537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(finetuneds.embedding_list)\n",
    "len(finetuneds.bow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daded5c",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a3e6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9818d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_measure_hungarian = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86eb85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d91e4",
   "metadata": {},
   "source": [
    "# Seperate Platform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df7a3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from collections import defaultdict\n",
    "\n",
    "class PlatformSampler(Sampler):\n",
    "    def __init__(self, dataset, platform_label):\n",
    "        self.indices = [i for i, label in enumerate(dataset.platform_label_list) if label == platform_label]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e66859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_platform_dataloader(dataset, platform_label, batch_size=32, num_workers=0):\n",
    "    sampler = PlatformSampler(dataset, platform_label)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01506b05",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd17d454",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 10.68118 - dist: 0.04441 - cons: -0.33590\n",
      "Epoch-1 / recon: 10.38936 - dist: 0.04398 - cons: -0.33562\n",
      "Epoch-2 / recon: 10.22574 - dist: 0.04602 - cons: -0.34295\n",
      "Epoch-3 / recon: 10.12088 - dist: 0.04785 - cons: -0.36033\n",
      "Epoch-4 / recon: 10.04564 - dist: 0.04752 - cons: -0.37663\n",
      "Epoch-5 / recon: 9.98726 - dist: 0.04712 - cons: -0.39320\n",
      "Epoch-6 / recon: 9.93895 - dist: 0.04642 - cons: -0.40729\n",
      "Epoch-7 / recon: 9.89825 - dist: 0.04492 - cons: -0.41888\n",
      "Epoch-8 / recon: 9.86333 - dist: 0.04387 - cons: -0.42810\n",
      "Epoch-9 / recon: 9.83305 - dist: 0.04332 - cons: -0.43593\n",
      "Epoch-10 / recon: 9.80656 - dist: 0.04257 - cons: -0.44235\n",
      "Epoch-11 / recon: 9.78316 - dist: 0.04182 - cons: -0.44817\n",
      "Epoch-12 / recon: 9.76237 - dist: 0.04155 - cons: -0.45309\n",
      "Epoch-13 / recon: 9.74369 - dist: 0.04114 - cons: -0.45738\n",
      "Epoch-14 / recon: 9.72688 - dist: 0.04090 - cons: -0.46125\n",
      "Epoch-15 / recon: 9.71166 - dist: 0.04071 - cons: -0.46483\n",
      "Epoch-16 / recon: 9.69781 - dist: 0.04033 - cons: -0.46794\n",
      "Epoch-17 / recon: 9.68514 - dist: 0.04008 - cons: -0.47067\n",
      "Epoch-18 / recon: 9.67347 - dist: 0.03995 - cons: -0.47324\n",
      "Epoch-19 / recon: 9.66271 - dist: 0.03996 - cons: -0.47564\n",
      "Epoch-20 / recon: 9.65276 - dist: 0.03983 - cons: -0.47779\n",
      "Epoch-21 / recon: 9.64352 - dist: 0.03957 - cons: -0.47984\n",
      "Epoch-22 / recon: 9.63487 - dist: 0.03941 - cons: -0.48169\n",
      "Epoch-23 / recon: 9.62682 - dist: 0.03932 - cons: -0.48354\n",
      "Epoch-24 / recon: 9.61926 - dist: 0.03932 - cons: -0.48524\n",
      "Epoch-25 / recon: 9.61216 - dist: 0.03923 - cons: -0.48682\n",
      "Epoch-26 / recon: 9.60550 - dist: 0.03914 - cons: -0.48836\n",
      "Epoch-27 / recon: 9.59920 - dist: 0.03894 - cons: -0.48982\n",
      "Epoch-28 / recon: 9.59321 - dist: 0.03870 - cons: -0.49125\n",
      "Epoch-29 / recon: 9.58757 - dist: 0.03861 - cons: -0.49261\n",
      "Epoch-30 / recon: 9.58218 - dist: 0.03854 - cons: -0.49391\n",
      "Epoch-31 / recon: 9.57706 - dist: 0.03841 - cons: -0.49522\n",
      "Epoch-32 / recon: 9.57222 - dist: 0.03831 - cons: -0.49647\n",
      "Epoch-33 / recon: 9.56760 - dist: 0.03827 - cons: -0.49764\n",
      "Epoch-34 / recon: 9.56320 - dist: 0.03820 - cons: -0.49878\n",
      "Epoch-35 / recon: 9.55899 - dist: 0.03815 - cons: -0.49986\n",
      "Epoch-36 / recon: 9.55492 - dist: 0.03803 - cons: -0.50092\n",
      "Epoch-37 / recon: 9.55101 - dist: 0.03789 - cons: -0.50195\n",
      "Epoch-38 / recon: 9.54726 - dist: 0.03776 - cons: -0.50289\n",
      "Epoch-39 / recon: 9.54366 - dist: 0.03755 - cons: -0.50387\n",
      "Epoch-40 / recon: 9.54023 - dist: 0.03742 - cons: -0.50480\n",
      "Epoch-41 / recon: 9.53689 - dist: 0.03733 - cons: -0.50570\n",
      "Epoch-42 / recon: 9.53369 - dist: 0.03729 - cons: -0.50659\n",
      "Epoch-43 / recon: 9.53060 - dist: 0.03723 - cons: -0.50741\n",
      "Epoch-44 / recon: 9.52763 - dist: 0.03725 - cons: -0.50822\n",
      "Epoch-45 / recon: 9.52475 - dist: 0.03717 - cons: -0.50901\n",
      "Epoch-46 / recon: 9.52198 - dist: 0.03712 - cons: -0.50975\n",
      "Epoch-47 / recon: 9.51928 - dist: 0.03708 - cons: -0.51051\n",
      "Epoch-48 / recon: 9.51665 - dist: 0.03701 - cons: -0.51123\n",
      "Epoch-49 / recon: 9.51411 - dist: 0.03693 - cons: -0.51197\n",
      "Epoch-50 / recon: 9.51163 - dist: 0.03689 - cons: -0.51270\n",
      "Epoch-51 / recon: 9.50923 - dist: 0.03681 - cons: -0.51341\n",
      "Epoch-52 / recon: 9.50690 - dist: 0.03678 - cons: -0.51409\n",
      "Epoch-53 / recon: 9.50464 - dist: 0.03668 - cons: -0.51478\n",
      "Epoch-54 / recon: 9.50242 - dist: 0.03659 - cons: -0.51544\n",
      "Epoch-55 / recon: 9.50029 - dist: 0.03656 - cons: -0.51608\n",
      "Epoch-56 / recon: 9.49818 - dist: 0.03654 - cons: -0.51672\n",
      "Epoch-57 / recon: 9.49613 - dist: 0.03647 - cons: -0.51735\n",
      "Epoch-58 / recon: 9.49413 - dist: 0.03636 - cons: -0.51798\n",
      "Epoch-59 / recon: 9.49219 - dist: 0.03632 - cons: -0.51856\n",
      "Epoch-60 / recon: 9.49030 - dist: 0.03623 - cons: -0.51913\n",
      "Epoch-61 / recon: 9.48846 - dist: 0.03617 - cons: -0.51969\n",
      "Epoch-62 / recon: 9.48665 - dist: 0.03612 - cons: -0.52026\n",
      "Epoch-63 / recon: 9.48487 - dist: 0.03610 - cons: -0.52080\n",
      "Epoch-64 / recon: 9.48313 - dist: 0.03603 - cons: -0.52136\n",
      "Epoch-65 / recon: 9.48144 - dist: 0.03603 - cons: -0.52188\n",
      "Epoch-66 / recon: 9.47978 - dist: 0.03602 - cons: -0.52239\n",
      "Epoch-67 / recon: 9.47814 - dist: 0.03597 - cons: -0.52290\n",
      "Epoch-68 / recon: 9.47656 - dist: 0.03593 - cons: -0.52342\n",
      "Epoch-69 / recon: 9.47502 - dist: 0.03590 - cons: -0.52390\n",
      "Epoch-70 / recon: 9.47349 - dist: 0.03582 - cons: -0.52437\n",
      "Epoch-71 / recon: 9.47198 - dist: 0.03581 - cons: -0.52485\n",
      "Epoch-72 / recon: 9.47052 - dist: 0.03577 - cons: -0.52531\n",
      "Epoch-73 / recon: 9.46909 - dist: 0.03576 - cons: -0.52577\n",
      "Epoch-74 / recon: 9.46767 - dist: 0.03573 - cons: -0.52623\n",
      "Epoch-75 / recon: 9.46629 - dist: 0.03569 - cons: -0.52666\n",
      "Epoch-76 / recon: 9.46493 - dist: 0.03567 - cons: -0.52711\n",
      "Epoch-77 / recon: 9.46358 - dist: 0.03563 - cons: -0.52752\n",
      "Epoch-78 / recon: 9.46225 - dist: 0.03558 - cons: -0.52793\n",
      "Epoch-79 / recon: 9.46096 - dist: 0.03554 - cons: -0.52833\n",
      "Epoch-80 / recon: 9.45968 - dist: 0.03550 - cons: -0.52873\n",
      "Epoch-81 / recon: 9.45843 - dist: 0.03548 - cons: -0.52912\n",
      "Epoch-82 / recon: 9.45719 - dist: 0.03547 - cons: -0.52952\n",
      "Epoch-83 / recon: 9.45596 - dist: 0.03547 - cons: -0.52989\n",
      "Epoch-84 / recon: 9.45476 - dist: 0.03547 - cons: -0.53026\n",
      "Epoch-85 / recon: 9.45357 - dist: 0.03543 - cons: -0.53063\n",
      "Epoch-86 / recon: 9.45241 - dist: 0.03542 - cons: -0.53099\n",
      "Epoch-87 / recon: 9.45127 - dist: 0.03540 - cons: -0.53134\n",
      "Epoch-88 / recon: 9.45013 - dist: 0.03539 - cons: -0.53167\n",
      "Epoch-89 / recon: 9.44903 - dist: 0.03542 - cons: -0.53202\n",
      "Epoch-90 / recon: 9.44794 - dist: 0.03535 - cons: -0.53236\n",
      "Epoch-91 / recon: 9.44686 - dist: 0.03532 - cons: -0.53269\n",
      "Epoch-92 / recon: 9.44579 - dist: 0.03530 - cons: -0.53300\n",
      "Epoch-93 / recon: 9.44475 - dist: 0.03528 - cons: -0.53331\n",
      "Epoch-94 / recon: 9.44373 - dist: 0.03529 - cons: -0.53361\n",
      "Epoch-95 / recon: 9.44272 - dist: 0.03528 - cons: -0.53391\n",
      "Epoch-96 / recon: 9.44172 - dist: 0.03525 - cons: -0.53420\n",
      "Epoch-97 / recon: 9.44074 - dist: 0.03525 - cons: -0.53449\n",
      "Epoch-98 / recon: 9.43977 - dist: 0.03526 - cons: -0.53477\n",
      "Epoch-99 / recon: 9.43880 - dist: 0.03527 - cons: -0.53505\n",
      "Best Epoch: 79 with NPMI: 0.3283007668441505\n",
      "------- Evaluation results -------\n",
      "topic-0 ['blomberg', 'sid', 'hank', 'greenberg', 'greek', 'housewives', 'umm', 'literal', 'steering', 'unprepared']\n",
      "topic-1 ['zippy', 'spelling', 'discourse', 'blessing', 'saquon', 'deluded', 'severed', 'bravado', 'cox', 'pulitzer']\n",
      "topic-2 ['telecast', 'permanent', 'intricate', 'transformation', 'update', 'business', 'market', 'virus', 'latest', 'news']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import chi2_contingency \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from coherence import get_topic_coherence\n",
    "\n",
    "args.stage_2_repeat = 1\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=True, num_workers=0)\n",
    "    testloader = DataLoader(testfinetuneds, batch_size=bsz, shuffle=True, num_workers=0)\n",
    "    newsgroups_data_trainloader = create_platform_dataloader(finetuneds, '20newsgroups', batch_size=bsz, num_workers=0)\n",
    "    nyt_data_trainloader = create_platform_dataloader(finetuneds, 'nyt', batch_size=bsz, num_workers=0)\n",
    "    bbc_trainloader = create_platform_dataloader(finetuneds, 'bbc', batch_size=bsz, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    best_npmi = -1\n",
    "    best_epoch = 0\n",
    "    best_model_state = None  # 모델 상태를 저장하기 위한 변수\n",
    "    \n",
    "    # 각 플랫폼별 DataLoader의 이터레이터 생성\n",
    "    newsgroups_iter = iter(newsgroups_data_trainloader)\n",
    "    nyt_iter = iter(nyt_data_trainloader)\n",
    "    bbc_iter = iter(bbc_trainloader)\n",
    "\n",
    "    max_length = max(len(newsgroups_data_trainloader), len(nyt_data_trainloader), len(bbc_trainloader))\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # 플랫폼별 DataLoader에서 배치를 순차적으로 가져오기\n",
    "            try:\n",
    "                newsgroups_batch = next(newsgroups_iter)\n",
    "            except StopIteration:\n",
    "                newsgroups_iter = iter(newsgroups_data_trainloader)\n",
    "                newsgroups_batch = next(newsgroups_iter)\n",
    "\n",
    "            try:\n",
    "                nyt_batch = next(nyt_iter)\n",
    "            except StopIteration:\n",
    "                nyt_iter = iter(nyt_data_trainloader)\n",
    "                nyt_batch = next(nyt_iter)\n",
    "\n",
    "            # 기존 코드의 수정 사항\n",
    "            try:\n",
    "                bbc_batch = next(bbc_iter)\n",
    "            except StopIteration:\n",
    "                bbc_iter = iter(bbc_trainloader)  # 새로운 이터레이터 생성\n",
    "                bbc_batch = next(bbc_iter)  # 새로운 이터레이터에서 첫 번째 배치를 가져옴\n",
    "\n",
    "            # 각 배치에 대한 학습 로직 구현\n",
    "            for batch in [newsgroups_batch, nyt_batch, bbc_batch]:\n",
    "                _, org_input, pos_input, org_bow, pos_bow, _ ,_= batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_bow = org_bow.cuda(gpu_ids[0])\n",
    "                pos_input = pos_input.cuda(gpu_ids[0])\n",
    "                pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "                batch_size = org_input.size(0) #org_input_ids.size(0)\n",
    "\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "                # 텐서 크기 맞춰줌\n",
    "                org_dists = org_dists[:, :org_bow.size(1)]\n",
    "                pos_dists = pos_dists[:, :pos_bow.size(1)]\n",
    "\n",
    "                recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * (1-org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * (1-pos_bow), axis=1), axis=0)\n",
    "                recons_loss *= 0.5\n",
    "\n",
    "                # consistency loss\n",
    "                pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "                cons_loss = -pos_sim.mean()\n",
    "\n",
    "                distmatch_loss = dist_match_loss(torch.cat((org_topic,), dim=0), dirichlet_alpha_2)\n",
    "\n",
    "\n",
    "                loss = args.coeff_2_recon * recons_loss + \\\n",
    "                       args.coeff_2_cons * cons_loss + \\\n",
    "                       args.coeff_2_dist * distmatch_loss \n",
    "\n",
    "                losses.update(loss.item(), bsz)\n",
    "                closses.update(cons_loss.item(), bsz)\n",
    "                rlosses.update(recons_loss.item(), bsz)\n",
    "                distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "        # Epoch 마다 실행\n",
    "        model.eval()\n",
    "\n",
    "        # 각 토픽에 대한 상위 10개 단어 추출\n",
    "        top_words_per_topic = {}\n",
    "        for topic_idx in range(model.N_topic):\n",
    "            top_words_indices = model.beta[topic_idx].topk(10).indices\n",
    "            top_words = [vocab_dict_reverse[idx.item()] for idx in top_words_indices]\n",
    "            top_words_per_topic[topic_idx] = top_words\n",
    "            \n",
    "        reference_corpus=[doc.split() for doc in valds.preprocess_ctm(valds.nonempty_text)]\n",
    "        topic_words_list = list(top_words_per_topic.values())\n",
    "        result = get_topic_coherence(topic_words_list, reference_corpus)\n",
    "        avg_npmi = result['NPMI']\n",
    "\n",
    "        # 최적의 NPMI와 epoch 추적\n",
    "        if avg_npmi > best_npmi:\n",
    "            best_npmi = avg_npmi\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()  # 현재 모델 상태 저장\n",
    "\n",
    "    print(f\"Best Epoch: {best_epoch} with NPMI: {best_npmi}\")\n",
    "    # 훈련 완료 후, 최적 모델 상태 저장\n",
    "    torch.save(best_model_state, 'our_best_model_state.pth')\n",
    "    model.load_state_dict(torch.load('our_best_model_state.pth'))\n",
    "    \n",
    "    print(\"------- Evaluation results -------\")\n",
    "    #각 토픽당 가지는 워드셋\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(10, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d2a22",
   "metadata": {},
   "source": [
    "# Hungarian matching & Label purity\n",
    "\n",
    "- 헝가리안 매칭은 topic 수 3개로 설정 (acc 재기 위함)\n",
    "- label purity는 topic 수 20개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de396b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import chi2_contingency\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from coherence import get_topic_coherence\n",
    "\n",
    "# 헝가리안 매칭을 위한 함수 정의\n",
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc\n",
    "\n",
    "def _hungarian_match(predicted, target, num_samples, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=np.uint64)\n",
    "    for i in range(num_samples):\n",
    "        matrix[predicted[i], target[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(matrix.max() - matrix)\n",
    "    return list(zip(row_ind, col_ind))\n",
    "\n",
    "# 테스트 데이터 로더 및 모델 준비\n",
    "test_loader = DataLoader(testfinetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "\n",
    "# 테스트 데이터셋에 대한 라벨 할당 및 정확도 계산\n",
    "topic_labels = [[] for _ in range(model.N_topic)]\n",
    "for batch in test_loader:\n",
    "    _, org_input, _, _, _, platform_labels, actual_labels = batch\n",
    "    org_input = org_input.to(device)\n",
    "    with torch.no_grad():\n",
    "        org_topic_logit = model.decode(org_input)[1]\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "    dominant_topics = torch.argmax(org_topic, dim=1).cpu().numpy()\n",
    "    for topic_idx, label in zip(dominant_topics, actual_labels):\n",
    "        topic_labels[topic_idx].append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "512b40d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sport', 'politics', 'tech', 'sport', 'tech', 'politics', 'tech', 'sport', 'tech', 'tech', 'politics', 'sport', 'sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'sport', 'tech', 'politics', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech')\n",
      "('sport', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'politics', 'sport', 'tech', 'sport', 'tech', 'politics', 'tech', 'tech', 'politics')\n",
      "('sport', 'politics', 'politics', 'sport', 'politics', 'sport', 'sport', 'sport', 'tech', 'politics', 'tech', 'tech', 'sport', 'sport', 'politics', 'politics', 'tech', 'sport', 'sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'politics', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech', 'sport')\n",
      "('tech', 'sport', 'politics', 'politics', 'sport', 'politics', 'sport', 'sport', 'tech', 'tech', 'politics', 'sport', 'tech', 'politics', 'politics', 'politics', 'tech', 'politics', 'tech', 'sport', 'sport', 'tech', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'politics', 'tech', 'politics')\n",
      "('tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'politics', 'politics', 'politics', 'tech', 'tech', 'politics', 'tech', 'tech', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech', 'politics', 'sport', 'tech', 'tech', 'sport', 'sport', 'tech', 'tech', 'sport')\n",
      "('politics', 'politics', 'politics', 'politics', 'sport', 'politics', 'sport', 'politics', 'tech', 'politics', 'sport', 'tech', 'politics', 'tech', 'tech', 'politics', 'politics', 'tech', 'sport', 'politics', 'politics', 'sport', 'tech', 'tech', 'sport', 'sport', 'tech', 'tech', 'tech', 'tech', 'politics', 'sport')\n",
      "('tech', 'politics', 'sport', 'politics', 'politics', 'politics', 'politics', 'sport', 'politics', 'sport', 'politics', 'tech', 'sport', 'tech', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'politics', 'sport', 'politics', 'politics', 'politics', 'politics', 'sport', 'politics', 'tech', 'tech', 'tech', 'tech')\n",
      "('politics', 'sport', 'sport', 'politics', 'sport', 'tech', 'sport', 'sport', 'politics', 'tech', 'sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'politics', 'tech', 'tech', 'politics', 'sport', 'politics', 'tech', 'sport', 'politics', 'politics', 'sport', 'tech', 'sport', 'sport', 'politics', 'tech')\n",
      "('sport', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'tech', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'tech', 'politics', 'tech', 'sport', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'sport', 'politics', 'politics', 'politics', 'politics', 'politics')\n",
      "('politics', 'sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'tech', 'politics', 'politics', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'tech', 'politics', 'sport', 'sport', 'politics', 'tech', 'politics', 'tech', 'sport', 'politics', 'tech', 'politics', 'sport', 'politics', 'tech', 'tech')\n",
      "('tech', 'tech', 'sport', 'sport', 'tech', 'tech', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'politics', 'tech', 'tech', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'politics', 'sport', 'politics', 'politics', 'tech', 'tech', 'tech', 'tech', 'sport', 'tech', 'tech')\n",
      "('tech', 'tech', 'tech', 'sport', 'politics', 'tech', 'tech', 'politics', 'sport', 'politics', 'sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'politics', 'politics', 'tech', 'politics', 'tech', 'sport', 'politics', 'politics')\n",
      "('sport', 'sport', 'politics', 'politics', 'politics', 'sport', 'politics', 'sport', 'politics', 'sport', 'sport', 'sport', 'tech', 'tech', 'sport', 'sport', 'politics', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'politics', 'tech', 'sport', 'sport', 'tech', 'tech', 'politics', 'tech')\n",
      "('tech', 'politics', 'tech', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'tech', 'sport', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'sport', 'politics', 'politics', 'politics', 'politics', 'tech', 'politics', 'tech', 'sport', 'politics', 'sport', 'politics', 'tech', 'tech')\n",
      "('politics', 'tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'tech', 'tech', 'politics', 'tech', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'sport', 'tech', 'politics', 'sport', 'tech', 'tech', 'sport', 'tech', 'tech', 'sport')\n",
      "('tech', 'politics', 'tech', 'sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'politics', 'sport', 'tech', 'tech', 'politics', 'politics', 'sport', 'politics', 'tech', 'tech', 'tech', 'sport', 'politics', 'tech', 'sport', 'sport', 'tech', 'politics', 'tech', 'tech', 'politics', 'sport', 'sport')\n",
      "('tech', 'sport', 'tech', 'politics', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'politics', 'tech', 'sport', 'politics', 'tech', 'tech', 'tech', 'tech', 'politics', 'politics', 'politics', 'tech', 'tech', 'sport', 'tech', 'tech', 'sport', 'tech', 'politics', 'tech', 'sport')\n",
      "('tech', 'tech', 'tech', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'politics')\n",
      "('sport', 'tech', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'politics', 'politics', 'sport', 'sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'tech', 'politics', 'politics', 'sport')\n",
      "('sport', 'sport', 'politics', 'politics', 'tech', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'sport', 'sport', 'sport', 'tech', 'politics', 'politics', 'politics', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech', 'politics', 'tech', 'sport')\n",
      "('tech', 'sport', 'tech', 'politics', 'politics', 'sport', 'tech', 'sport', 'sport', 'tech', 'politics', 'sport', 'politics', 'politics', 'sport', 'sport', 'politics', 'tech', 'tech', 'politics', 'sport', 'politics', 'politics', 'politics', 'sport', 'tech', 'tech', 'sport', 'politics', 'sport', 'tech', 'sport')\n",
      "('sport', 'tech', 'politics', 'tech', 'sport', 'politics', 'tech', 'politics', 'sport', 'sport', 'politics', 'tech', 'politics', 'tech', 'sport', 'politics', 'sport', 'politics', 'politics', 'politics', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'politics', 'sport', 'sport', 'sport', 'politics', 'tech')\n",
      "('sport', 'sport', 'tech', 'tech', 'tech', 'tech', 'politics', 'politics', 'tech')\n",
      "Test Accuracy: 0.3520336605890603\n"
     ]
    }
   ],
   "source": [
    "# 라벨('politics', 'sport', 'tech')\n",
    "\n",
    "topic_distributions = []\n",
    "for batch in testloader:\n",
    "    _, org_input, _, _, _, platform_labels, actual_labels = batch\n",
    "    org_input = org_input.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, org_topic_logit = model.decode(org_input)\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "    topic_distributions.append(org_topic.cpu().numpy())\n",
    "\n",
    "topic_dist_test = np.concatenate(topic_distributions, axis=0)\n",
    "test_target = np.array(test_total_label_list)  \n",
    "\n",
    "test_target_adjusted = test_target[:len(topic_dist_test)]\n",
    "\n",
    "label_to_int = {label: idx for idx, label in enumerate(sorted(set(test_total_label_list)))}\n",
    "int_test_target = np.array([label_to_int[label] for label in test_total_label_list])\n",
    "\n",
    "int_test_target_adjusted = int_test_target[:len(topic_dist_test)]\n",
    "\n",
    "# 테스트 데이터셋에 대한 정확도 계산\n",
    "accuracy_test = measure_hungarian_score(topic_dist_test, int_test_target_adjusted)\n",
    "print(\"Test Accuracy:\", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99bb1c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3380084151472651\n"
     ]
    }
   ],
   "source": [
    "# 플랫폼 \n",
    "topic_distributions = []\n",
    "for batch in testloader:\n",
    "    _, org_input, _, _, _, platform_labels, actual_labels = batch\n",
    "    org_input = org_input.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, org_topic_logit = model.decode(org_input)\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "    topic_distributions.append(org_topic.cpu().numpy())\n",
    "\n",
    "# 배열 형태로 변환\n",
    "topic_dist_test = np.concatenate(topic_distributions, axis=0)\n",
    "test_target = np.array(test_total_platform_list)  # 실제 라벨을 배열 형태로 변환\n",
    "\n",
    "test_target_adjusted = test_target[:len(topic_dist_test)]\n",
    "label_to_int = {label: idx for idx, label in enumerate(sorted(set(test_total_platform_list)))}\n",
    "int_test_target = np.array([label_to_int[label] for label in test_total_platform_list])\n",
    "int_test_target_adjusted = int_test_target[:len(topic_dist_test)]\n",
    "\n",
    "# 테스트 데이터셋에 대한 정확도 계산\n",
    "accuracy_test = measure_hungarian_score(topic_dist_test, int_test_target_adjusted)\n",
    "print(\"Test Accuracy:\", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72af2a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tech', 'tech', 'politics', 'tech', 'tech', 'sport', 'sport', 'politics', 'politics', 'tech', 'sport', 'tech', 'tech', 'sport', 'sport', 'sport', 'politics', 'tech', 'tech', 'sport', 'sport', 'sport', 'sport', 'tech', 'politics', 'sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'sport')\n",
      "('politics', 'tech', 'tech', 'tech', 'politics', 'tech', 'politics', 'tech', 'sport', 'tech', 'tech', 'politics', 'tech', 'tech', 'politics', 'politics', 'politics', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics', 'tech', 'tech', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech')\n",
      "('sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'tech', 'politics', 'tech', 'sport', 'sport', 'tech', 'politics', 'tech', 'politics', 'sport', 'sport', 'politics', 'sport', 'sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'tech', 'politics', 'politics', 'tech', 'politics')\n",
      "('sport', 'tech', 'politics', 'tech', 'tech', 'tech', 'politics', 'politics', 'tech', 'politics', 'sport', 'politics', 'politics', 'politics', 'tech', 'sport', 'sport', 'tech', 'sport', 'politics', 'sport', 'politics', 'sport', 'politics', 'sport', 'politics', 'tech', 'sport', 'politics', 'sport', 'sport', 'politics')\n",
      "('sport', 'politics', 'politics', 'tech', 'tech', 'politics', 'sport', 'sport', 'tech', 'sport', 'sport', 'politics', 'politics', 'sport', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'tech', 'sport', 'politics', 'sport', 'politics', 'politics', 'politics', 'politics', 'tech', 'tech', 'politics', 'tech')\n",
      "('tech', 'sport', 'sport', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'politics', 'politics', 'tech', 'tech', 'politics', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'tech', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport', 'sport')\n",
      "('politics', 'politics', 'tech', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'sport', 'sport', 'politics', 'politics', 'politics', 'sport', 'politics', 'sport', 'tech', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'tech')\n",
      "('sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'politics', 'politics', 'politics', 'politics', 'politics', 'sport', 'tech', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'sport', 'tech', 'politics', 'tech', 'tech', 'tech', 'sport', 'tech', 'tech', 'politics', 'sport', 'politics', 'politics')\n",
      "('sport', 'politics', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'tech', 'politics', 'tech', 'tech', 'politics', 'tech', 'politics', 'politics', 'sport', 'tech', 'sport', 'sport', 'sport', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'tech')\n",
      "('politics', 'tech', 'politics', 'sport', 'politics', 'sport', 'politics', 'sport', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics', 'sport', 'politics', 'tech', 'sport', 'sport', 'politics', 'tech', 'politics', 'tech', 'sport', 'tech', 'sport', 'tech', 'sport', 'politics', 'tech')\n",
      "('sport', 'sport', 'politics', 'tech', 'sport', 'tech', 'politics', 'politics', 'tech', 'sport', 'tech', 'politics', 'politics', 'sport', 'politics', 'tech', 'tech', 'sport', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'tech', 'politics', 'sport', 'tech', 'sport', 'politics')\n",
      "('politics', 'tech', 'sport', 'politics', 'tech', 'sport', 'tech', 'politics', 'sport', 'sport', 'tech', 'tech', 'tech', 'tech', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech', 'tech', 'sport', 'politics', 'tech', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'sport', 'tech')\n",
      "('politics', 'politics', 'politics', 'politics', 'tech', 'sport', 'tech', 'sport', 'sport', 'politics', 'tech', 'politics', 'sport', 'politics', 'tech', 'sport', 'tech', 'tech', 'politics', 'tech', 'tech', 'politics', 'tech', 'sport', 'sport', 'politics', 'politics', 'sport', 'tech', 'politics', 'politics', 'tech')\n",
      "('politics', 'politics', 'tech', 'politics', 'tech', 'sport', 'politics', 'tech', 'sport', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'tech', 'sport', 'politics', 'tech', 'politics', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics')\n",
      "('politics', 'tech', 'politics', 'tech', 'tech', 'politics', 'tech', 'politics', 'politics', 'tech', 'sport', 'politics', 'tech', 'tech', 'sport', 'politics', 'sport', 'politics', 'sport', 'tech', 'politics', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'sport', 'tech', 'tech', 'sport', 'politics')\n",
      "('politics', 'sport', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'tech', 'tech', 'politics', 'tech', 'tech', 'tech', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'tech', 'politics', 'tech', 'tech', 'tech', 'tech', 'politics', 'tech', 'tech', 'politics', 'sport')\n",
      "('sport', 'tech', 'tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'sport', 'politics', 'sport', 'politics', 'politics', 'politics', 'sport', 'sport', 'politics', 'tech', 'tech', 'sport', 'sport', 'politics', 'sport', 'politics', 'tech', 'politics', 'sport', 'politics', 'sport', 'politics', 'tech', 'politics')\n",
      "('politics', 'tech', 'tech', 'sport', 'tech', 'tech', 'tech', 'politics', 'sport', 'tech', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech', 'sport', 'politics', 'tech', 'politics', 'sport', 'sport', 'politics', 'politics', 'politics', 'politics')\n",
      "('politics', 'sport', 'politics', 'politics', 'sport', 'sport', 'tech', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'sport', 'tech', 'politics', 'tech', 'sport', 'politics', 'tech', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'politics', 'tech')\n",
      "('sport', 'politics', 'politics', 'politics', 'politics', 'tech', 'politics', 'politics', 'tech', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'tech', 'tech', 'politics', 'tech', 'sport', 'sport', 'politics', 'politics', 'sport', 'sport', 'tech', 'tech', 'sport', 'politics', 'politics', 'sport')\n",
      "('sport', 'tech', 'sport', 'politics', 'tech', 'sport', 'tech', 'tech', 'politics', 'politics', 'tech', 'tech', 'politics', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics', 'sport', 'politics', 'tech', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics', 'tech', 'tech')\n",
      "('sport', 'sport', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'politics', 'tech', 'tech', 'sport', 'tech', 'sport', 'sport', 'tech', 'sport', 'tech', 'politics', 'sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'tech', 'sport', 'tech', 'tech', 'tech', 'politics', 'sport')\n",
      "('sport', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'sport', 'politics', 'sport', 'sport', 'sport', 'politics', 'tech', 'politics', 'sport', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'politics', 'tech', 'sport', 'tech', 'tech', 'tech', 'tech')\n",
      "('politics', 'politics', 'politics', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'tech', 'tech', 'sport', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'tech', 'politics', 'tech', 'tech', 'sport', 'politics', 'sport', 'sport', 'politics', 'sport')\n",
      "('politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'sport', 'tech', 'tech', 'politics', 'tech', 'tech', 'politics', 'sport', 'tech', 'politics', 'tech', 'sport', 'politics', 'tech', 'sport', 'tech', 'sport', 'sport', 'politics', 'politics', 'sport', 'sport', 'politics', 'tech', 'tech', 'politics')\n",
      "('politics', 'sport', 'politics', 'tech', 'tech', 'sport', 'sport', 'tech', 'politics', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'politics', 'sport', 'politics', 'tech', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'tech')\n",
      "('politics', 'tech', 'tech', 'tech', 'tech', 'tech', 'sport', 'politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'tech', 'sport', 'tech', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics')\n",
      "('sport', 'politics', 'politics', 'sport', 'tech', 'sport', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics', 'tech', 'politics', 'politics', 'politics', 'tech', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'tech', 'politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'tech')\n",
      "('politics', 'politics', 'politics', 'politics', 'politics', 'tech', 'tech', 'politics', 'sport', 'tech', 'politics', 'politics', 'politics', 'tech', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'politics', 'sport', 'tech', 'sport', 'sport', 'sport', 'politics', 'sport', 'tech', 'politics', 'tech', 'sport')\n",
      "('politics', 'politics', 'tech', 'sport', 'sport', 'tech', 'politics', 'sport', 'politics', 'sport', 'sport', 'sport', 'politics', 'tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'sport', 'sport', 'politics', 'tech', 'tech', 'politics', 'tech', 'tech', 'tech', 'tech', 'sport', 'tech', 'sport')\n",
      "('sport', 'tech', 'tech', 'tech', 'politics', 'sport', 'tech', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'sport', 'tech', 'politics', 'politics', 'politics', 'politics', 'tech', 'politics', 'sport', 'politics', 'politics', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'tech', 'tech')\n",
      "('politics', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'sport', 'politics', 'tech', 'sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech', 'tech', 'tech', 'tech', 'politics', 'politics', 'politics', 'sport', 'tech', 'sport', 'tech', 'tech')\n",
      "('politics', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'politics', 'politics', 'tech', 'politics', 'tech', 'politics', 'sport', 'politics', 'sport', 'tech', 'politics', 'politics', 'tech', 'politics', 'sport', 'tech', 'politics', 'tech', 'tech', 'sport')\n",
      "('politics', 'politics', 'tech', 'sport', 'tech', 'tech', 'politics', 'politics', 'tech', 'sport', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'sport', 'politics', 'sport', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'tech', 'politics', 'politics', 'sport', 'sport', 'tech', 'sport')\n",
      "('sport', 'tech', 'tech', 'politics', 'tech', 'tech', 'sport', 'tech', 'sport', 'sport', 'politics', 'sport', 'politics', 'sport', 'tech', 'tech', 'sport', 'politics', 'sport', 'politics', 'politics', 'sport', 'sport', 'sport', 'politics', 'tech', 'tech', 'sport', 'politics', 'tech', 'politics', 'sport')\n",
      "('politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'sport', 'politics', 'tech', 'sport', 'tech', 'politics', 'politics', 'sport', 'politics', 'sport', 'tech', 'tech', 'tech', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics')\n",
      "('tech', 'tech', 'tech', 'politics', 'politics', 'politics', 'sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'politics', 'tech', 'sport', 'sport', 'politics', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'tech')\n",
      "('sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'sport', 'sport', 'tech', 'sport', 'tech', 'tech', 'politics', 'politics', 'sport', 'politics')\n",
      "('sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech', 'politics', 'tech', 'tech', 'tech', 'politics', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'sport', 'politics')\n",
      "('tech', 'politics', 'sport', 'politics', 'politics', 'sport', 'politics', 'politics', 'sport', 'politics', 'politics', 'sport', 'tech', 'sport', 'tech', 'politics', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'tech', 'sport', 'tech', 'tech', 'sport', 'politics', 'sport', 'politics', 'sport', 'sport')\n",
      "('sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'sport', 'politics', 'tech', 'sport', 'sport', 'politics', 'politics', 'politics', 'politics', 'sport', 'tech', 'politics', 'politics', 'sport', 'tech', 'politics', 'tech', 'sport', 'sport')\n",
      "('politics', 'politics', 'sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'tech', 'sport', 'sport', 'tech', 'politics', 'sport', 'politics', 'sport', 'sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics', 'tech', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport')\n",
      "('sport', 'tech', 'politics', 'politics', 'tech', 'politics', 'sport', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'tech', 'tech', 'sport', 'politics', 'politics', 'tech', 'tech', 'sport', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'sport', 'politics')\n",
      "('politics', 'tech', 'politics', 'tech', 'tech', 'tech', 'tech', 'sport', 'sport', 'tech', 'sport', 'tech', 'tech', 'tech', 'sport', 'sport', 'sport', 'tech', 'tech', 'tech', 'tech', 'politics', 'sport', 'tech', 'sport', 'politics', 'sport', 'sport', 'politics', 'politics', 'politics', 'politics')\n",
      "('politics', 'politics', 'politics', 'politics', 'tech', 'politics', 'tech', 'tech', 'sport', 'politics', 'tech', 'tech', 'politics', 'tech', 'politics', 'politics', 'politics', 'politics', 'tech', 'sport', 'politics', 'politics', 'politics', 'sport', 'politics', 'tech', 'politics', 'tech', 'sport', 'tech', 'sport', 'sport')\n",
      "('politics', 'sport', 'politics', 'sport', 'tech', 'sport', 'politics', 'tech', 'tech', 'politics', 'politics', 'tech', 'politics', 'sport', 'sport', 'politics', 'tech', 'politics', 'politics', 'tech', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics', 'politics', 'politics', 'tech', 'sport', 'sport', 'tech')\n",
      "('politics', 'politics', 'sport', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'sport', 'tech', 'sport', 'tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'tech', 'tech', 'sport', 'tech', 'sport', 'sport', 'politics', 'politics')\n",
      "('sport', 'tech', 'tech', 'politics', 'tech', 'sport', 'tech', 'tech', 'sport', 'sport', 'politics', 'sport', 'politics', 'tech', 'politics', 'sport', 'politics', 'sport', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'politics', 'tech')\n",
      "('politics', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport', 'politics', 'tech', 'tech', 'politics', 'politics', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'politics', 'tech', 'politics', 'tech', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'tech', 'sport', 'politics', 'tech')\n",
      "('tech', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'sport', 'politics', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'sport', 'tech', 'sport', 'politics', 'politics', 'tech', 'tech')\n",
      "('politics', 'politics', 'sport', 'sport', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics', 'politics', 'tech', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech', 'sport', 'sport', 'sport', 'sport', 'tech', 'politics', 'sport', 'politics', 'tech', 'sport', 'sport')\n",
      "('politics', 'sport', 'sport', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'tech', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'tech', 'politics', 'sport', 'tech', 'sport', 'politics', 'tech', 'tech', 'politics', 'sport', 'sport', 'tech', 'politics', 'tech', 'politics')\n",
      "('tech', 'tech', 'tech', 'sport', 'tech', 'tech', 'politics', 'politics', 'tech', 'tech', 'sport', 'politics', 'tech', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'tech', 'politics', 'sport', 'tech', 'tech', 'sport', 'sport', 'sport', 'politics', 'tech', 'tech', 'sport', 'politics')\n",
      "('sport', 'tech', 'tech', 'tech', 'politics', 'politics', 'tech', 'sport', 'tech', 'sport', 'sport', 'sport', 'tech', 'sport', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'sport', 'tech', 'sport', 'tech', 'politics')\n",
      "('tech', 'tech', 'politics', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'politics', 'tech', 'sport', 'politics', 'politics', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'sport', 'tech', 'politics', 'politics', 'sport', 'sport', 'politics', 'politics', 'sport', 'sport', 'politics', 'tech')\n",
      "('sport', 'politics', 'tech', 'sport', 'politics', 'sport', 'politics', 'sport', 'politics', 'tech', 'politics', 'sport', 'sport', 'tech', 'tech', 'politics', 'sport', 'politics', 'tech', 'tech', 'politics', 'politics', 'tech', 'politics', 'tech', 'politics', 'tech', 'tech', 'tech', 'sport', 'tech', 'politics')\n",
      "('tech', 'politics', 'sport', 'politics', 'politics', 'sport', 'sport', 'politics', 'politics', 'politics', 'tech', 'politics', 'tech', 'tech', 'sport', 'tech', 'sport', 'tech', 'sport', 'tech', 'tech', 'politics', 'sport', 'sport', 'sport', 'tech', 'politics', 'tech', 'sport', 'politics', 'politics', 'politics')\n",
      "('tech', 'tech', 'tech', 'politics', 'tech', 'politics', 'tech', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'tech', 'tech', 'tech', 'politics', 'politics', 'sport', 'tech', 'tech', 'politics')\n",
      "('sport', 'sport', 'sport', 'politics', 'politics', 'tech', 'tech', 'sport', 'politics', 'tech', 'tech', 'tech', 'tech', 'politics', 'politics', 'politics', 'sport', 'tech', 'politics', 'tech', 'tech', 'sport', 'politics', 'tech', 'sport', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'tech')\n",
      "('tech', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'politics', 'sport', 'politics', 'politics', 'sport', 'sport', 'tech', 'politics', 'tech', 'politics', 'tech', 'politics', 'tech', 'politics', 'tech', 'tech', 'sport', 'politics', 'sport', 'politics', 'politics', 'sport', 'politics', 'tech', 'tech')\n",
      "('sport', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'sport', 'tech', 'tech', 'sport', 'sport', 'sport', 'sport', 'politics', 'tech', 'sport', 'politics', 'tech', 'politics', 'tech', 'sport', 'tech', 'politics')\n",
      "('politics', 'tech', 'tech', 'sport', 'politics', 'politics', 'tech', 'politics', 'tech', 'politics', 'sport', 'sport', 'politics', 'politics', 'tech', 'sport', 'politics', 'politics', 'politics', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'politics', 'politics')\n",
      "('politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'politics', 'sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'tech', 'politics', 'tech', 'tech', 'tech', 'politics', 'tech', 'sport', 'sport', 'sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'tech', 'sport', 'politics')\n",
      "('tech', 'tech', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'sport', 'politics', 'politics', 'politics', 'sport', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'tech', 'sport', 'tech', 'tech', 'politics')\n",
      "('tech', 'sport', 'sport', 'politics', 'sport', 'politics', 'sport', 'tech', 'sport', 'politics', 'sport', 'sport', 'tech', 'politics', 'sport', 'sport', 'sport', 'tech', 'tech', 'politics', 'tech', 'tech', 'politics', 'politics', 'sport', 'tech', 'tech', 'sport', 'sport', 'sport', 'tech', 'tech')\n",
      "('politics', 'politics', 'tech', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'tech', 'sport', 'tech', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'tech', 'sport', 'sport', 'sport', 'tech', 'tech', 'tech', 'sport')\n",
      "('sport', 'tech', 'tech', 'politics', 'politics', 'tech', 'politics', 'sport', 'sport', 'sport', 'tech', 'sport', 'politics', 'tech', 'politics', 'politics', 'politics', 'politics', 'sport', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'politics', 'sport', 'politics', 'politics')\n",
      "('politics', 'tech', 'tech', 'sport', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'tech', 'politics', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'sport', 'sport', 'politics', 'tech', 'tech', 'politics', 'politics', 'sport')\n",
      "('politics', 'sport', 'sport', 'sport', 'politics', 'sport', 'politics', 'tech', 'politics', 'politics', 'tech', 'politics', 'politics', 'tech', 'politics', 'politics', 'sport', 'politics', 'politics', 'politics', 'sport', 'politics', 'politics', 'politics', 'politics', 'sport', 'tech', 'politics', 'politics', 'tech', 'sport', 'tech')\n",
      "('politics', 'tech', 'politics', 'tech', 'politics', 'tech', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'politics', 'tech', 'politics', 'tech', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'tech', 'tech', 'tech', 'sport')\n",
      "('tech', 'sport', 'tech', 'tech', 'politics', 'politics', 'politics', 'politics', 'sport', 'politics', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'tech', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'tech', 'tech', 'politics', 'politics', 'politics', 'politics', 'tech', 'tech', 'sport')\n",
      "('tech', 'politics', 'sport', 'tech', 'tech', 'tech', 'tech', 'sport', 'politics', 'politics', 'sport', 'sport', 'politics', 'politics', 'politics', 'tech', 'tech', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech')\n",
      "('sport', 'tech', 'tech', 'politics', 'sport', 'tech', 'politics', 'politics', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'politics', 'tech', 'sport', 'politics', 'politics', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'tech')\n",
      "('politics', 'sport', 'sport', 'tech', 'tech', 'politics', 'tech', 'sport', 'sport', 'tech', 'tech', 'tech', 'politics', 'tech', 'sport', 'sport', 'sport', 'tech', 'sport', 'sport', 'sport', 'sport', 'sport', 'politics', 'politics', 'politics', 'politics', 'tech', 'sport', 'sport', 'sport', 'politics')\n",
      "('politics', 'sport', 'tech', 'politics', 'sport', 'tech', 'sport', 'sport', 'tech', 'politics', 'politics', 'politics', 'politics', 'politics', 'tech', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'politics', 'tech', 'politics', 'tech', 'tech', 'sport', 'politics', 'politics', 'tech', 'tech', 'tech')\n",
      "('politics', 'sport', 'sport', 'politics', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'politics', 'tech', 'politics', 'tech', 'politics', 'sport', 'sport', 'politics', 'sport', 'sport', 'tech', 'tech', 'tech', 'politics', 'politics', 'politics', 'sport', 'tech', 'tech', 'politics', 'sport', 'tech')\n",
      "('politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'tech', 'politics', 'tech', 'tech', 'sport', 'sport', 'tech', 'tech', 'sport', 'sport', 'sport', 'tech', 'tech', 'politics', 'sport', 'tech', 'politics', 'sport', 'politics', 'tech', 'sport', 'tech', 'tech', 'tech', 'sport', 'tech')\n",
      "('sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'sport', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'tech', 'sport', 'sport', 'sport', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'politics', 'sport', 'sport', 'tech', 'sport', 'politics', 'sport', 'tech', 'politics')\n",
      "('tech', 'politics', 'sport', 'politics', 'tech', 'politics', 'sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'tech', 'tech', 'tech', 'politics', 'tech', 'tech', 'tech', 'tech', 'politics', 'sport', 'sport', 'politics', 'sport', 'tech', 'tech', 'sport', 'sport', 'politics', 'tech', 'sport')\n",
      "('tech', 'sport', 'sport', 'tech', 'tech', 'sport', 'tech', 'sport', 'sport', 'sport', 'politics', 'sport', 'tech', 'politics', 'tech', 'politics', 'tech', 'politics', 'politics', 'tech', 'sport', 'politics', 'sport', 'sport', 'tech', 'sport', 'politics', 'politics', 'sport', 'sport', 'sport', 'tech')\n",
      "('tech', 'sport', 'politics', 'sport', 'tech', 'tech', 'politics', 'politics', 'tech', 'sport', 'tech', 'tech', 'sport', 'sport', 'sport', 'politics', 'tech', 'tech', 'politics', 'tech', 'sport', 'sport', 'sport')\n",
      "각 토픽의 라벨 퓨리티: [0.4246935201401051, 0.4273743016759777, 0.41379310344827586]\n",
      "평균 라벨 퓨리티: 0.4219536417547862\n"
     ]
    }
   ],
   "source": [
    "# 라벨 퓨리티 계산\n",
    "def calculate_label_purity(topic_labels):\n",
    "    purity_scores = []\n",
    "    for labels in topic_labels:\n",
    "        if not labels:\n",
    "            continue\n",
    "        most_common_label = max(set(labels), key=labels.count)\n",
    "        purity = labels.count(most_common_label) / len(labels)\n",
    "        purity_scores.append(purity)\n",
    "    average_purity = sum(purity_scores) / len(purity_scores) if purity_scores else 0\n",
    "    return purity_scores, average_purity\n",
    "\n",
    "# 토픽 별 라벨 할당\n",
    "topic_labels = [[] for _ in range(model.N_topic)] \n",
    "\n",
    "for batch in trainloader:\n",
    "    _, org_input, _, _, _, platform_labels, actual_labels = batch\n",
    "    print(actual_labels)\n",
    "    org_input = org_input.to(device)\n",
    "    with torch.no_grad():  \n",
    "        org_topic_logit = model.decode(org_input)[1] \n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)  \n",
    "\n",
    "    dominant_topics = torch.argmax(org_topic, dim=1).cpu().numpy()  # 가장 높은 확률의 토픽 인덱스 추출\n",
    "\n",
    "    for topic_idx, label in zip(dominant_topics, actual_labels):\n",
    "        topic_labels[topic_idx].append(label)\n",
    "\n",
    "\n",
    "purity_scores, average_purity = calculate_label_purity(topic_labels)\n",
    "\n",
    "print(\"각 토픽의 라벨 퓨리티:\", purity_scores)\n",
    "print(\"평균 라벨 퓨리티:\", average_purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33093b3",
   "metadata": {},
   "source": [
    "# MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b36e7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information (NMI): 0.03602773971758664\n",
      "H(Y): 2.8874862389659492\n",
      "H(Y|X): 2.815682920913974\n",
      "Mutual Information (MI): 0.07180331805197504\n",
      "Original Mutual Information Score: 0.0718033180846549\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 기존 코드의 나머지 부분...\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터 로더 설정\n",
    "testloader = DataLoader(testfinetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    \n",
    "# 토픽 라벨과 플랫폼 라벨 추출\n",
    "ourmodel_test_topic_labels = []\n",
    "test_platform_labels = []\n",
    "\n",
    "for batch in testloader:\n",
    "#     _, org_embedding, _, org_bow, _, platform_labels = batch\n",
    "    _, org_embedding, _, org_bow, _, platform_labels, actual_labels = batch\n",
    "    org_embedding = org_embedding.to(gpu_ids[0])\n",
    "    _, topic_logit = model.decode(org_embedding)\n",
    "    topic_label = torch.argmax(F.softmax(topic_logit, dim=1), dim=1)\n",
    "    ourmodel_test_topic_labels.extend(topic_label.cpu().numpy())\n",
    "    test_platform_labels.extend(platform_labels)\n",
    "\n",
    "# 플랫폼별 토픽 분포 계산\n",
    "topic_dist_df_test = pd.crosstab(pd.Series(ourmodel_test_topic_labels, name='Topic'),\n",
    "                            pd.Series(test_platform_labels, name='Platform'), normalize='index')\n",
    "\n",
    "# 플랫폼별 및 전체에 대한 토픽 분포를 계산\n",
    "platform_counts = pd.Series(test_platform_labels).value_counts()\n",
    "platform_probabilities = platform_counts / platform_counts.sum()\n",
    "\n",
    "# 전체 데이터셋에 대한 토픽 분포의 엔트로피 계산 (H(Y))\n",
    "topic_probabilities = pd.Series(ourmodel_test_topic_labels).value_counts(normalize=True)\n",
    "H_Y = -np.sum(topic_probabilities * np.log(topic_probabilities + 1e-10))\n",
    "\n",
    "# 각 플랫폼별 조건부 엔트로피 계산 및 H(Y|X) 계산\n",
    "H_Y_given_X_total = 0\n",
    "for platform in platform_probabilities.index:\n",
    "    # 해당 플랫폼에 대한 토픽 라벨 필터링\n",
    "    platform_indices = [i for i, x in enumerate(test_platform_labels) if x == platform]\n",
    "    platform_topic_labels = [ourmodel_test_topic_labels[i] for i in platform_indices]\n",
    "    platform_topic_prob = pd.Series(platform_topic_labels).value_counts(normalize=True)\n",
    "    \n",
    "    # 플랫폼별 조건부 엔트로피 계산\n",
    "    H_Y_given_X = -np.sum(platform_topic_prob * np.log(platform_topic_prob + 1e-10))\n",
    "    H_Y_given_X_total += platform_probabilities[platform] * H_Y_given_X\n",
    "\n",
    "# 상호정보량(MI) 계산\n",
    "mi = H_Y - H_Y_given_X_total\n",
    "# 플랫폼 분포의 엔트로피 H(X) 계산\n",
    "H_X = -np.sum(platform_probabilities * np.log(platform_probabilities + 1e-10))\n",
    "\n",
    "# 상호정보량(MI)을 이미 계산했다고 가정\n",
    "# MI = H_Y - H_Y_given_X_total\n",
    "\n",
    "# 정규화된 상호정보량(NMI) 계산\n",
    "NMI = 2 * mi / (H_Y + H_X)\n",
    "\n",
    "print(\"Normalized Mutual Information (NMI):\", NMI)\n",
    "\n",
    "print('H(Y):', H_Y)\n",
    "print('H(Y|X):', H_Y_given_X_total)\n",
    "print('Mutual Information (MI):', mi)\n",
    "\n",
    "mi_score = mutual_info_score(ourmodel_test_topic_labels, test_platform_labels)\n",
    "print(\"Original Mutual Information Score:\", mi_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc385eec",
   "metadata": {},
   "source": [
    "# Coherence & Topic Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4b144618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Diversity: 0.975\n"
     ]
    }
   ],
   "source": [
    "# 주제 다양성 계산 함수\n",
    "def calculate_topic_diversity(topic_words_list):\n",
    "    all_words = set()\n",
    "    for words in topic_words_list:\n",
    "        all_words.update(words)\n",
    "    unique_words_count = len(all_words)\n",
    "    total_words_count = sum(len(words) for words in topic_words_list)\n",
    "    return unique_words_count / total_words_count\n",
    "\n",
    "# 주제 다양성 계산\n",
    "topic_diversity = calculate_topic_diversity(topic_words_list)\n",
    "print(\"Topic Diversity:\",topic_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e0e23aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.6073848301599067, 'UCI': 3.7812460006494004, 'UMASS': -1.091080646445589, 'CV': 0.7076743326717905, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "topic_words_list = list(all_list.values())\n",
    "reference_corpus=[doc.split() for doc in testds.preprocess_ctm(testds.nonempty_text)]\n",
    "\n",
    "topics=topic_words_list\n",
    "texts=reference_corpus\n",
    "print(get_topic_coherence(topics, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bbf52595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bbc' 문서 수: 233\n",
      "'nyt' 문서 수: 240\n",
      "'20newsgroups' 문서 수: 240\n"
     ]
    }
   ],
   "source": [
    "reference_corpus = [doc.split() for doc in testds.preprocess_ctm(testds.nonempty_text)]\n",
    "platform_labels = testds.platform_label_list  \n",
    "\n",
    "platform_corpora = {'bbc': [], 'nyt': [], '20newsgroups': []}\n",
    "\n",
    "for doc, platform in zip(reference_corpus, platform_labels):\n",
    "    if platform in platform_corpora:\n",
    "        platform_corpora[platform].append(doc)\n",
    "\n",
    "bbc_texts = platform_corpora['bbc']\n",
    "nyt_texts = platform_corpora['nyt']\n",
    "news_texts = platform_corpora['20newsgroups']\n",
    "\n",
    "# 각 플랫폼 별 문서 수 확인\n",
    "print(f\"'bbc' 문서 수: {len(bbc_texts)}\")\n",
    "print(f\"'nyt' 문서 수: {len(nyt_texts)}\")\n",
    "print(f\"'20newsgroups' 문서 수: {len(news_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7f872183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.547141676456582, 'UCI': 2.9629632092033424, 'UMASS': -1.1028965338813588, 'CV': 0.7743565001071542, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, bbc_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "772a5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.6110145593992476, 'UCI': 2.798549250650759, 'UMASS': -0.6914497119293894, 'CV': 0.8624470661183867, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, nyt_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1c9a11cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.7354736894690324, 'UCI': 4.03086575972686, 'UMASS': -0.44793986730701374, 'CV': 0.81696670820916, 'Topic_Diversity': 0.975}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, news_texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "don",
   "language": "python",
   "name": "don"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
