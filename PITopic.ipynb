{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c355e0",
   "metadata": {},
   "source": [
    "# Setting(dataset, parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a009520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 ' + \\\n",
    "            '--dataset all --n-word 30000 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 ' + \\\n",
    "            '--n-cluster 20 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c1d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/don12/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/don12/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/don12/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from utils import AverageMeter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "import scipy.stats\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset, BertDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "\n",
    "from data import BertDataset, Stage2Dataset\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ab2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09bce59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    parser.add_argument('--n-word', default=30000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8d1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "# textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a80e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_ds = TwitterDataset()\n",
    "reddit_ds = RedditDataset()\n",
    "youtube_ds = YoutubeDataset()\n",
    "\n",
    "train_twitter_texts, test_twitter_texts, train_twitter_labels, test_twitter_labels = train_test_split(\n",
    "    twitter_ds.texts, twitter_ds.labels, train_size=0.7, random_state=42)\n",
    "\n",
    "train_reddit_texts, test_reddit_texts, train_reddit_labels, test_reddit_labels = train_test_split(\n",
    "    reddit_ds.texts, reddit_ds.labels, train_size=0.7, random_state=42)\n",
    "\n",
    "train_youtube_texts, test_youtube_texts, train_youtube_labels, test_youtube_labels = train_test_split(\n",
    "    youtube_ds.texts, youtube_ds.labels, train_size=0.7, random_state=42)\n",
    "\n",
    "# Split train data 9:1 between train and val\n",
    "train_twitter_texts, val_twitter_texts, train_twitter_labels, val_twitter_labels = train_test_split(\n",
    "    train_twitter_texts, train_twitter_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "train_reddit_texts, val_reddit_texts, train_reddit_labels, val_reddit_labels = train_test_split(\n",
    "    train_reddit_texts, train_reddit_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "train_youtube_texts, val_youtube_texts, train_youtube_labels, val_youtube_labels = train_test_split(\n",
    "    train_youtube_texts, train_youtube_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "train_total_label = train_twitter_labels + train_reddit_labels + train_youtube_labels\n",
    "train_total_text_list = train_twitter_texts + train_reddit_texts + train_youtube_texts\n",
    "\n",
    "val_total_label = val_twitter_labels + val_reddit_labels + val_youtube_labels\n",
    "val_total_text_list = val_twitter_texts + val_reddit_texts + val_youtube_texts\n",
    "\n",
    "test_total_label = test_twitter_labels + test_reddit_labels + test_youtube_labels\n",
    "test_total_text_list = test_twitter_texts + test_reddit_texts + test_youtube_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc66b351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898cf3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, bert, text_list, platform_label, N_word, vectorizer=None, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.nonempty_text = [text for text in text_list if len(text) > 0]\n",
    "        \n",
    "        # Remove new lines\n",
    "        self.nonempty_text = [re.sub(\"\\n\",\" \", sent) for sent in self.nonempty_text]\n",
    "                \n",
    "        # Remove Emails\n",
    "        self.nonempty_text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove new line characters\n",
    "        self.nonempty_text = [re.sub('\\s+', ' ', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove distracting single quotes\n",
    "        self.nonempty_text = [re.sub(\"\\'\", \"\", sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # jargons list via c-TF-IDF \n",
    "        self.jargons =  set(['coinex', 'announces', 'seos', 'chatgptpowered', 'launches', 'bigdata', 'unveils', 'openaichatgpt', 'tags', 'hn', 'stablediffusion', 'chatgptstyle', 'reportedly', 'mba', 'marketers', 'baidu', 'technews', 'fintech', 'chatgptlike', 'elonmusk', 'notion', 'goog', 'googleai', 'digitalmarketing', 'artificalintelligence', 'rt', 'googl', 'bardai', 'edtech', 'malware', 'wharton', 'agix', 'chatgptplus', 'datascience', 'deeplearning', 'msft', 'weirdness', 'tweets', 'amid', 'aitools', 'cybersecurity', 'airdrop', 'cc', 'valentines', 'startups', 'snapchat', 'generativeai', 'buzzfeed', 'fastestgrowing', 'anthropic', 'maker', 'rival', 'techcrunch', 'aiart', 'nocode', 'invests', 'cybercriminals', 'abstracts', 'nyc', 'webinar', 'retweet', 'educators', 'brilliance', 'rescue', 'daysofcode', 'gm', 'rtechnology', 'linkedin', 'licensing', 'copywriting', 'copywriters', 'contentmarketing', 'revolutionizing', 'technologynews', 'warns', 'metaverse', 'cofounder', 'trending', 'founders', 'aipowered', 'openaichat', 'releases', 'microsofts', 'chinas', 'infosec', 'launching', 'jasper', 'nfts', 'newsletter', 'chatgptgod', 'futureofwork', 'digitaltransformation', 'founder', 'feb', 'buzz', 'rn', 'ux', 'courtesy', 'nick', 'claude',\n",
    "'remindme', 'giphy', 'gif', 'deleted', 'giphydownsized', 'chadgpt', 'removed', 'patched', 'nerfed', 'yup', 'waitlist', 'refresh', 'sydney', 'mods', 'nsfw', 'characterai', 'screenshot', 'downvoted', 'youcom', 'meth', 'ascii', 'karma', 'hahaha', 'hangman', 'chatopenaicom', 'emojis', 'porn', 'redditor', 'vpn', 'upvotes', 'blah', 'upvote', 'violated', 'yep', 'joking', 'nope', 'offended', 'mod', 'bruh', 'roleplay', 'ops', 'bob', 'dans', 'redditors', 'nerf', 'firefox', 'trolling', 'sarcastic', 'huh', 'turbo', 'troll', 'patch', 'tag', 'url', 'sus', 'erotica', 'chad', 'gotcha', 'basilisk', 'login', 'lmfao', 'temperature', 'poll', 'emoji', 'rick', 'dm', 'jailbreak', 'orange', 'sub', 'quack', 'davinci', 'uh', 'flagged', 'op', 'markdown', 'flair', 'cares', 'refreshing', 'hitler', 'cookies', 'hmm', 'yikes', 'erotic', 'gti', 'paywall', 'elaborate', 'yea', 'ah', 'uncensored', 'rude', 'colour', 'bitch', 'therapy', 'neutered', 'deny', 'chats', 'jailbroken', 'cake', 'dungeon', 'dang',\n",
    "'zronx', 'tuce', 'jontron', 'levy', 'bishop', 'rook', 'thumbnail', 'quotquot', 'jon', 'linus', 'hrefaboutinvalidzcsafeza', 'beluga', 'vid', 'bhai', 'gemx', 'raid', 'ohio', 'circle', 'subscribed', 'anna', 'stare', 'canva', 'napster', 'shapiro', 'sponsor', 'broker', 'websiteapp', 'manoj', 'subscriber', 'bluewillow', 'alex', 'vids', 'legends', 'ryan', 'shes', 'hackbanzer', 'quotoquot', 'pictory', 'youtuber', 'profitable', 'pawn', 'joma', 'folders', 'lifechanging', 'thomas', 'ur', 'plz', 'mike', 'scott', 'casey', 'adrian', 'enjoyed', 'stockfish', 'invideo', 'shortlisted', 'hikaru', 'bless', 'corpsb', 'chatgbt', 'bfuture', 'curve', 'accent', 'amc', 'tutorials', 'gotham', 'mrs', 'earning', 'bra', 'elo', 'oliver', 'youtubers', 'quotcontinuequot', 'membership', 'labels', 'dagogo', 'eonr', 'hai', 'quotai', 'affiliate', 'congratulationsbryou', 'subscribers', 'thumbnails', 'azn', 'beast', 'tom', 'trader', 'garetz', 'quot', 'subbed', 'pls', 'quotchatgpt', 'gtp', 'machina', 'quoti', 'bret', 'terminator', 'watchingbrdm', 'quothow', 'nowi', 'mint'])\n",
    "\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert)\n",
    "        self.model = AutoModel.from_pretrained(bert).to(device)\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        self.N_word = N_word\n",
    "        \n",
    "        if vectorizer == None:\n",
    "            self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "            self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text))\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            \n",
    "        self.org_list = []\n",
    "        self.bow_list = []\n",
    "        self.platform_label_list = platform_label\n",
    "        \n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            org_input = self.tokenizer(sent, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "            org_input['input_ids'] = torch.squeeze(org_input['input_ids'])\n",
    "            org_input['attention_mask'] = torch.squeeze(org_input['attention_mask'])\n",
    "            self.org_list.append(org_input)\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray()\n",
    "        vectorized_input = vectorized_input.astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                             for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp    \n",
    "    \n",
    "    # mean_pooling 함수 정의\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()\n",
    "        sum_embeddings = torch.sum(model_output * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nonempty_text)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.nonempty_text[idx]\n",
    "        encoded_input = self.tokenizer(sentence, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input.to(device))\n",
    "        pooled_embedding = self.mean_pooling(model_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "        return self.org_list[idx], self.bow_list[idx], pooled_embedding, self.platform_label_list[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9c2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 19:30:03.310093: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 19:30:03.505009: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 19:30:04.424355: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2024-03-20 19:30:04.424478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:\n",
      "2024-03-20 19:30:04.424485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 37800/37800 [01:37<00:00, 388.08it/s]\n",
      "100%|██████████| 4200/4200 [00:09<00:00, 446.65it/s]\n",
      "100%|██████████| 18000/18000 [00:45<00:00, 395.32it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=train_total_text_list, platform_label = train_total_label, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "valds = BertDataset(bert=bert_name, text_list=val_total_text_list, platform_label = val_total_label, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds = BertDataset(bert=bert_name, text_list=test_total_text_list, platform_label = test_total_label, N_word=n_word, vectorizer=None, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba29252",
   "metadata": {},
   "source": [
    "#  Mean pooling progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a88ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data set\n",
    "trainds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in trainds:\n",
    "    trainds_embeddings.append(pooled_embedding.to(device))  \n",
    "train_mean_pooled_embeddings = torch.stack(trainds_embeddings).to(device) \n",
    "\n",
    "# validation data set\n",
    "valds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in valds:\n",
    "    valds_embeddings.append(pooled_embedding.to(device))\n",
    "val_mean_pooled_embeddings = torch.stack(valds_embeddings).to(device)\n",
    "\n",
    "# test data set\n",
    "testds_embeddings = []\n",
    "for _,_,pooled_embedding,_ in testds:\n",
    "    testds_embeddings.append(pooled_embedding.to(device))\n",
    "test_mean_pooled_embeddings = torch.stack(testds_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd08ab9",
   "metadata": {},
   "source": [
    "# Re_fornulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85826fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97909bc",
   "metadata": {},
   "source": [
    "# Get pos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_cosine_similarity_indices(mean_pooled_embeddings, batch_size=500):\n",
    "    n_rows = mean_pooled_embeddings.size(0)\n",
    "    max_similarity_indices = torch.zeros(n_rows, dtype=torch.int64, device=mean_pooled_embeddings.device)\n",
    "\n",
    "    for start_idx in range(0, n_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_rows)\n",
    "        batch_data = mean_pooled_embeddings[start_idx:end_idx]\n",
    "\n",
    "        batch_data_norm = torch.nn.functional.normalize(batch_data, p=2, dim=1)\n",
    "        mean_pooled_embeddings_norm = torch.nn.functional.normalize(mean_pooled_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # cosine similarty\n",
    "        batch_similarity = torch.mm(batch_data_norm, mean_pooled_embeddings_norm.transpose(0, 1))\n",
    "        batch_similarity[torch.arange(end_idx-start_idx), torch.arange(start_idx, end_idx)] = -1\n",
    "\n",
    "        max_indices = torch.argmax(batch_similarity, dim=1)\n",
    "        max_similarity_indices[start_idx:end_idx] = max_indices\n",
    "\n",
    "    return max_similarity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3016fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mean_pooled_embeddings to two-dimensional form\n",
    "train_mean_pooled_embeddings_2d = train_mean_pooled_embeddings.to(device).squeeze()\n",
    "val_mean_pooled_embeddings_2d = val_mean_pooled_embeddings.to(device).squeeze()\n",
    "test_mean_pooled_embeddings_2d = test_mean_pooled_embeddings.to(device).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc96d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity matrix\n",
    "train_similarity_matrix = compute_max_cosine_similarity_indices(train_mean_pooled_embeddings_2d)\n",
    "val_similarity_matrix = compute_max_cosine_similarity_indices(val_mean_pooled_embeddings_2d)\n",
    "test_similarity_matrix = compute_max_cosine_similarity_indices(test_mean_pooled_embeddings_2d)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12fdaa9f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContBertTopicExtractorAE(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=384, out_features=20, bias=True)\n",
       "  (adapt_bert): Linear(in_features=384, out_features=100, bias=True)\n",
       "  (activation): Softplus(beta=1, threshold=20)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (drop_theta): Dropout(p=0.2, inplace=False)\n",
       "  (decode_fc): Sequential(\n",
       "    (l_0): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       "  (f_mu): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (f_mu_batchnorm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (f_sigma): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (f_sigma_batchnorm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (beta_batchnorm): BatchNorm1d(30000, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "model.to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4196c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, similarity_matrix, N_word, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        self.N_word = N_word\n",
    "            \n",
    "      \n",
    "        self.jargons =  set(['coinex', 'announces', 'seos', 'chatgptpowered', 'launches', 'bigdata', 'unveils', 'openaichatgpt', 'tags', 'hn', 'stablediffusion', 'chatgptstyle', 'reportedly', 'mba', 'marketers', 'baidu', 'technews', 'fintech', 'chatgptlike', 'elonmusk', 'notion', 'goog', 'googleai', 'digitalmarketing', 'artificalintelligence', 'rt', 'googl', 'bardai', 'edtech', 'malware', 'wharton', 'agix', 'chatgptplus', 'datascience', 'deeplearning', 'msft', 'weirdness', 'tweets', 'amid', 'aitools', 'cybersecurity', 'airdrop', 'cc', 'valentines', 'startups', 'snapchat', 'generativeai', 'buzzfeed', 'fastestgrowing', 'anthropic', 'maker', 'rival', 'techcrunch', 'aiart', 'nocode', 'invests', 'cybercriminals', 'abstracts', 'nyc', 'webinar', 'retweet', 'educators', 'brilliance', 'rescue', 'daysofcode', 'gm', 'rtechnology', 'linkedin', 'licensing', 'copywriting', 'copywriters', 'contentmarketing', 'revolutionizing', 'technologynews', 'warns', 'metaverse', 'cofounder', 'trending', 'founders', 'aipowered', 'openaichat', 'releases', 'microsofts', 'chinas', 'infosec', 'launching', 'jasper', 'nfts', 'newsletter', 'chatgptgod', 'futureofwork', 'digitaltransformation', 'founder', 'feb', 'buzz', 'rn', 'ux', 'courtesy', 'nick', 'claude',\n",
    "'remindme', 'giphy', 'gif', 'deleted', 'giphydownsized', 'chadgpt', 'removed', 'patched', 'nerfed', 'yup', 'waitlist', 'refresh', 'sydney', 'mods', 'nsfw', 'characterai', 'screenshot', 'downvoted', 'youcom', 'meth', 'ascii', 'karma', 'hahaha', 'hangman', 'chatopenaicom', 'emojis', 'porn', 'redditor', 'vpn', 'upvotes', 'blah', 'upvote', 'violated', 'yep', 'joking', 'nope', 'offended', 'mod', 'bruh', 'roleplay', 'ops', 'bob', 'dans', 'redditors', 'nerf', 'firefox', 'trolling', 'sarcastic', 'huh', 'turbo', 'troll', 'patch', 'tag', 'url', 'sus', 'erotica', 'chad', 'gotcha', 'basilisk', 'login', 'lmfao', 'temperature', 'poll', 'emoji', 'rick', 'dm', 'jailbreak', 'orange', 'sub', 'quack', 'davinci', 'uh', 'flagged', 'op', 'markdown', 'flair', 'cares', 'refreshing', 'hitler', 'cookies', 'hmm', 'yikes', 'erotic', 'gti', 'paywall', 'elaborate', 'yea', 'ah', 'uncensored', 'rude', 'colour', 'bitch', 'therapy', 'neutered', 'deny', 'chats', 'jailbroken', 'cake', 'dungeon', 'dang',\n",
    "'zronx', 'tuce', 'jontron', 'levy', 'bishop', 'rook', 'thumbnail', 'quotquot', 'jon', 'linus', 'hrefaboutinvalidzcsafeza', 'beluga', 'vid', 'bhai', 'gemx', 'raid', 'ohio', 'circle', 'subscribed', 'anna', 'stare', 'canva', 'napster', 'shapiro', 'sponsor', 'broker', 'websiteapp', 'manoj', 'subscriber', 'bluewillow', 'alex', 'vids', 'legends', 'ryan', 'shes', 'hackbanzer', 'quotoquot', 'pictory', 'youtuber', 'profitable', 'pawn', 'joma', 'folders', 'lifechanging', 'thomas', 'ur', 'plz', 'mike', 'scott', 'casey', 'adrian', 'enjoyed', 'stockfish', 'invideo', 'shortlisted', 'hikaru', 'bless', 'corpsb', 'chatgbt', 'bfuture', 'curve', 'accent', 'amc', 'tutorials', 'gotham', 'mrs', 'earning', 'bra', 'elo', 'oliver', 'youtubers', 'quotcontinuequot', 'membership', 'labels', 'dagogo', 'eonr', 'hai', 'quotai', 'affiliate', 'congratulationsbryou', 'subscribers', 'thumbnails', 'azn', 'beast', 'tom', 'trader', 'garetz', 'quot', 'subbed', 'pls', 'quotchatgpt', 'gtp', 'machina', 'quoti', 'bret', 'terminator', 'watchingbrdm', 'quothow', 'nowi', 'mint'])\n",
    "\n",
    "\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "        self.pos_dict = similarity_matrix\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        self.platform_label_list = self.ds.platform_label_list\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "        \n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float).to(device) \n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return idx, self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx],self.platform_label_list[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3abcb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37800/37800 [01:00<00:00, 623.94it/s]\n",
      "100%|██████████| 37800/37800 [04:47<00:00, 131.62it/s]\n",
      "100%|██████████| 4200/4200 [00:06<00:00, 697.67it/s]\n",
      "100%|██████████| 4200/4200 [00:31<00:00, 132.96it/s]\n",
      "100%|██████████| 18000/18000 [00:27<00:00, 664.31it/s]\n",
      "100%|██████████| 18000/18000 [02:17<00:00, 130.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, train_similarity_matrix, n_word, lemmatize=True)\n",
    "valfinetuneds = Stage2Dataset(model.encoder, valds, val_similarity_matrix, n_word, lemmatize=True) \n",
    "testfinetuneds = Stage2Dataset(model.encoder, testds, test_similarity_matrix, n_word, lemmatize=True) \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daded5c",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a3e6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9818d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_measure_hungarian = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86eb85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d91e4",
   "metadata": {},
   "source": [
    "# Seperate Platform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df7a3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from collections import defaultdict\n",
    "\n",
    "# Implementing a custom sampler\n",
    "class PlatformSampler(Sampler):\n",
    "    def __init__(self, dataset, platform_label):\n",
    "        self.indices = [i for i, label in enumerate(dataset.platform_label_list) if label == platform_label]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e66859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_platform_dataloader(dataset, platform_label, batch_size=32, num_workers=0):\n",
    "    sampler = PlatformSampler(dataset, platform_label)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01506b05",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd17d454",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 9.21985 - dist: 0.16057 - cons: -0.07186\n",
      "Epoch-1 / recon: 8.99693 - dist: 0.14935 - cons: -0.10571\n",
      "Epoch-2 / recon: 8.86256 - dist: 0.14490 - cons: -0.12476\n",
      "Epoch-3 / recon: 8.75588 - dist: 0.14007 - cons: -0.13809\n",
      "Epoch-4 / recon: 8.66498 - dist: 0.13428 - cons: -0.14827\n",
      "Epoch-5 / recon: 8.58487 - dist: 0.12931 - cons: -0.15606\n",
      "Epoch-6 / recon: 8.51427 - dist: 0.12522 - cons: -0.16180\n",
      "Epoch-7 / recon: 8.45157 - dist: 0.12224 - cons: -0.16571\n",
      "Epoch-8 / recon: 8.39531 - dist: 0.12025 - cons: -0.16844\n",
      "Epoch-9 / recon: 8.34415 - dist: 0.11897 - cons: -0.17017\n",
      "Epoch-10 / recon: 8.29713 - dist: 0.11828 - cons: -0.17118\n",
      "Epoch-11 / recon: 8.25348 - dist: 0.11795 - cons: -0.17158\n",
      "Epoch-12 / recon: 8.21254 - dist: 0.11803 - cons: -0.17151\n",
      "Epoch-13 / recon: 8.17392 - dist: 0.11846 - cons: -0.17108\n",
      "Epoch-14 / recon: 8.13720 - dist: 0.11914 - cons: -0.17041\n",
      "Epoch-15 / recon: 8.10210 - dist: 0.11998 - cons: -0.16951\n",
      "Epoch-16 / recon: 8.06845 - dist: 0.12107 - cons: -0.16843\n",
      "Epoch-17 / recon: 8.03620 - dist: 0.12228 - cons: -0.16724\n",
      "Epoch-18 / recon: 8.00506 - dist: 0.12356 - cons: -0.16598\n",
      "Epoch-19 / recon: 7.97510 - dist: 0.12494 - cons: -0.16467\n",
      "Epoch-20 / recon: 7.94611 - dist: 0.12633 - cons: -0.16332\n",
      "Epoch-21 / recon: 7.91807 - dist: 0.12784 - cons: -0.16197\n",
      "Epoch-22 / recon: 7.89096 - dist: 0.12936 - cons: -0.16061\n",
      "Epoch-23 / recon: 7.86454 - dist: 0.13089 - cons: -0.15923\n",
      "Epoch-24 / recon: 7.83895 - dist: 0.13240 - cons: -0.15787\n",
      "Epoch-25 / recon: 7.81405 - dist: 0.13398 - cons: -0.15651\n",
      "Epoch-26 / recon: 7.78986 - dist: 0.13554 - cons: -0.15518\n",
      "Epoch-27 / recon: 7.76638 - dist: 0.13711 - cons: -0.15387\n",
      "Epoch-28 / recon: 7.74357 - dist: 0.13861 - cons: -0.15257\n",
      "Epoch-29 / recon: 7.72135 - dist: 0.14015 - cons: -0.15131\n",
      "Epoch-30 / recon: 7.69974 - dist: 0.14164 - cons: -0.15008\n",
      "Epoch-31 / recon: 7.67872 - dist: 0.14320 - cons: -0.14887\n",
      "Epoch-32 / recon: 7.65823 - dist: 0.14471 - cons: -0.14770\n",
      "Epoch-33 / recon: 7.63834 - dist: 0.14616 - cons: -0.14656\n",
      "Epoch-34 / recon: 7.61897 - dist: 0.14762 - cons: -0.14544\n",
      "Epoch-35 / recon: 7.60011 - dist: 0.14908 - cons: -0.14436\n",
      "Epoch-36 / recon: 7.58167 - dist: 0.15047 - cons: -0.14330\n",
      "Epoch-37 / recon: 7.56375 - dist: 0.15185 - cons: -0.14226\n",
      "Epoch-38 / recon: 7.54622 - dist: 0.15322 - cons: -0.14126\n",
      "Epoch-39 / recon: 7.52915 - dist: 0.15454 - cons: -0.14028\n",
      "Epoch-40 / recon: 7.51251 - dist: 0.15589 - cons: -0.13932\n",
      "Epoch-41 / recon: 7.49631 - dist: 0.15717 - cons: -0.13840\n",
      "Epoch-42 / recon: 7.48046 - dist: 0.15847 - cons: -0.13750\n",
      "Epoch-43 / recon: 7.46500 - dist: 0.15971 - cons: -0.13662\n",
      "Epoch-44 / recon: 7.44987 - dist: 0.16092 - cons: -0.13576\n",
      "Epoch-45 / recon: 7.43510 - dist: 0.16214 - cons: -0.13494\n",
      "Epoch-46 / recon: 7.42069 - dist: 0.16333 - cons: -0.13413\n",
      "Epoch-47 / recon: 7.40657 - dist: 0.16446 - cons: -0.13335\n",
      "Epoch-48 / recon: 7.39282 - dist: 0.16560 - cons: -0.13258\n",
      "Epoch-49 / recon: 7.37934 - dist: 0.16671 - cons: -0.13184\n",
      "Epoch-50 / recon: 7.36618 - dist: 0.16779 - cons: -0.13112\n",
      "Epoch-51 / recon: 7.35329 - dist: 0.16888 - cons: -0.13040\n",
      "Epoch-52 / recon: 7.34069 - dist: 0.16994 - cons: -0.12971\n",
      "Epoch-53 / recon: 7.32836 - dist: 0.17098 - cons: -0.12905\n",
      "Epoch-54 / recon: 7.31630 - dist: 0.17198 - cons: -0.12839\n",
      "Epoch-55 / recon: 7.30449 - dist: 0.17298 - cons: -0.12775\n",
      "Epoch-56 / recon: 7.29292 - dist: 0.17396 - cons: -0.12713\n",
      "Epoch-57 / recon: 7.28163 - dist: 0.17491 - cons: -0.12653\n",
      "Epoch-58 / recon: 7.27054 - dist: 0.17584 - cons: -0.12594\n",
      "Epoch-59 / recon: 7.25969 - dist: 0.17675 - cons: -0.12536\n",
      "Epoch-60 / recon: 7.24905 - dist: 0.17764 - cons: -0.12480\n",
      "Epoch-61 / recon: 7.23865 - dist: 0.17853 - cons: -0.12425\n",
      "Epoch-62 / recon: 7.22845 - dist: 0.17940 - cons: -0.12371\n",
      "Epoch-63 / recon: 7.21843 - dist: 0.18025 - cons: -0.12318\n",
      "Epoch-64 / recon: 7.20862 - dist: 0.18108 - cons: -0.12267\n",
      "Epoch-65 / recon: 7.19899 - dist: 0.18190 - cons: -0.12217\n",
      "Epoch-66 / recon: 7.18957 - dist: 0.18268 - cons: -0.12168\n",
      "Epoch-67 / recon: 7.18033 - dist: 0.18348 - cons: -0.12120\n",
      "Epoch-68 / recon: 7.17127 - dist: 0.18425 - cons: -0.12073\n",
      "Epoch-69 / recon: 7.16235 - dist: 0.18500 - cons: -0.12028\n",
      "Epoch-70 / recon: 7.15359 - dist: 0.18576 - cons: -0.11983\n",
      "Epoch-71 / recon: 7.14499 - dist: 0.18650 - cons: -0.11940\n",
      "Epoch-72 / recon: 7.13656 - dist: 0.18723 - cons: -0.11897\n",
      "Epoch-73 / recon: 7.12830 - dist: 0.18793 - cons: -0.11855\n",
      "Epoch-74 / recon: 7.12016 - dist: 0.18864 - cons: -0.11814\n",
      "Epoch-75 / recon: 7.11218 - dist: 0.18932 - cons: -0.11774\n",
      "Epoch-76 / recon: 7.10434 - dist: 0.18999 - cons: -0.11735\n",
      "Epoch-77 / recon: 7.09663 - dist: 0.19065 - cons: -0.11697\n",
      "Epoch-78 / recon: 7.08906 - dist: 0.19129 - cons: -0.11660\n",
      "Epoch-79 / recon: 7.08162 - dist: 0.19195 - cons: -0.11623\n",
      "Epoch-80 / recon: 7.07430 - dist: 0.19258 - cons: -0.11587\n",
      "Epoch-81 / recon: 7.06709 - dist: 0.19318 - cons: -0.11552\n",
      "Epoch-82 / recon: 7.06002 - dist: 0.19379 - cons: -0.11517\n",
      "Epoch-83 / recon: 7.05305 - dist: 0.19439 - cons: -0.11483\n",
      "Epoch-84 / recon: 7.04619 - dist: 0.19499 - cons: -0.11450\n",
      "Epoch-85 / recon: 7.03945 - dist: 0.19557 - cons: -0.11418\n",
      "Epoch-86 / recon: 7.03282 - dist: 0.19615 - cons: -0.11386\n",
      "Epoch-87 / recon: 7.02631 - dist: 0.19672 - cons: -0.11355\n",
      "Epoch-88 / recon: 7.01987 - dist: 0.19727 - cons: -0.11324\n",
      "Epoch-89 / recon: 7.01356 - dist: 0.19780 - cons: -0.11294\n",
      "Epoch-90 / recon: 7.00734 - dist: 0.19833 - cons: -0.11264\n",
      "Epoch-91 / recon: 7.00121 - dist: 0.19887 - cons: -0.11235\n",
      "Epoch-92 / recon: 6.99520 - dist: 0.19939 - cons: -0.11207\n",
      "Epoch-93 / recon: 6.98928 - dist: 0.19989 - cons: -0.11179\n",
      "Epoch-94 / recon: 6.98344 - dist: 0.20039 - cons: -0.11151\n",
      "Epoch-95 / recon: 6.97768 - dist: 0.20088 - cons: -0.11124\n",
      "Epoch-96 / recon: 6.97203 - dist: 0.20138 - cons: -0.11098\n",
      "Epoch-97 / recon: 6.96645 - dist: 0.20186 - cons: -0.11072\n",
      "Epoch-98 / recon: 6.96095 - dist: 0.20233 - cons: -0.11047\n",
      "Epoch-99 / recon: 6.95553 - dist: 0.20281 - cons: -0.11021\n",
      "Best Epoch: 91 with NPMI: 0.850043818488738\n",
      "------- Evaluation results -------\n",
      "topic-0 ['whore', 'hilarious', 'wwwboyfriendgptcom', 'clippygpt', 'vanoss', 'copypasta', 'chilling', 'rofl', 'tshirt', 'shatgpt']\n",
      "topic-1 ['neural', 'explainable', 'redefinition', 'calculation', 'xai', 'soso', 'operates', 'relearn', 'mathematical', 'kdnuggets']\n",
      "topic-2 ['inferred', 'burp', 'sucker', 'blaming', 'cringe', 'dumbass', 'cocaine', 'pete', 'hahahah', 'erit']\n",
      "topic-3 ['uganda', 'currency', 'dollar', 'fiat', 'lottery', 'coingecko', 'sellside', 'bitcoin', 'procrastinating', 'aitoken']\n",
      "topic-4 ['messaged', 'stepchange', 'oof', 'youfrom', 'exemplar', 'danial', 'copyrighted', 'kakkar', 'quotultimate', 'toolbarquot']\n",
      "topic-5 ['agreed', 'appoint', 'chatgptgtgoogleassistant', 'preach', 'fantastic', 'supportive', 'maximizing', 'sonu', 'apologetic', 'protip']\n",
      "topic-6 ['emojies', 'antihindu', 'toying', 'fu', 'probaron', 'amp', 'ftw', 'oof', 'quetta', 'mem']\n",
      "topic-7 ['yus', 'uuugggg', 'spaghetti', 'egypt', 'vunnai', 'elanti', 'storytho', 'pyramid', 'dong', 'ruuullleeesss']\n",
      "topic-8 ['discord', 'performed', 'moderator', 'prevent', 'friendly', 'compose', 'subreddit', 'experiment', 'bot', 'contact']\n",
      "topic-9 ['copypasta', 'portugalcykablyat', 'thesaurus', 'verbalize', 'refrence', 'proceeds', 'pivoting', 'brazil', 'contribution', 'upvoted']\n",
      "topic-10 ['tomato', 'brutal', 'tinkled', 'cherredith', 'cherry', 'pineapple', 'jelly', 'franz', 'soda', 'lmao']\n",
      "topic-11 ['server', 'moderator', 'comment', 'bot', 'textdavinci', 'contact', 'discord', 'friendly', 'compose', 'performed']\n",
      "topic-12 ['radical', 'leftwing', 'supremists', 'fascist', 'leftist', 'anticompetitive', 'stagnation', 'vat', 'lunatic', 'boycott']\n",
      "topic-13 ['scripters', 'storywork', 'screenplay', 'yesteryear', 'detective', 'robs', 'writingcommmunity', 'comedy', 'conciseness', 'lagaan']\n",
      "topic-14 ['teachergpt', 'printer', 'filthy', 'doggerel', 'poet', 'genuary', 'thurber', 'paperclip', 'thermonuclear', 'chatgptwritten']\n",
      "topic-15 ['bff', 'serach', 'pawer', 'kaha', 'download', 'bud', 'wesh', 'dogg', 'snoop', 'puro']\n",
      "topic-16 ['empathy', 'robotized', 'worldbrthe', 'writersofinstagram', 'writerscommunity', 'cutest', 'datadriveninvestor', 'privy', 'contractpilled', 'screwdriver']\n",
      "topic-17 ['cleaning', 'shehmmm', 'signups', 'deleting', 'exploiting', 'omfg', 'communist', 'wetting', 'compromising', 'outsourcing']\n",
      "topic-18 ['artificialintelliegence', 'evolveit', 'disrupt', 'thedigitalexecutive', 'inept', 'sociobits', 'trolly', 'gtthat', 'intensifying', 'journalist']\n",
      "topic-19 ['texted', 'queue', 'samaritan', 'slept', 'wellshit', 'exhales', 'dishwasher', 'username', 'failed', 'broke']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import chi2_contingency \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from coherence import get_topic_coherence\n",
    "\n",
    "# Seed fixation functions\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)  \n",
    "    np.random.seed(seed_value)  \n",
    "    torch.manual_seed(seed_value)  \n",
    "    if torch.cuda.is_available():  \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(41)\n",
    "\n",
    "args.stage_2_repeat = 1\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(device)\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    ##수정\n",
    "    twitter_trainloader = create_platform_dataloader(finetuneds, 'twitter', batch_size=bsz, num_workers=0)\n",
    "    reddit_trainloader = create_platform_dataloader(finetuneds, 'reddit', batch_size=bsz, num_workers=0)\n",
    "    youtube_trainloader = create_platform_dataloader(finetuneds, 'youtube', batch_size=bsz, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    ##\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(device), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "\n",
    "    best_npmi = -1\n",
    "    best_epoch = 0\n",
    "    best_model_state = None  \n",
    "    \n",
    "    # Create an iterator for each platform-specific DataLoader\n",
    "    twitter_iter = iter(twitter_trainloader)\n",
    "    reddit_iter = iter(reddit_trainloader)\n",
    "    youtube_iter = iter(youtube_trainloader)\n",
    "\n",
    "    # Calculate the length of the longest DataLoader to scope the training loop\n",
    "    max_length = max(len(twitter_trainloader), len(reddit_trainloader), len(youtube_trainloader))\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Sequentially fetching batches from platform-specific DataLoaders\n",
    "            try:\n",
    "                twitter_batch = next(twitter_iter)\n",
    "            except StopIteration:\n",
    "                # When Twitter DataLoader reaches the end, restart the iterator\n",
    "                twitter_iter = iter(twitter_trainloader)\n",
    "                twitter_batch = next(twitter_iter)\n",
    "\n",
    "            try:\n",
    "                reddit_batch = next(reddit_iter)\n",
    "            except StopIteration:\n",
    "                reddit_iter = iter(reddit_trainloader)\n",
    "                reddit_batch = next(reddit_iter)\n",
    "\n",
    "            try:\n",
    "                youtube_batch = next(youtube_iter)\n",
    "            except StopIteration:\n",
    "                youtube_iter = iter(youtube_trainloader)\n",
    "                youtube_batch = next(youtube_iter)\n",
    "\n",
    "            # Implement learning logic for each batch\n",
    "            for batch in [twitter_batch, reddit_batch, youtube_batch]:\n",
    "                _, org_input, pos_input, org_bow, pos_bow, _ = batch\n",
    "                org_input = org_input.cuda(device)\n",
    "                org_bow = org_bow.cuda(device)\n",
    "                pos_input = pos_input.cuda(device)\n",
    "                pos_bow = pos_bow.cuda(device)\n",
    "\n",
    "                batch_size = org_input.size(0) #org_input_ids.size(0)\n",
    "\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "                org_dists = org_dists[:, :org_bow.size(1)]\n",
    "                pos_dists = pos_dists[:, :pos_bow.size(1)]\n",
    "\n",
    "                recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * (1-org_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow), axis=1), axis=0)\n",
    "                recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * (1-pos_bow), axis=1), axis=0)\n",
    "                recons_loss *= 0.5\n",
    "\n",
    "                # consistency loss\n",
    "                pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "                cons_loss = -pos_sim.mean()\n",
    "\n",
    "                # distribution loss\n",
    "                # batchmean\n",
    "                distmatch_loss = dist_match_loss(torch.cat((org_topic,), dim=0), dirichlet_alpha_2)\n",
    "\n",
    "\n",
    "                loss = args.coeff_2_recon * recons_loss + \\\n",
    "                       args.coeff_2_cons * cons_loss + \\\n",
    "                       args.coeff_2_dist * distmatch_loss\n",
    "            \n",
    "            \n",
    "\n",
    "                losses.update(loss.item(), bsz)\n",
    "                closses.update(cons_loss.item(), bsz)\n",
    "                rlosses.update(recons_loss.item(), bsz)\n",
    "                distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    \n",
    "        model.eval()\n",
    "\n",
    "        # Extract the top 10 words for each topic\n",
    "        top_words_per_topic = {}\n",
    "        for topic_idx in range(model.N_topic):\n",
    "            top_words_indices = model.beta[topic_idx].topk(10).indices\n",
    "            top_words = [vocab_dict_reverse[idx.item()] for idx in top_words_indices]\n",
    "            top_words_per_topic[topic_idx] = top_words\n",
    "            \n",
    "        reference_corpus=[doc.split() for doc in valds.preprocess_ctm(valds.nonempty_text)]\n",
    "        topic_words_list = list(top_words_per_topic.values())\n",
    "        result = get_topic_coherence(topic_words_list, reference_corpus)\n",
    "        avg_npmi = result['NPMI']\n",
    "\n",
    "        # Optimal NPMI and epoch tracking\n",
    "        if avg_npmi > best_npmi:\n",
    "            best_npmi = avg_npmi\n",
    "            best_epoch = epoch\n",
    "            best_model_state = model.state_dict()  # 현재 모델 상태 저장\n",
    "\n",
    "    print(f\"Best Epoch: {best_epoch} with NPMI: {best_npmi}\")\n",
    "    torch.save(best_model_state, 'our_best_model_state.pth')\n",
    "    model.load_state_dict(torch.load('our_best_model_state.pth'))\n",
    "    \n",
    "    print(\"------- Evaluation results -------\")\n",
    "    # Each topic has its own wordset\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(10, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac0fd82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.7122771917868125, 'UCI': 5.188708587945945, 'UMASS': -1.1483764321230514, 'CV': 0.7297783087113847, 'Topic_Diversity': 0.955}\n"
     ]
    }
   ],
   "source": [
    "topic_words_list = list(all_list.values())\n",
    "reference_corpus=[doc.split() for doc in testds.preprocess_ctm(testds.nonempty_text)]\n",
    "\n",
    "topics=topic_words_list\n",
    "texts=reference_corpus\n",
    "print(get_topic_coherence(topics, texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb71e5",
   "metadata": {},
   "source": [
    "# MI Calulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5ba027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y): 2.9031547807445492\n",
      "H(Y|X): 2.8742561848737305\n",
      "Mutual Information (MI): 0.028898595870818777\n",
      "Original Mutual Information Score: 0.02889859587081954\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model.eval()\n",
    "testloader = DataLoader(testfinetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    \n",
    "\n",
    "ourmodel_test_topic_labels = []\n",
    "test_platform_labels = []\n",
    "\n",
    "for batch in testloader:\n",
    "    _, org_embedding, _, org_bow, _, platform_labels = batch\n",
    "    org_embedding = org_embedding.to(device)\n",
    "    _, topic_logit = model.decode(org_embedding)\n",
    "    topic_label = torch.argmax(F.softmax(topic_logit, dim=1), dim=1)\n",
    "    ourmodel_test_topic_labels.extend(topic_label.cpu().numpy())\n",
    "    test_platform_labels.extend(platform_labels)\n",
    "\n",
    "# Calculate topic distribution by platform\n",
    "topic_dist_df_test = pd.crosstab(pd.Series(ourmodel_test_topic_labels, name='Topic'),\n",
    "                            pd.Series(test_platform_labels, name='Platform'), normalize='index')\n",
    "\n",
    "# Calculate topic distribution by platform and overall\n",
    "platform_counts = pd.Series(test_platform_labels).value_counts()\n",
    "platform_probabilities = platform_counts / platform_counts.sum()\n",
    "\n",
    "# Calculate the entropy of the topic distribution over the entire dataset (H(Y))\n",
    "topic_probabilities = pd.Series(ourmodel_test_topic_labels).value_counts(normalize=True)\n",
    "H_Y = -np.sum(topic_probabilities * np.log(topic_probabilities + 1e-10))\n",
    "\n",
    "# Compute conditional entropy and H(Y|X) for each platform\n",
    "H_Y_given_X_total = 0\n",
    "for platform in platform_probabilities.index:\n",
    "    platform_indices = [i for i, x in enumerate(test_platform_labels) if x == platform]\n",
    "    platform_topic_labels = [ourmodel_test_topic_labels[i] for i in platform_indices]\n",
    "    platform_topic_prob = pd.Series(platform_topic_labels).value_counts(normalize=True)\n",
    "    \n",
    "    H_Y_given_X = -np.sum(platform_topic_prob * np.log(platform_topic_prob + 1e-10))\n",
    "    H_Y_given_X_total += platform_probabilities[platform] * H_Y_given_X\n",
    "\n",
    "mi = H_Y - H_Y_given_X_total\n",
    "H_X = -np.sum(platform_probabilities * np.log(platform_probabilities + 1e-10))\n",
    "\n",
    "\n",
    "print('H(Y):', H_Y)\n",
    "print('H(Y|X):', H_Y_given_X_total)\n",
    "print('Mutual Information (MI):', mi)\n",
    "\n",
    "mi_score = mutual_info_score(ourmodel_test_topic_labels, test_platform_labels)\n",
    "print(\"Original Mutual Information Score:\", mi_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24802d41",
   "metadata": {},
   "source": [
    "# Seperate platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "003e0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of reference_corpus\n",
    "total_length = len(reference_corpus)\n",
    "\n",
    "if total_length >= 18000:\n",
    "    twitter_texts = reference_corpus[:6000]\n",
    "    reddit_texts = reference_corpus[6000:12000]\n",
    "    youtube_texts = reference_corpus[12000:18000]\n",
    "else:\n",
    "    print(\"Not enough data.\")\n",
    "\n",
    "twitter_dictionary = Dictionary(twitter_texts)\n",
    "twitter_dictionary.add_documents(topic_words_list)\n",
    "\n",
    "reddit_dictionary = Dictionary(reddit_texts)\n",
    "reddit_dictionary.add_documents(topic_words_list)\n",
    "\n",
    "youtube_dictionary = Dictionary(youtube_texts)\n",
    "youtube_dictionary.add_documents(topic_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4558c",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "839fee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.5257036585398003, 'UCI': 4.529252108654194, 'UMASS': -1.8080285939387835, 'CV': 0.8279785814522855, 'Topic_Diversity': 0.955}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, twitter_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edd2a9",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e5e213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.7669463298158827, 'UCI': 3.5512475447898737, 'UMASS': -0.9595442389240559, 'CV': 0.7988206610347721, 'Topic_Diversity': 0.955}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, reddit_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8c453",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f56427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPMI': 0.6444031679463139, 'UCI': 5.15491069547586, 'UMASS': -0.8707065182521553, 'CV': 0.8135890235745974, 'Topic_Diversity': 0.955}\n"
     ]
    }
   ],
   "source": [
    "print(get_topic_coherence(topics, youtube_texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
