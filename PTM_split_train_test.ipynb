{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361278b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# 로거 설정\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                    filename='dataset_log.log')  # 로그를 저장할 파일명 지정\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a009520",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_text = '--base-model sentence-transformers/paraphrase-MiniLM-L6-v2 ' + \\\n",
    "            '--dataset all --n-word 30000 --epochs-1 100 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 50 ' + \\\n",
    "            '--n-cluster 20 ' + \\\n",
    "            '--stage-1-ckpt trained_model/news_model_paraphrase-MiniLM-L6-v2_stage1_20t_2000w_99e.ckpt ' + \\\n",
    "            '--palmetto-dir /home/minseo/jupyter_dir/PTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c1d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/minseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from data import TwitterDataset, RedditDataset, YoutubeDataset, BertDataset\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import ConcatDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ab2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bce59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    # 각 stage에서의 epochs수 \n",
    "    parser.add_argument('--epochs-1', default=50, type=int,\n",
    "                        help='Number of training epochs for Stage 1')   \n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    #각 stage에서의 batch size\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    #data set정의 \n",
    "    parser.add_argument('--dataset', default='twitter', type=str,\n",
    "                        choices=['twitter', 'reddit', 'youtube','all'],\n",
    "                        help='Name of the dataset')\n",
    "    # 클러스터 수와 topic의 수는 20 (k==20)\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    # 단어vocabulary는 2000로 setting\n",
    "    parser.add_argument('--n-word', default=30000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1,2,3], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "   \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    " \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    \n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str,\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "#데이터들을 로드함(텍스트중에 none값을 제거/dataset_name에 따라 textData에 들어가는 내용이 달라짐)\n",
    "def data_load(dataset_name, sample_size=10000):\n",
    "    should_measure_hungarian = False\n",
    "    textData = []\n",
    "    \n",
    "    if dataset_name == 'twitter':\n",
    "        dataset = TwitterDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Twitter\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'reddit':\n",
    "        dataset = RedditDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"Reddit\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'youtube':\n",
    "        dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        textData = [(text, \"YouTube\") for text in dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    elif dataset_name == 'all':\n",
    "        twitter_dataset = TwitterDataset(sample_size=sample_size)\n",
    "        reddit_dataset = RedditDataset(sample_size=sample_size)\n",
    "        youtube_dataset = YoutubeDataset(sample_size=sample_size)\n",
    "        \n",
    "        # filtering NaN values\n",
    "        textData += [(text, \"Twitter\") for text in twitter_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"Reddit\") for text in reddit_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "        textData += [(text, \"YouTube\") for text in youtube_dataset.texts if not (isinstance(text, float) and math.isnan(text))]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name!\")\n",
    "    \n",
    "    return textData, should_measure_hungarian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8d1e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da43edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a80e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 각 데이터셋 초기화(data.py에서 확인 가능)\n",
    "twitter_ds = TwitterDataset()\n",
    "reddit_ds = RedditDataset()\n",
    "youtube_ds = YoutubeDataset()\n",
    "\n",
    "# train_twitter_texts, test_twitter_texts, train_twitter_labels, test_twitter_labels = train_test_split(twitter_ds.texts, twitter_ds.labels, train_size = 0.7, test_size = 0.3)\n",
    "# train_reddit_texts, test_reddit_texts, train_reddit_labels, test_reddit_labels = train_test_split(reddit_ds.texts, reddit_ds.labels, train_size = 0.7, test_size = 0.3)\n",
    "# train_youtube_texts, test_youtube_texts, train_youtube_labels, test_youtube_labels = train_test_split(youtube_ds.texts, youtube_ds.labels, train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "train_twitter_texts, test_twitter_texts = train_test_split(twitter_ds.texts, train_size = 0.7, test_size = 0.3)\n",
    "train_reddit_texts, test_reddit_texts = train_test_split(reddit_ds.texts, train_size = 0.7, test_size = 0.3)\n",
    "train_youtube_texts, test_youtube_texts = train_test_split(youtube_ds.texts, train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "# train_total_gpt_label = train_twitter_labels + train_reddit_labels + train_youtube_labels\n",
    "train_total_text_list = train_twitter_texts + train_reddit_texts + train_youtube_texts\n",
    "\n",
    "# test_total_gpt_label = test_twitter_labels + test_reddit_labels + test_youtube_labels\n",
    "test_total_text_list = test_twitter_texts + test_reddit_texts + test_youtube_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d049e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, bert, text_list, N_word, vectorizer=None, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.nonempty_text = [text for text in text_list if len(text) > 0]\n",
    "        \n",
    "        # Remove new lines\n",
    "        self.nonempty_text = [re.sub(\"\\n\",\" \", sent) for sent in self.nonempty_text]\n",
    "                \n",
    "        # Remove Emails\n",
    "        self.nonempty_text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove new line characters\n",
    "        self.nonempty_text = [re.sub('\\s+', ' ', sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        # Remove distracting single quotes\n",
    "        self.nonempty_text = [re.sub(\"\\'\", \"\", sent) for sent in self.nonempty_text]\n",
    "        \n",
    "        self.jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert)\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        self.N_word = N_word\n",
    "        \n",
    "        if vectorizer == None:\n",
    "            self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "            self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text))\n",
    "        else:\n",
    "            self.vectorizer = vectorizer\n",
    "            \n",
    "        self.org_list = []\n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            org_input = self.tokenizer(sent, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "            org_input['input_ids'] = torch.squeeze(org_input['input_ids'])\n",
    "            org_input['attention_mask'] = torch.squeeze(org_input['attention_mask'])\n",
    "            self.org_list.append(org_input)\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray()\n",
    "        vectorized_input = vectorized_input.astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                             for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp        \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.nonempty_text)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.org_list[idx], self.bow_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9c2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105000/105000 [03:30<00:00, 499.48it/s]\n",
      "100%|██████████| 45000/45000 [01:24<00:00, 532.75it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=train_total_text_list, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "\n",
    "testds = BertDataset(bert=bert_name, text_list=test_total_text_list, N_word=n_word, vectorizer=None, lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b97ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all_texts to create a BoW matrix\n",
    "total_bow_matrix = trainds.vectorizer.transform(train_total_text_list).toarray()\n",
    "\n",
    "test_bow_matrix = testds.vectorizer.transform(test_total_text_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e5d973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105000, 30000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확인용 코드\n",
    "total_bow_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab9e2097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배열의 크기: (105000, 30000)\n",
      "배열의 메모리 사용량: 25200000000 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"배열의 크기:\", total_bow_matrix.shape)\n",
    "print(\"배열의 메모리 사용량:\", total_bow_matrix.nbytes, \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13da6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_word = total_bow_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd08ab9",
   "metadata": {},
   "source": [
    "# Re_fornulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85826fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f727a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_cosine_similarity_indices(total_bow_matrix, batch_size=500):\n",
    "    n_rows = total_bow_matrix.shape[0]\n",
    "    max_similarity_indices = np.zeros(n_rows, dtype=np.int64)\n",
    "\n",
    "    for start_idx in range(0, n_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_rows)\n",
    "        batch_data = total_bow_matrix[start_idx:end_idx]\n",
    "\n",
    "        batch_similarity = cosine_similarity(batch_data, total_bow_matrix)\n",
    "\n",
    "        # 자기 자신과의 유사도를 -1로 설정\n",
    "        for i, original_idx in enumerate(range(start_idx, end_idx)):\n",
    "            batch_similarity[i, original_idx] = -1\n",
    "\n",
    "        # 각 행에서 가장 큰 값을 가진 인덱스 찾기\n",
    "        max_indices = np.argmax(batch_similarity, axis=1)\n",
    "        max_similarity_indices[start_idx:end_idx] = max_indices\n",
    "\n",
    "        logger.info(f\"{end_idx}/{n_rows} 데이터 처리 완료\")\n",
    "\n",
    "    return max_similarity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51234164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 매트릭스를 계산합니다.\n",
    "similarity_matrix = compute_max_cosine_similarity_indices(total_bow_matrix)\n",
    "\n",
    "test_similarity_matrix = compute_max_cosine_similarity_indices(test_bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745200ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, similarity_matrix, N_word, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        self.N_word = N_word\n",
    "        \n",
    "        self.jargons = set(['bigdata', 'RT', 'announces', 'rival', 'MSFT', 'launches', 'SEO', 'tweets', 'WSJ', 'unveils', 'MBA', 'machinelearning', 'HN', 'artificalintelligence', 'DALLE', 'NFT', 'NLP', 'edtech', 'cybersecurity', 'malware', 'CEO', 'ML', 'JPEG', 'GOOGL', 'UX', 'artificialintelligence', 'technews', 'fintech', 'tweet', 'CHATGPT', 'viral', 'blockchain', 'BARD', 'reportedly', 'BREAKING', 'deeplearning', 'educators', 'datascience', 'GOOG', 'competitor', 'FREE', 'startups', 'AGIX', 'CHAT', 'CNET', 'integrating', 'BB', 'maker', 'passes', 'NYT', 'yall', 'rescue', 'heres', 'NYC', 'LLM', 'airdrop', 'powered', 'podcast', 'Q', 'PM', 'newsletter', 'startup', 'TSLA', 'WIRED', 'digitalmarketing', 'CTO', 'changer', 'announced', 'database', 'launched', 'warns', 'popularity', 'AWS', 'nocode', 'trending', 'metaverse', 'cc', 'PR', 'RSS', 'MWC', 'USMLE', 'copywriting', 'marketers', 'tweeting', 'amid', 'AGI', 'socialmedia', 'webinar', 'agrees', 'invests', 'launch', 'killer', 'GM', 'bullish', 'edchat', 'RLHF', 'integration', 'fastestgrowing', 'CNN', 'exam',\n",
    "                  'deleted', 'gif', 'giphy', 'dm', 'removed', 'remindme', 'yup', 'sydney', 'yep', 'patched', 'nope', 'giphydownsized', 'vpn', 'ascii', 'ah', 'chadgpt', 'nerfed', 'jesus', 'xd', 'wtf', 'upvote', 'nah', 'op', 'mods', 'hahaha', 'nsfw', 'huh', 'holy', 'iq', 'jailbreak', 'blah', 'bruh', 'yea', 'agi', 'porn', 'waitlist', 'nerf', 'downvoted', 'refresh', 'omg', 'sus', 'characterai', 'meth', 'chinese', 'sub', 'rick', 'american', 'elon', 'sam', 'quack', 'youchat', 'uk', 'chad', 'archived', 'youcom', 'screenshot', 'llm', 'hitler', 'lmao', 'playground', 'rpg', 'delete', 'tldr', 'davinci', 'trump', 'hangman', 'haha', 'tay', 'karma', 'john', 'chatgtp', 'url', 'wokegpt', 'offended', 'fucked', 'redditor', 'ceo', 'agreed', 'emojis', 'cheers', 'ais', 'tag', 'wow', 'lmfao', 'p', 'rip', 'chats', 'hmm', 'bypass', 'llms', 'temperature', 'login', 'cgpt', 'windows', 'novelai', 'biden', 'donald', 'christmas', 'ms', 'cringe',\n",
    "                   'ZRONX', 'rook', 'thumbnail', 'vid', 'bhai', 'bishop', 'circle', 'subscribed', 'quot', 'bless', 'tutorial', 'XD', 'sir', 'GEMX', 'profitable', 'earning', 'quotquot', 'enjoyed', 'ur', 'bra', 'JIM', 'broker', 'levy', 'vids', 'stare', 'tutorials', 'subscribers', 'sponsor', 'hai', 'lifechanging', 'curve', 'shorts', 'earn', 'trader', 'PC', 'folders', 'informative', 'br', 'chess', 'jontron', 'brother', 'T', 'YT', 'upload', 'O', 'subscriber', 'intro', 'DAN', 'aint', 'download', 'LOL', 'shes', 'moves', 'telegram', 'shortlisted', 'liked', 'websiteapp', 'watched', 'grant', 'plz', 'KINGDOM', 'YOU', 'MESSIAH', 'mate', 'ki', 'subs', 'pawn', 'hes', 'U', 'HACKBANZER', 'ka', 'brbr', 'affiliate', 'clip', 'beast', 'trade', 'ive', 'ho', 'approved', 'bhi', 'gotta', 'profits', 'wanna', 'subscribe', 'funds', 'labels', 'recommended', 'audio', 'uploaded', 'appreciated', 'UBI', 'pls', 'upto', 'alot', 'twist', 'GTP', 'accent', 'monetized', 'S', 'btw'\n",
    "                  ])\n",
    "        \n",
    "#         english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "#         self.stopwords_list = set(english_stopwords)\n",
    "        self.stopwords_list = set(TfidfVectorizer(stop_words=\"english\").get_stop_words()).union(self.jargons)\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(stop_words=None, max_features=self.N_word, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "#         self.similarity_matrix = torch.tensor(similarity_matrix)  # NumPy 배열을 PyTorch 텐서로 변환\n",
    "#         sim_weight, sim_indices = self.similarity_matrix.topk(k=k, dim=-1)\n",
    "#         zip_iterator = zip(np.arange(len(sim_weight)), sim_indices.squeeze().data.numpy())\n",
    "#         self.pos_dict = dict(zip_iterator)\n",
    "        self.pos_dict = similarity_matrix\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return idx, self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd671d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abcb0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, similarity_matrix, n_word, lemmatize=True)  \n",
    "\n",
    "testfinetuneds = Stage2Dataset(model.encoder, testds, test_similarity_matrix, n_word, lemmatize=True) \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(finetuneds.embedding_list)\n",
    "len(finetuneds.bow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daded5c",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_measure_hungarian = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01506b05",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1086db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import chi2_contingency \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "args.stage_2_repeat = 1\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    ##수정\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=True, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    ##\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        for batch_idx, batch in enumerate(trainloader):  \n",
    "            _,org_input, pos_input, org_bow, pos_bow = batch\n",
    "            org_input = org_input.cuda(gpu_ids[0])\n",
    "            org_bow = org_bow.cuda(gpu_ids[0])\n",
    "            pos_input = pos_input.cuda(gpu_ids[0])\n",
    "            pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "            batch_size = org_input.size(0) #org_input_ids.size(0)\n",
    "\n",
    "            org_dists, org_topic_logit = model.decode(org_input)\n",
    "            pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "            org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "            pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "            # reconstruction loss\n",
    "            # batchmean\n",
    "#             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "#             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "            \n",
    "#             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "#             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "            \n",
    "            recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * (1-org_bow), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * (1-pos_bow), axis=1), axis=0)\n",
    "            recons_loss *= 0.5\n",
    "\n",
    "            # consistency loss\n",
    "            pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "            cons_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution loss\n",
    "            # batchmean\n",
    "#             distmatch_loss = dist_match_loss(torch.cat((org_topic), dim=0), dirichlet_alpha_2)\n",
    "            distmatch_loss = dist_match_loss(torch.cat((org_topic,), dim=0), dirichlet_alpha_2)\n",
    "\n",
    "\n",
    "            loss = args.coeff_2_recon * recons_loss + \\\n",
    "                   args.coeff_2_cons * cons_loss + \\\n",
    "                   args.coeff_2_dist * distmatch_loss \n",
    "            \n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(cons_loss.item(), bsz)\n",
    "            rlosses.update(recons_loss.item(), bsz)\n",
    "            distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    print(\"------- Evaluation results -------\")\n",
    "    #각 토픽당 가지는 워드셋\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(10, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)\n",
    "\n",
    "    topic_words_list = list(all_list.values())\n",
    "#     now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "    \n",
    "    reference_corpus=[doc.split() for doc in testds.preprocess_ctm(testds.nonempty_text)]\n",
    "    reference_dictionary = Dictionary(reference_corpus)\n",
    "    reference_dictionary.add_documents(topic_words_list)\n",
    "    \n",
    "    # c_v 코히어런스 모델 생성 및 점수 계산\n",
    "    cv_model = CoherenceModel(topics=topic_words_list, texts=reference_corpus, dictionary=reference_dictionary, coherence='c_v', topn = 10)\n",
    "    cv_score = cv_model.get_coherence()\n",
    "\n",
    "    # npmi 코히어런스 모델 생성 및 점수 계산\n",
    "    npmi_model = CoherenceModel(topics=topic_words_list, texts=reference_corpus, dictionary=reference_dictionary, coherence='c_npmi',topn = 10)\n",
    "    npmi_score = npmi_model.get_coherence()\n",
    "\n",
    "    # umass 코히어런스 모델 생성 및 점수 계산\n",
    "    umass_model = CoherenceModel(topics=topic_words_list, texts=reference_corpus, dictionary=reference_dictionary, coherence='u_mass',topn = 10)\n",
    "    umass_score = umass_model.get_coherence()\n",
    "\n",
    "    # uci 코히어런스 모델 생성 및 점수 계산\n",
    "    uci_model = CoherenceModel(topics=topic_words_list, texts=reference_corpus, dictionary=reference_dictionary, coherence='c_uci',topn = 10)\n",
    "    uci_score = uci_model.get_coherence()\n",
    "\n",
    "    # 각 코히어런스 점수 출력\n",
    "    print(f\"c_v Score: {cv_score}\")\n",
    "    print(f\"NPMI Score: {npmi_score}\")\n",
    "    print(f\"UMass Score: {umass_score}\")\n",
    "    print(f\"UCI Score: {uci_score}\")\n",
    "    \n",
    "#     results = get_topic_qualities(topic_words_list, \n",
    "#             reference_corpus, \n",
    "#             dictionary=dictionary,\n",
    "#             filename=f'results/{now}.txt')\n",
    "\n",
    "#     print(results)\n",
    "#     print()\n",
    "#     results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # umass 코히어런스 모델 생성 및 점수 계산\n",
    "    umass_model = CoherenceModel(topics=topic_words_list, texts=reference_corpus, dictionary=reference_dictionary, coherence='u_mass', topn=10)\n",
    "    umass_score = umass_model.get_coherence()\n",
    "\n",
    "    print(f\"UMass Score: {umass_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kms",
   "language": "python",
   "name": "kms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
